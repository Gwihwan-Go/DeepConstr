{
    "solved": {
        "torch.Tensor.__and__": [
            "\"bitwise_and_cpu\" not implemented for 'Float'"
        ],
        "torch.Tensor.__iand__": [
            "output with shape [] doesn't match the broadcast shape [5]",
            "The size of tensor a (7) must match the size of tensor b (0) at non-singleton dimension 1",
            "\"bitwise_and_cpu\" not implemented for 'Float'",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.__ior__": [
            "result type Float can't be cast to the desired output type Int",
            "\"bitwise_or_cpu\" not implemented for 'Float'",
            "The size of tensor a (6) must match the size of tensor b (8) at non-singleton dimension 3"
        ],
        "torch.Tensor.__ixor__": [
            "output with shape [1] doesn't match the broadcast shape [1, 8, 8, 9, 8, 5]",
            "uint64",
            "\"bitwise_xor_cpu\" not implemented for 'Float'",
            "The size of tensor a (9) must match the size of tensor b (6) at non-singleton dimension 6"
        ],
        "torch.Tensor.__lshift__": [
            "The size of tensor a (9) must match the size of tensor b (3) at non-singleton dimension 2",
            "\"lshift_cpu\" not implemented for 'Float'"
        ],
        "torch.Tensor.__or__": [
            "The size of tensor a (7) must match the size of tensor b (4) at non-singleton dimension 0",
            "uint32",
            "\"bitwise_or_cpu\" not implemented for 'Float'"
        ],
        "torch.Tensor.__rshift__": [
            "The size of tensor a (5) must match the size of tensor b (8) at non-singleton dimension 6",
            "\"rshift_cpu\" not implemented for 'Float'"
        ],
        "torch.Tensor.__xor__": [
            "uint32",
            "\"bitwise_xor_cpu\" not implemented for 'Half'",
            "The size of tensor a (9) must match the size of tensor b (6) at non-singleton dimension 5",
            "\"bitwise_xor_cpu\" not implemented for 'Float'"
        ],
        "torch.Tensor.abs": [],
        "torch.Tensor.abs_": [],
        "torch.Tensor.acos": [],
        "torch.Tensor.acos_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.acosh": [],
        "torch.Tensor.add": [
            "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 6"
        ],
        "torch.Tensor.add_": [
            "output with shape [1] doesn't match the broadcast shape [1, 9, 8, 2, 9]"
        ],
        "torch.Tensor.addbmm": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
            "The expanded size of the tensor (4) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [2, 4].  Tensor sizes: [5]",
            "Dimension specified as 1 but tensor has no dimensions",
            "batch2 must be a 3D tensor",
            "Incompatible matrix sizes for bmm (6x8 and 5x2)",
            "batch1 and batch2 must have same number of batches, got 9 and 8"
        ],
        "torch.Tensor.addcdiv": [
            "The size of tensor a (7) must match the size of tensor b (5) at non-singleton dimension 5"
        ],
        "torch.Tensor.addcdiv_": [
            "Integer division with addcdiv is no longer supported, and in a future  release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes.",
            "The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 0",
            "output with shape [1, 9, 1, 5] doesn't match the broadcast shape [1, 9, 9, 5]"
        ],
        "torch.Tensor.addcmul": [
            "The size of tensor a (4) must match the size of tensor b (6) at non-singleton dimension 5"
        ],
        "torch.Tensor.addcmul_": [
            "result type Float can't be cast to the desired output type Int",
            "The size of tensor a (9) must match the size of tensor b (4) at non-singleton dimension 6",
            "output with shape [1] doesn't match the broadcast shape [1, 6, 6, 6]"
        ],
        "torch.Tensor.addmm": [
            "The expanded size of the tensor (9) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [7, 9].  Tensor sizes: [8, 5]",
            "expand(torch.FloatTensor{[8, 2, 9, 2, 1, 5, 4]}, size=[2, 5]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (7)",
            "mat1 and mat2 shapes cannot be multiplied (5x9 and 7x5)",
            "self and mat2 must have the same dtype, but got Int and Float",
            "mat1 must be a matrix, got 7-D tensor",
            "mat1 and mat2 must have the same dtype"
        ],
        "torch.Tensor.addmm_": [
            "mat1 must be a matrix, got 4-D tensor",
            "self and mat2 must have the same dtype, but got Float and Int",
            "mat1 and mat2 shapes cannot be multiplied (3x4 and 8x5)",
            "Bad in-place call: input tensor size [8, 6, 7, 8, 2, 7, 8] and output tensor size [9, 0] should match",
            "mat1 and mat2 must have the same dtype"
        ],
        "torch.Tensor.addmv": [
            "vector + matrix @ vector expected, got 1, 1, 1"
        ],
        "torch.Tensor.addmv_": [
            "Bad in-place call: input tensor dtype int and output tensor dtype float should match",
            "Bad in-place call: input tensor size [1] and output tensor size [2] should match",
            "Dimension specified as 0 but tensor has no dimensions",
            "size mismatch, got input (4), mat (4x7), vec (3)",
            "vector + matrix @ vector expected, got 7, 0, 7"
        ],
        "torch.Tensor.addr": [
            "addr: Expected 1-D argument vec1, but got 2-D",
            "The expanded size of the tensor (7) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [6, 7].  Tensor sizes: [4]"
        ],
        "torch.Tensor.adjoint": [
            "tensor.adjoint() is only supported on matrices or batches of matrices. Got 1-D tensor."
        ],
        "torch.Tensor.all": [],
        "torch.Tensor.amax": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 5)"
        ],
        "torch.Tensor.amin": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 6)"
        ],
        "torch.Tensor.aminmax": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 3)"
        ],
        "torch.Tensor.angle": [],
        "torch.Tensor.any": [],
        "torch.Tensor.argmax": [
            "Dimension out of range (expected to be in range of [-1, 0], but got -4)",
            "argmax(): Expected reduction dim 0 to have non-zero size."
        ],
        "torch.Tensor.argmin": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 5)",
            "argmin(): Expected reduction dim to be specified for input.numel() == 0.",
            "argmin(): Expected reduction dim 0 to have non-zero size."
        ],
        "torch.Tensor.argsort": [
            "Dimension out of range (expected to be in range of [-1, 0], but got -4)"
        ],
        "torch.Tensor.argwhere": [],
        "torch.Tensor.as_strided": [
            "Storage size calculation overflowed with sizes=[-3, -2] and strides=[9, 7]",
            "as_strided: Negative strides are not supported at the moment, got strides: [9, 7, 3, -3, -4, 6, -1]",
            "mismatch in length of strides and shape"
        ],
        "torch.Tensor.as_strided_": [
            "mismatch in length of strides and shape",
            "numel: integer multiplication overflow",
            "as_strided: Negative strides are not supported at the moment, got strides: [-1]"
        ],
        "torch.Tensor.asin": [],
        "torch.Tensor.asin_": [],
        "torch.Tensor.asinh": [],
        "torch.Tensor.atan": [],
        "torch.Tensor.atan2": [
            "The size of tensor a (2) must match the size of tensor b (9) at non-singleton dimension 5"
        ],
        "torch.Tensor.atan2_": [
            "The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 5",
            "Too large tensor shape: shape = [6, 9, 9, 9, 9, 9, 9, 9, 9]",
            "output with shape [3, 5, 1, 5, 5, 5] doesn't match the broadcast shape [3, 5, 3, 5, 5, 5]",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.atan_": [],
        "torch.Tensor.atanh": [],
        "torch.Tensor.bitwise_and": [
            "\"bitwise_and_cpu\" not implemented for 'Float'",
            "The size of tensor a (9) must match the size of tensor b (6) at non-singleton dimension 1"
        ],
        "torch.Tensor.bitwise_left_shift": [
            "\"lshift_cpu\" not implemented for 'Float'",
            "The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 0"
        ],
        "torch.Tensor.bitwise_not": [
            "\"bitwise_not_cpu\" not implemented for 'Float'"
        ],
        "torch.Tensor.bitwise_not_": [
            "\"bitwise_not_cpu\" not implemented for 'Float'"
        ],
        "torch.Tensor.bitwise_or": [
            "Too large tensor shape: shape = [5, 8, 9, 9, 9, 9, 9, 9, 9]",
            "\"bitwise_or_cpu\" not implemented for 'Float'",
            "The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 5"
        ],
        "torch.Tensor.bitwise_xor": [
            "\"bitwise_xor_cpu\" not implemented for 'Float'",
            "The size of tensor a (9) must match the size of tensor b (3) at non-singleton dimension 4"
        ],
        "torch.Tensor.bmm": [
            "batch1 must be a 3D tensor",
            "expected scalar type Int but found Float",
            "Expected size for first two dimensions of batch2 tensor to be: [2, 8] but got: [1, 5].",
            "\"bmm\" not implemented for 'Bool'",
            "'complex32'"
        ],
        "torch.Tensor.broadcast_to": [
            "numel: integer multiplication overflow",
            "expand(torch.FloatTensor{[8, 4, 9, 4, 4, 4, 4, 4, 4]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (9)"
        ],
        "torch.Tensor.ceil": [],
        "torch.Tensor.ceil_": [],
        "torch.Tensor.cholesky": [],
        "torch.Tensor.cholesky_inverse": [],
        "torch.Tensor.cholesky_solve": [
            "Too large tensor shape: shape = [8, 8, 9, 8, 8, 8, 8, 8, 8]",
            "Incompatible matrix sizes for cholesky_solve: each A matrix is 1 by 1 but each b matrix is 9 by 9",
            "u should have at least 2 dimensions, but has 0 dimensions instead"
        ],
        "torch.Tensor.chunk": [
            "chunk expects at least a 1-dimensional tensor",
            "chunk expects `chunks` to be greater than 0, got: 0",
            "Dimension out of range (expected to be in range of [-1, 0], but got 82)"
        ],
        "torch.Tensor.clamp": [
            "negative dimensions are not allowed"
        ],
        "torch.Tensor.clamp_": [],
        "torch.Tensor.clamp_max": [],
        "torch.Tensor.clamp_max_": [],
        "torch.Tensor.clamp_min": [],
        "torch.Tensor.clamp_min_": [],
        "torch.Tensor.conj": [],
        "torch.Tensor.conj_physical": [],
        "torch.Tensor.copysign": [
            "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 2"
        ],
        "torch.Tensor.corrcoef": [],
        "torch.Tensor.cos": [],
        "torch.Tensor.cos_": [],
        "torch.Tensor.cosh": [],
        "torch.Tensor.cosh_": [],
        "torch.Tensor.count_nonzero": [
            "Dimension out of range (expected to be in range of [-4, 3], but got 5)"
        ],
        "torch.Tensor.cov": [
            "cov(): expected fweights to have integral dtype but got fweights with Float dtype",
            "cov(): expected aweights to have one or fewer dimensions but got aweights with 2 dimensions",
            "cov(): expected fweights to have one or fewer dimensions but got fweights with 9 dimensions",
            "cov(): expected input to have two or fewer dimensions but got an input with 8 dimensions"
        ],
        "torch.Tensor.cummax": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 98)"
        ],
        "torch.Tensor.cummin": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 75)"
        ],
        "torch.Tensor.cumprod": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 38)"
        ],
        "torch.Tensor.cumsum": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 5)"
        ],
        "torch.Tensor.cumsum_": [
            "Dimension out of range (expected to be in range of [-4, 3], but got 81)"
        ],
        "torch.Tensor.deg2rad": [],
        "torch.Tensor.det": [
            "linalg.det: A must be batches of square matrices, but they are 3 by 7 matrices",
            "linalg.det: The input tensor A must have at least 2 dimensions."
        ],
        "torch.Tensor.diag": [
            "diag(): Supports 1D or 2D tensors. Got 3D"
        ],
        "torch.Tensor.diag_embed": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 87)",
            "diagonal dimensions cannot be identical 0, 0"
        ],
        "torch.Tensor.diagflat": [],
        "torch.Tensor.diagonal": [
            "Too large tensor shape: shape = [8, 9, 9, 9, 9, 9, 9, 9, 9]",
            "diagonal dimensions cannot be identical -1, 0",
            "Dimension out of range (expected to be in range of [-1, 0], but got 51)"
        ],
        "torch.Tensor.diff": [
            "diff expects the shape of tensor to prepend or append to match that of input except along the differencing dimension; input.size(1) = 7, but got tensor.size(1) = 5",
            "order must be non-negative but got -4",
            "diff expects prepend or append to be the same dimension as input",
            "Too large tensor shape: shape = [9, 8, 9, 7, 9, 9, 9, 9, 9]",
            "diff expects input to be at least one-dimensional",
            "Dimension out of range (expected to be in range of [-9, 8], but got 96)"
        ],
        "torch.Tensor.digamma": [],
        "torch.Tensor.digamma_": [],
        "torch.Tensor.dist": [
            "Too large tensor shape: shape = [5, 9, 9, 9, 9, 9, 9, 9, 9]",
            "The size of tensor a (8) must match the size of tensor b (5) at non-singleton dimension 8"
        ],
        "torch.Tensor.div": [
            "The size of tensor a (7) must match the size of tensor b (5) at non-singleton dimension 5"
        ],
        "torch.Tensor.div_": [
            "The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 2",
            "Too large tensor shape: shape = [8, 9, 9, 9, 9, 9, 9, 9, 9]",
            "output with shape [8, 9, 9, 9] doesn't match the broadcast shape [7, 8, 9, 9, 9]"
        ],
        "torch.Tensor.divide": [
            "The size of tensor a (9) must match the size of tensor b (4) at non-singleton dimension 6"
        ],
        "torch.Tensor.divide_": [
            "output with shape [] doesn't match the broadcast shape [5, 6]"
        ],
        "torch.Tensor.dot": [
            "dot : expected both vectors to have same dtype, but found Int and Float",
            "inconsistent tensor size, expected tensor [3] and src [9] to have the same number of elements, but got 3 and 9 elements respectively",
            "1D tensors expected, but got 5D and 7D tensors"
        ],
        "torch.Tensor.dsplit": [
            "number of sections must be larger than 0, got -1",
            "torch.dsplit requires a tensor with at least 3 dimension, but got a tensor with 2 dimensions!"
        ],
        "torch.Tensor.eq": [
            "Too large tensor shape: shape = [7, 9, 6, 8, 9, 9, 9, 9, 9]"
        ],
        "torch.Tensor.erf": [],
        "torch.Tensor.erf_": [],
        "torch.Tensor.erfc": [],
        "torch.Tensor.erfc_": [],
        "torch.Tensor.erfinv": [],
        "torch.Tensor.erfinv_": [],
        "torch.Tensor.exp": [],
        "torch.Tensor.exp2": [],
        "torch.Tensor.exp_": [],
        "torch.Tensor.expand": [
            "numel: integer multiplication overflow",
            "expand(torch.FloatTensor{[1, 1]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (2)"
        ],
        "torch.Tensor.expand_as": [
            "The expanded size of the tensor (8) must match the existing size (5) at non-singleton dimension 5.  Target sizes: [7, 8, 8, 9, 8, 8].  Tensor sizes: [5, 5, 5, 5]"
        ],
        "torch.Tensor.expm1": [],
        "torch.Tensor.expm1_": [],
        "torch.Tensor.fill_": [],
        "torch.Tensor.flatten": [
            "Too large tensor shape: shape = [9, 9, 9, 9, 9, 9, 9, 9, 9]",
            "flatten() has invalid args: start_dim cannot come after end_dim",
            "Dimension out of range (expected to be in range of [-4, 3], but got 9)"
        ],
        "torch.Tensor.flip": [
            "negative dimensions are not allowed",
            "Too large tensor shape: shape = [7, 8, 9, 9, 9, 9, 9, 9, 9]",
            "dim 8 appears multiple times in the list of dims"
        ],
        "torch.Tensor.fliplr": [],
        "torch.Tensor.flipud": [],
        "torch.Tensor.float_power": [
            "Too large tensor shape: shape = [8, 9, 9, 9, 9, 9, 9, 9, 9]"
        ],
        "torch.Tensor.float_power_": [
            "output with shape [] doesn't match the broadcast shape [4, 8, 8, 9, 8]",
            "the base given to float_power_ has dtype Float but the operation's result requires dtype Double",
            "Too large tensor shape: shape = [9, 8, 6, 7, 9, 8, 9, 9, 9]"
        ],
        "torch.Tensor.floor": [],
        "torch.Tensor.floor_": [],
        "torch.Tensor.floor_divide": [
            "Too large tensor shape: shape = [4, 9, 9, 9, 9, 9, 9, 9, 9]",
            "The size of tensor a (7) must match the size of tensor b (2) at non-singleton dimension 5"
        ],
        "torch.Tensor.fmax": [
            "Too large tensor shape: shape = [8, 9, 9, 9, 9, 6, 9, 9, 9]"
        ],
        "torch.Tensor.fmin": [
            "Too large tensor shape: shape = [9, 9, 9, 9, 9, 9, 9, 9, 9]",
            "The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 2"
        ],
        "torch.Tensor.fmod": [
            "Too large tensor shape: shape = [4, 9, 9, 9, 9, 9, 9, 9, 9]",
            "The size of tensor a (2) must match the size of tensor b (8) at non-singleton dimension 3"
        ],
        "torch.Tensor.fmod_": [
            "result type Float can't be cast to the desired output type Int",
            "output with shape [7, 7] doesn't match the broadcast shape [1, 7, 7]"
        ],
        "torch.Tensor.frac": [],
        "torch.Tensor.frac_": [],
        "torch.Tensor.frexp": [],
        "torch.Tensor.gather": [
            "gather(): Expected dtype int64 for index",
            "Dimension out of range (expected to be in range of [-2, 1], but got 78)",
            "Too large tensor shape: shape = [5, 9, 9, 9, 9, 9, 9, 9, 9]",
            "Index tensor must have the same number of dimensions as input tensor",
            "Size does not match at dimension 1 expected index [4, 4] to be smaller than self [1, 1] apart from dimension 0",
            "negative dimensions are not allowed",
            "index 789643 is out of bounds for dimension 0 with size 6"
        ],
        "torch.Tensor.ge": [
            "The size of tensor a (2) must match the size of tensor b (9) at non-singleton dimension 6"
        ],
        "torch.Tensor.ge_": [],
        "torch.Tensor.gt": [
            "The size of tensor a (7) must match the size of tensor b (6) at non-singleton dimension 3"
        ],
        "torch.Tensor.hardshrink": [
            "\"hardshrink_cpu\" not implemented for 'Long'",
            "\"hardshrink_cpu\" not implemented for 'Int'"
        ],
        "torch.Tensor.heaviside": [
            "heaviside is not yet implemented for tensors with different dtypes.",
            "The size of tensor a (2) must match the size of tensor b (7) at non-singleton dimension 6"
        ],
        "torch.Tensor.heaviside_": [
            "heaviside is not yet implemented for tensors with different dtypes.",
            "output with shape [8, 2, 9, 8, 5, 8] doesn't match the broadcast shape [1, 8, 2, 9, 8, 5, 8]"
        ],
        "torch.Tensor.index_select": [
            "Dimension out of range (expected to be in range of [-5, 4], but got 24)",
            "index_select(): Index is supposed to be a vector",
            "index_select(): Expected dtype int32 or int64 for index",
            "index out of range in self"
        ],
        "torch.Tensor.isinf": [],
        "torch.Tensor.isnan": [],
        "torch.Tensor.ldexp": [
            "The size of tensor a (9) must match the size of tensor b (7) at non-singleton dimension 2"
        ],
        "torch.Tensor.ldexp_": [
            "result type Float can't be cast to the desired output type Int",
            "output with shape [] doesn't match the broadcast shape [2, 2, 2, 2, 9, 2, 2]",
            "The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 2"
        ],
        "torch.Tensor.le": [
            "The size of tensor a (7) must match the size of tensor b (6) at non-singleton dimension 3"
        ],
        "torch.Tensor.lerp": [
            "The size of tensor a (9) must match the size of tensor b (7) at non-singleton dimension 6",
            "\"lerp_kernel_scalar\" not implemented for 'Int'",
            "expected dtype float for `end` but got dtype int"
        ],
        "torch.Tensor.lgamma": [],
        "torch.Tensor.lgamma_": [],
        "torch.Tensor.log": [],
        "torch.Tensor.log10": [],
        "torch.Tensor.log10_": [],
        "torch.Tensor.log1p": [],
        "torch.Tensor.log1p_": [],
        "torch.Tensor.log2": [],
        "torch.Tensor.log2_": [],
        "torch.Tensor.log_": [],
        "torch.Tensor.log_softmax": [
            "\"softmax_kernel_impl\" not implemented for 'Int'",
            "Dimension out of range (expected to be in range of [-2, 1], but got 38)"
        ],
        "torch.Tensor.logical_and": [
            "The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 4"
        ],
        "torch.Tensor.logical_not": [],
        "torch.Tensor.logical_not_": [],
        "torch.Tensor.logical_or": [
            "The size of tensor a (2) must match the size of tensor b (9) at non-singleton dimension 6"
        ],
        "torch.Tensor.logical_or_": [
            "The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 5",
            "output with shape [] doesn't match the broadcast shape [9, 9, 9, 5, 9, 9, 9]"
        ],
        "torch.Tensor.logical_xor": [
            "The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 3"
        ],
        "torch.Tensor.logical_xor_": [
            "The size of tensor a (7) must match the size of tensor b (4) at non-singleton dimension 6",
            "output with shape [6, 6] doesn't match the broadcast shape [1, 6, 6]"
        ],
        "torch.Tensor.lt": [
            "The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 5"
        ],
        "torch.Tensor.masked_fill": [
            "masked_fill_ only supports boolean masks, but got mask with dtype float",
            "Too large tensor shape: shape = [8, 9, 9, 9, 9, 9, 9, 9, 9]"
        ],
        "torch.Tensor.masked_fill_": [
            "Too large tensor shape: shape = [6, 9, 9, 9, 9, 9, 9, 9, 9]",
            "masked_fill_ only supports boolean masks, but got mask with dtype float"
        ],
        "torch.Tensor.masked_scatter": [
            "masked_scatter: expected self and source to have same dtypes but gotInt and Float",
            "masked_scatter_ only supports boolean masks, but got mask with dtype Float",
            "Too large tensor shape: shape = [9, 9, 9, 9, 9, 9, 9, 9, 9]"
        ],
        "torch.Tensor.masked_select": [
            "The size of tensor a (7) must match the size of tensor b (5) at non-singleton dimension 8",
            "masked_select: expected BoolTensor for mask",
            "Too large tensor shape: shape = [8, 9, 7, 8, 9, 9, 7, 9, 9]"
        ],
        "torch.Tensor.matmul": [
            "expected scalar type Float but found Int",
            "mat1 and mat2 shapes cannot be multiplied (30240x9 and 1x6)",
            "The size of tensor a (6) must match the size of tensor b (7) at non-singleton dimension 6",
            "both arguments to matmul need to be at least 1D, but they are 2D and 0D",
            "Too large tensor shape: shape = [8, 8, 8, 8, 8, 8, 9, 8, 8]",
            "size mismatch, got input (56), mat (56x8), vec (2)"
        ],
        "torch.Tensor.matrix_exp": [],
        "torch.Tensor.matrix_power": [],
        "torch.Tensor.max": [
            "The size of tensor a (9) must match the size of tensor b (4) at non-singleton dimension 5"
        ],
        "torch.Tensor.maximum": [
            "Too large tensor shape: shape = [4, 9, 9, 9, 9, 9, 9, 9, 9]",
            "The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 4"
        ],
        "torch.Tensor.mean": [],
        "torch.Tensor.median": [],
        "torch.Tensor.min": [
            "Too large tensor shape: shape = [5, 9, 9, 9, 9, 9, 9, 9, 9]"
        ],
        "torch.Tensor.minimum": [
            "The size of tensor a (7) must match the size of tensor b (6) at non-singleton dimension 3"
        ],
        "torch.Tensor.mm": [
            "expected m1 and m2 to have the same dtype, but got: int != float",
            "mat1 and mat2 shapes cannot be multiplied (8x7 and 9x9)",
            "mat2 must be a matrix",
            "self must be a matrix"
        ],
        "torch.Tensor.mode": [
            "Too large tensor shape: shape = [9, 8, 8, 8, 8, 8, 8, 8, 8]",
            "mode(): Expected reduction dim 0 to have non-zero size.",
            "Dimension out of range (expected to be in range of [-1, 0], but got 5)"
        ],
        "torch.Tensor.moveaxis": [
            "negative dimensions are not allowed",
            "movedim: repeated dim in `source` ([-1, -1, -1, -1, -3])",
            "Too large tensor shape: shape = [4, 8, 9, 9, 9, 9, 9, 9, 9]",
            "Dimension out of range (expected to be in range of [-8, 7], but got 9)",
            "movedim: Invalid source or destination dims: source ([-2, -4, -3, -3, -3, -3, -3] dims) should contain the same number of dims as destination ([-3, 9, -1, 9, 3, 4, 9, 9] dims)"
        ],
        "torch.Tensor.movedim": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 37)"
        ],
        "torch.Tensor.msort": [],
        "torch.Tensor.mul": [
            "The size of tensor a (2) must match the size of tensor b (6) at non-singleton dimension 3"
        ],
        "torch.Tensor.mul_": [
            "The size of tensor a (9) must match the size of tensor b (2) at non-singleton dimension 6",
            "output with shape [] doesn't match the broadcast shape [3, 1, 2, 1]",
            "Too large tensor shape: shape = [8, 9, 8, 8, 8, 8, 8, 8, 8]",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.mv": [
            "expected scalar type Int but found Float",
            "size mismatch, got input (3), mat (3x6), vec (8)",
            "Too large tensor shape: shape = [8, 8, 8, 8, 8, 8, 8, 9, 8]",
            "Dimension specified as 0 but tensor has no dimensions"
        ],
        "torch.Tensor.mvlgamma": [
            "p has to be greater than or equal to 1"
        ],
        "torch.Tensor.nan_to_num": [],
        "torch.Tensor.nan_to_num_": [],
        "torch.Tensor.nanmean": [
            "nanmean(): expected input to have floating point or complex dtype but got Int"
        ],
        "torch.Tensor.nanmedian": [],
        "torch.Tensor.narrow": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 4)",
            "narrow() cannot be applied to a 0-dim tensor.",
            "negative dimensions are not allowed",
            "start (7) + length (8) exceeds dimension size (9)."
        ],
        "torch.Tensor.ne": [
            "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 2"
        ],
        "torch.Tensor.neg": [],
        "torch.Tensor.neg_": [],
        "torch.Tensor.new_full": [
            "Need to provide pin_memory allocator to use pin memory."
        ],
        "torch.Tensor.new_ones": [
            "Need to provide pin_memory allocator to use pin memory."
        ],
        "torch.Tensor.new_zeros": [
            "Need to provide pin_memory allocator to use pin memory."
        ],
        "torch.Tensor.nonzero": [],
        "torch.Tensor.norm": [
            "linalg.vector_norm cannot compute the -3 norm on an empty tensor because the operation does not have an identity",
            "linalg.vector_norm: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch.Tensor.permute": [
            "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 2 is not equal to len(dims) = 0",
            "Dimension out of range (expected to be in range of [-2, 1], but got 6)"
        ],
        "torch.Tensor.polygamma_": [
            "result type Float can't be cast to the desired output type Int",
            "polygamma(n, x) does not support negative n."
        ],
        "torch.Tensor.pow": [
            "The size of tensor a (9) must match the size of tensor b (7) at non-singleton dimension 6"
        ],
        "torch.Tensor.pow_": [
            "Integers to negative integer powers are not allowed."
        ],
        "torch.Tensor.prod": [],
        "torch.Tensor.ravel": [],
        "torch.Tensor.reciprocal": [],
        "torch.Tensor.reciprocal_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.relu": [],
        "torch.Tensor.relu_": [],
        "torch.Tensor.remainder_": [
            "The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 2",
            "output with shape [] doesn't match the broadcast shape [2, 4, 4, 4, 4, 4, 4]",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.repeat": [
            "Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor",
            "Trying to create tensor with negative dimension -4: [-4, -4, -4, -4, 5, -4, -4, -4, -4]"
        ],
        "torch.Tensor.reshape": [],
        "torch.Tensor.reshape_as": [
            "shape '[1, 1, 7, 6, 6, 2, 1]' is invalid for input of size 1"
        ],
        "torch.Tensor.resolve_conj": [],
        "torch.Tensor.rot90": [
            "expected total rotation dims == 2, but got dims = 0"
        ],
        "torch.Tensor.round": [],
        "torch.Tensor.round_": [],
        "torch.Tensor.rsqrt": [],
        "torch.Tensor.rsqrt_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.scatter_": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 9)",
            "scatter(): Expected dtype int64 for index",
            "Index tensor must have the same number of dimensions as self tensor",
            "Expected index [8, 5, 6, 2, 4] to be smaller than self [3, 2, 8, 8, 4] apart from dimension 1 and to be smaller size than src [6, 1, 6, 6, 9]"
        ],
        "torch.Tensor.scatter_add_": [
            "Dimension out of range (expected to be in range of [-5, 4], but got 33)",
            "scatter(): Expected dtype int64 for index",
            "Index tensor must have the same number of dimensions as self tensor",
            "Expected index [7, 5, 5, 5, 9, 5, 5] to be smaller than self [9, 8, 8, 8, 8, 8, 8] apart from dimension 3 and to be smaller size than src [7, 6, 6, 6, 9, 6, 6]",
            "index -406719 is out of bounds for dimension 0 with size 2"
        ],
        "torch.Tensor.scatter_reduce_": [
            "Dimension out of range (expected to be in range of [-4, 3], but got 5)",
            "scatter(): Expected dtype int64 for index",
            "Index tensor must have the same number of dimensions as self tensor",
            "Expected index [3, 6, 6, 6, 6] to be smaller than self [1, 9, 9, 9, 9] apart from dimension 1 and to be smaller size than src [7, 4, 4, 4, 4]",
            "reduce argument must be either sum, prod, mean, amax or amin, got gEbJ"
        ],
        "torch.Tensor.sigmoid": [],
        "torch.Tensor.sigmoid_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.sign": [],
        "torch.Tensor.sign_": [],
        "torch.Tensor.sin": [],
        "torch.Tensor.sin_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.sinh": [],
        "torch.Tensor.sinh_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.softmax": [
            "Dimension out of range (expected to be in range of [-5, 4], but got 6)"
        ],
        "torch.Tensor.sort": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 9)"
        ],
        "torch.Tensor.sqrt": [],
        "torch.Tensor.sqrt_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.squeeze": [],
        "torch.Tensor.squeeze_": [],
        "torch.Tensor.sub": [
            "The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 6"
        ],
        "torch.Tensor.sub_": [
            "The size of tensor a (7) must match the size of tensor b (5) at non-singleton dimension 6"
        ],
        "torch.Tensor.sum": [],
        "torch.Tensor.swapaxes_": [
            "Dimension out of range (expected to be in range of [-5, 4], but got 24)"
        ],
        "torch.Tensor.swapdims_": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 8)"
        ],
        "torch.Tensor.t": [
            "t() expects a tensor with <= 2 dimensions, but self is 5D"
        ],
        "torch.Tensor.t_": [
            "t_() expects a tensor with <= 2 dimensions, but self is 3D"
        ],
        "torch.Tensor.tan": [],
        "torch.Tensor.tan_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.tanh": [],
        "torch.Tensor.tanh_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.transpose": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 71)"
        ],
        "torch.Tensor.transpose_": [
            "negative dimensions are not allowed",
            "Dimension out of range (expected to be in range of [-2, 1], but got 24)"
        ],
        "torch.Tensor.tril": [
            "tril: input tensor must have at least 2 dimensions"
        ],
        "torch.Tensor.tril_": [
            "tril: input tensor must have at least 2 dimensions"
        ],
        "torch.Tensor.triu_": [
            "triu: input tensor must have at least 2 dimensions"
        ],
        "torch.Tensor.true_divide_": [
            "result type Float can't be cast to the desired output type Int",
            "The size of tensor a (4) must match the size of tensor b (7) at non-singleton dimension 6"
        ],
        "torch.Tensor.trunc": [],
        "torch.Tensor.trunc_": [],
        "torch.Tensor.type_as": [],
        "torch.Tensor.unfold": [
            "step is 0 but must be > 0",
            "negative dimensions are not allowed",
            "Storage size calculation overflowed with sizes=[1, 9, 9, 9, 9, 14, -4] and strides=[59049, 6561, 729, 81, 9, 1, 1]",
            "maximum size for tensor at dimension 1 is 4 but size is 8",
            "Dimension out of range (expected to be in range of [-2, 1], but got 33)"
        ],
        "torch.Tensor.unique_consecutive": [
            "Dimension specified as 0 but tensor has no dimensions",
            "Dimension out of range (expected to be in range of [-1, 0], but got 57)"
        ],
        "torch.Tensor.unsqueeze": [
            "Dimension out of range (expected to be in range of [-3, 2], but got 91)"
        ],
        "torch.Tensor.unsqueeze_": [
            "Dimension out of range (expected to be in range of [-3, 2], but got 74)"
        ],
        "torch.Tensor.view": [
            "invalid shape dimension -4"
        ],
        "torch.Tensor.view_as": [
            "shape '[7, 9, 8, 7, 7]' is invalid for input of size 6561"
        ],
        "torch.Tensor.xlogy_": [
            "result type Float can't be cast to the desired output type Int",
            "output with shape [5, 5] doesn't match the broadcast shape [1, 5, 5]",
            "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 6"
        ],
        "torch.Tensor.zero_": [],
        "torch._C._fft.fft_fft": [
            "MKL FFT error: Intel MKL DFTI ERROR: Inconsistent configuration parameters",
            "negative dimensions are not allowed",
            "Invalid normalization mode: \"jfhs\"",
            "Invalid number of data points (-2) specified",
            "Dimension specified as 1 but tensor has no dimensions",
            "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
            "fft expects a complex output tensor, but got Float",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._fft.fft_fft2": [
            "Invalid normalization mode: \"wGzo\"",
            "Dimension out of range (expected to be in range of [-2, 1], but got 29)",
            "fftn expects a complex output tensor, but got Float",
            "'complex32'"
        ],
        "torch._C._fft.fft_fftn": [
            "Invalid normalization mode: \"cipM\"",
            "Trying to resize storage that is not resizable",
            "fftn expects a complex output tensor, but got Float"
        ],
        "torch._C._fft.fft_fftshift": [],
        "torch._C._fft.fft_hfft": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 6)",
            "Invalid number of data points (0) specified",
            "Trying to resize storage that is not resizable",
            "Dimension specified as 0 but tensor has no dimensions",
            "Invalid normalization mode: \"zxLX\""
        ],
        "torch._C._fft.fft_hfft2": [
            "hfftn must transform at least one axis",
            "Dimension out of range (expected to be in range of [-2, 1], but got 6)",
            "Dimension specified as 3 but tensor has no dimensions"
        ],
        "torch._C._fft.fft_hfftn": [
            "hfftn must transform at least one axis",
            "Invalid normalization mode: \"UbxY\"",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._fft.fft_ifft": [
            "Dimension out of range (expected to be in range of [-2, 1], but got -3)",
            "Invalid normalization mode: \"TNkG\"",
            "Invalid number of data points (-3) specified",
            "ifft expects a complex output tensor, but got Float",
            "Found dtype ComplexDouble but expected ComplexFloat",
            "Trying to resize storage that is not resizable",
            "Dimension specified as 9 but tensor has no dimensions"
        ],
        "torch._C._fft.fft_ifft2": [
            "Invalid normalization mode: \"Gyck\"",
            "Dimension out of range (expected to be in range of [-1, 0], but got 3)",
            "Dimension specified as -3 but tensor has no dimensions",
            "Trying to resize storage that is not resizable",
            "fftn expects a complex output tensor, but got Float"
        ],
        "torch._C._fft.fft_ifftn": [
            "Invalid normalization mode: \"CMPv\"",
            "fftn expects a complex output tensor, but got Float",
            "'complex32'"
        ],
        "torch._C._fft.fft_ifftshift": [],
        "torch._C._fft.fft_ihfft": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 6)",
            "Invalid normalization mode: \"kElB\"",
            "ihfft expects a complex output tensor, but got Float"
        ],
        "torch._C._fft.fft_ihfft2": [
            "ihfftn must transform at least one axis",
            "Dimension out of range (expected to be in range of [-2, 1], but got 6)"
        ],
        "torch._C._fft.fft_ihfftn": [
            "ihfftn must transform at least one axis",
            "Invalid normalization mode: \"u3WjXW76AK\"",
            "ihfftn expects a complex output tensor, but got Float"
        ],
        "torch._C._fft.fft_irfft": [
            "Dimension out of range (expected to be in range of [-2, 1], but got -4)",
            "Invalid number of data points (0) specified",
            "Dimension specified as 78 but tensor has no dimensions",
            "Invalid normalization mode: \"CiVX\"",
            "Trying to resize storage that is not resizable",
            "irfft expects a floating point output tensor, but got Int"
        ],
        "torch._C._fft.fft_irfft2": [
            "irfftn must transform at least one axis",
            "Dimension out of range (expected to be in range of [-2, 1], but got 6)"
        ],
        "torch._C._fft.fft_irfftn": [
            "Invalid normalization mode: \"yYGc\"",
            "Trying to resize storage that is not resizable",
            "irfftn must transform at least one axis"
        ],
        "torch._C._fft.fft_rfft": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
            "Invalid normalization mode: \"lDny\"",
            "rfft expects a complex output tensor, but got Float",
            "Trying to resize storage that is not resizable",
            "Dimension specified as 0 but tensor has no dimensions"
        ],
        "torch._C._fft.fft_rfft2": [
            "rfftn must transform at least one axis",
            "Dimension out of range (expected to be in range of [-2, 1], but got 6)",
            "Invalid normalization mode: \"mqLo\"",
            "rfftn expects a complex output tensor, but got Float"
        ],
        "torch._C._fft.fft_rfftn": [
            "Invalid normalization mode: \"CwJA\"",
            "rfftn must transform at least one axis",
            "rfftn expects a complex output tensor, but got Float",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._linalg.linalg_cholesky": [
            "linalg.cholesky: A must be batches of square matrices, but they are 3 by 2 matrices",
            "linalg.cholesky: The input tensor A must have at least 2 dimensions.",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._linalg.linalg_cholesky_ex": [],
        "torch._C._linalg.linalg_cond": [
            "linalg.svd: Expected a floating point or complex tensor as input. Got Int",
            "linalg.cond: The input tensor must have at least 2 dimensions.",
            "linalg.cond got an invalid norm type: 4",
            "linalg.cond(ord=fro): A must be batches of square matrices, but they are 2 by 9 matrices",
            "linalg.inv: Low precision dtypes not supported. Got Half",
            "linalg.cond: Expected result to be safely castable from Float dtype, but got result with dtype Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._linalg.linalg_det": [
            "linalg.det: Expected a floating point or complex tensor as input. Got Int",
            "linalg.det: A must be batches of square matrices, but they are 5 by 7 matrices",
            "linalg.det: The input tensor A must have at least 2 dimensions.",
            "Expected out tensor to have dtype c10::complex<float>, but got float instead",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._linalg.linalg_eig": [],
        "torch._C._linalg.linalg_eigh": [
            "Expected UPLO argument to be 'L' or 'U', but got Ypgg"
        ],
        "torch._C._linalg.linalg_eigvals": [
            "linalg.eigvals: A must be batches of square matrices, but they are 1 by 4 matrices",
            "linalg.eigvals: The input tensor A must have at least 2 dimensions.",
            "torch.linalg.eigvals: Expected eigenvalues to be safely castable from ComplexFloat dtype, but got eigenvalues with dtype Float",
            "'complex32'"
        ],
        "torch._C._linalg.linalg_eigvalsh": [
            "Expected UPLO argument to be 'L' or 'U', but got pGNG",
            "Expected out tensor to have dtype float, but got c10::Half instead",
            "negative dimensions are not allowed",
            "Trying to resize storage that is not resizable",
            "linalg.eigh: The input tensor A must have at least 2 dimensions.",
            "linalg.eigh: A must be batches of square matrices, but they are 1 by 6 matrices"
        ],
        "torch._C._linalg.linalg_inv": [
            "linalg.inv: Expected a floating point or complex tensor as input. Got Int",
            "linalg.inv: A must be batches of square matrices, but they are 5 by 8 matrices",
            "linalg.inv: The input tensor A must have at least 2 dimensions."
        ],
        "torch._C._linalg.linalg_inv_ex": [
            "linalg.inv: A must be batches of square matrices, but they are 6 by 2 matrices",
            "linalg.inv: The input tensor A must have at least 2 dimensions."
        ],
        "torch._C._linalg.linalg_ldl_factor": [
            "torch.linalg.ldl_factor_ex: A must be batches of square matrices, but they are 9 by 3 matrices",
            "torch.linalg.ldl_factor_ex: The input tensor A must have at least 2 dimensions."
        ],
        "torch._C._linalg.linalg_ldl_factor_ex": [
            "torch.linalg.ldl_factor_ex: A must be batches of square matrices, but they are 7 by 8 matrices",
            "torch.linalg.ldl_factor_ex: The input tensor A must have at least 2 dimensions."
        ],
        "torch._C._linalg.linalg_lstsq": [
            "torch.linalg.lstsq: input.dim() must be greater or equal to other.dim() and (input.dim() - other.dim()) <= 1",
            "torch.linalg.lstsq: input must have at least 2 dimensions.",
            "torch.linalg.lstsq: input.size(-2) should match other.size(-2)",
            "torch.linalg.lstsq: parameter `driver` should be one of (gels, gelsy, gelsd, gelss)",
            "The size of tensor a (7) must match the size of tensor b (5) at non-singleton dimension 1",
            "torch.linalg.lstsq: Expected input and other to have the same dtype, but got input's dtype Int and other's dtype Float"
        ],
        "torch._C._linalg.linalg_lu": [
            "\"lu_cpu\" not implemented for 'Int'",
            "linalg.lu: Expected tensor with 2 or more dimensions. Got size: [] instead",
            "linalg.lu_factor: LU without pivoting is not implemented on the CPU"
        ],
        "torch._C._linalg.linalg_lu_factor": [
            "\"lu_cpu\" not implemented for 'Int'",
            "torch.lu_factor: Expected tensor with 2 or more dimensions. Got size: [] instead",
            "linalg.lu_factor: LU without pivoting is not implemented on the CPU"
        ],
        "torch._C._linalg.linalg_lu_factor_ex": [
            "'complex32'",
            "\"lu_cpu\" not implemented for 'Bool'",
            "\"lu_cpu\" not implemented for 'Long'",
            "\"lu_cpu\" not implemented for 'Int'",
            "torch.lu_factor: Expected tensor with 2 or more dimensions. Got size: [] instead",
            "linalg.lu_factor: LU without pivoting is not implemented on the CPU"
        ],
        "torch._C._linalg.linalg_matrix_norm": [
            "linalg.matrix_norm: Order 0 not supported.",
            "linalg.matrix_norm: dim must be a 2-tuple. Got ",
            "Dimension out of range (expected to be in range of [-3, 2], but got 6)",
            "linalg.matrix_norm: dims must be different. Got (3, 3)",
            "linalg.matrix_norm expected out tensor dtype Double but got: Float",
            "linalg.matrix_norm: The input tensor A must have at least 2 dimensions.",
            "Trying to resize storage that is not resizable",
            "linalg.matrix_norm: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch._C._linalg.linalg_matrix_power": [
            "linalg.matrix_power: The input tensor A must have at least 2 dimensions.",
            "linalg.inv: Expected a floating point or complex tensor as input. Got Int",
            "Expected out tensor to have dtype int, but got float instead",
            "Trying to resize storage that is not resizable",
            "linalg.matrix_power: A must be batches of square matrices, but they are 9 by 4 matrices"
        ],
        "torch._C._linalg.linalg_matrix_rank": [
            "\"linalg_eigh_cpu\" not implemented for 'Int'",
            "linalg.svd: Expected a floating point or complex tensor as input. Got Int",
            "torch.linalg.matrix_rank: The input tensor input must have at least 2 dimensions.",
            "The size of tensor a (5) must match the size of tensor b (8) at non-singleton dimension 6",
            "linalg.eigh: A must be batches of square matrices, but they are 4 by 3 matrices",
            "This function doesn't handle types other than float and double",
            "negative dimensions are not allowed",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._linalg.linalg_norm": [
            "linalg.matrix_norm: Order 18 not supported.",
            "linalg.norm: If dim is not specified but ord is, the input must be 1D or 2D. Got 4D.",
            "linalg.matrix_norm: Expected a floating point or complex tensor as input. Got Int",
            "linalg.matrix_norm: The input tensor A must have at least 2 dimensions.",
            "linalg.norm expected out tensor dtype Double but got: Float",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._linalg.linalg_pinv": [
            "linalg.pinv(Int{[3, 5, 5, 5, 5, 5, 5]}): expected a tensor with 2 or more dimensions of float, double, cfloat or cdouble types",
            "linalg.eigh: A must be batches of square matrices, but they are 1 by 3 matrices",
            "The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 6",
            "Trying to resize storage that is not resizable",
            "linalg.pinv: Expected result to be safely castable from ComplexFloat dtype, but got result with dtype Float",
            "negative dimensions are not allowed"
        ],
        "torch._C._linalg.linalg_qr": [
            "linalg.qr: Expected a floating point or complex tensor as input. Got Int",
            "linalg.qr: The input tensor A must have at least 2 dimensions.",
            "qr received unrecognized mode 'RXoC' but expected one of 'reduced' (default), 'r', or 'complete'"
        ],
        "torch._C._linalg.linalg_slogdet": [
            "linalg.slogdet: Expected a floating point or complex tensor as input. Got Int",
            "linalg.slogdet: A must be batches of square matrices, but they are 3 by 2 matrices",
            "linalg.slogdet: The input tensor A must have at least 2 dimensions."
        ],
        "torch._C._linalg.linalg_solve": [
            "linalg.solve: A must be batches of square matrices, but they are 4 by 6 matrices",
            "linalg.solve: The input tensor A must have at least 2 dimensions.",
            "linalg.solve: Incompatible shapes of A and B for the equation XA = B (5x5 and 8x8)",
            "linalg.solve: Expected a floating point or complex tensor as input. Got Int",
            "negative dimensions are not allowed",
            "linalg.solve: Expected A and B to have the same dtype, but found A of type Float and B of type Int instead",
            "Trying to resize storage that is not resizable",
            "The size of tensor a (9) must match the size of tensor b (4) at non-singleton dimension 0"
        ],
        "torch._C._linalg.linalg_solve_ex": [
            "linalg.solve: A must be batches of square matrices, but they are 1 by 7 matrices",
            "linalg.solve: The input tensor A must have at least 2 dimensions.",
            "linalg.solve: Incompatible shapes of A and B for the equation AX = B (5x5 and 6x6)"
        ],
        "torch._C._linalg.linalg_solve_triangular": [
            "linalg.solve_triangular: Incompatible shapes of A and B for the equation XA = B (8x8 and 1x1)",
            "linalg.solve_triangular: The input tensor B must have at least 2 dimensions.",
            "linalg.solve_triangular: A must be batches of square matrices, but they are 6 by 7 matrices",
            "negative dimensions are not allowed",
            "\"triangular_solve_cpu\" not implemented for 'Half'",
            "\"triangular_solve_cpu\" not implemented for 'Int'"
        ],
        "torch._C._linalg.linalg_svd": [
            "torch.linalg.svd: keyword argument `driver=` is only supported on CUDA inputs with cuSOLVER backend."
        ],
        "torch._C._linalg.linalg_svdvals": [
            "torch.linalg.svd: keyword argument `driver=` is only supported on CUDA inputs with cuSOLVER backend."
        ],
        "torch._C._linalg.linalg_tensorinv": [
            "ArrayRef: invalid slice, N = 7; size = 2",
            "Expected a strictly positive integer for 'ind', but got 0"
        ],
        "torch._C._linalg.linalg_tensorsolve": [
            "cannot create std::vector larger than max_size()"
        ],
        "torch._C._linalg.linalg_vander": [
            "N must be greater than 1."
        ],
        "torch._C._linalg.linalg_vecdot": [
            "linalg.vecdot: Expected a floating point or complex tensor as input. Got Int",
            "linalg.vecdot: Expected x and y to have the same dtype, but found x of type Float and y of type Double instead",
            "The size of tensor a (2) must match the size of tensor b (8) at non-singleton dimension 6",
            "Dimension out of range (expected to be in range of [-7, 6], but got 86)",
            "linalg.vecdot: Expected out of dtypeFloat but found Int"
        ],
        "torch._C._linalg.linalg_vector_norm": [
            "linalg.vector_norm cannot compute the -3 norm on an empty tensor because the operation does not have an identity",
            "linalg.vector_norm: Expected a floating point or complex tensor as input. Got Int",
            "Expected out tensor to have dtype float, but got int instead",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._nn.adaptive_max_pool2d": [
            "adaptive_max_pool2d(): Expected input to have non-zero size for non-batch dimensions, but input has sizes [0, 0, 7, 9] with dimension 1 being empty",
            "'complex32'",
            "\"adaptive_max_pool2d\" not implemented for 'Bool'",
            "\"adaptive_max_pool2d\" not implemented for 'Int'",
            "Trying to create tensor with negative dimension -2: [8, 8, 5, -2]",
            "adaptive_max_pool2d(): internal error: output_size.size() must be 2",
            "adaptive_max_pool2d(): Expected 3D or 4D tensor, but got: [8, 5, 8, 8, 8, 8, 8]"
        ],
        "torch._C._nn.adaptive_max_pool3d": [
            "adaptive_max_pool3d(): Expected 4D or 5D tensor, but got: [1, 1]",
            "adaptive_max_pool3d(): internal error: output_size.size() must be 3",
            "adaptive_max_pool3d(): Expected input to have non-zero size for non-batch dimensions, but input has sizes [9, 8, 7, 0, 6] with dimension 3 being empty",
            "\"adaptive_max_pool3d_cpu\" not implemented for 'Int'"
        ],
        "torch._C._nn.avg_pool2d": [
            "avg_pool2d: kernel_size must either be a single int, or a tuple of two ints",
            "pad should be at most half of effective kernel size, but got pad=8, kernel_size=1 and dilation=1",
            "stride should not be zero",
            "pad must be non-negative, but got pad: -1",
            "stride should be greater than zero, but got dH: 8 dW: -1",
            "Dimension specified as -3 but tensor has no dimensions"
        ],
        "torch._C._nn.avg_pool3d": [
            "avg_pool3d: kernel_size must be a single int, or a tuple of three ints",
            "non-empty 4D or 5D (batch mode) tensor expected for input",
            "pad must be non-negative, but got pad: -3",
            "pad should be at most half of effective kernel size, but got pad=3, kernel_size=5 and dilation=1",
            "stride should be greater than zero, but got dT: 5 dH: -1 dW: -1"
        ],
        "torch._C._nn.flatten_dense_tensors": [
            "torch.cat(): expected a non-empty list of Tensors"
        ],
        "torch._C._nn.gelu": [
            "\"GeluKernelImpl\" not implemented for 'Int'",
            "approximate argument must be either none or tanh.",
            "Found dtype Int but expected Float",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._nn.gelu_": [
            "\"GeluKernelImpl\" not implemented for 'Int'",
            "approximate argument must be either none or tanh."
        ],
        "torch._C._nn.huber_loss": [
            "\"huber_cpu\" not implemented for 'Int'",
            "result type Float can't be cast to the desired output type Int",
            "huber_loss does not support non-positive values for delta.",
            "The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 2",
            "Trying to resize storage that is not resizable",
            "\"huber_cpu\" not implemented for 'Long'",
            "\"huber_cpu\" not implemented for 'Bool'",
            "'complex32'"
        ],
        "torch._C._nn.l1_loss": [
            "The size of tensor a (3) must match the size of tensor b (8) at non-singleton dimension 6"
        ],
        "torch._C._nn.log_sigmoid": [
            "\"log_sigmoid_cpu\" not implemented for 'Int'",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._nn.max_pool2d_with_indices": [
            "max_pool2d: kernel_size must either be a single int, or a tuple of two ints",
            "non-empty 3D or 4D (batch mode) tensor expected for input"
        ],
        "torch._C._nn.mse_loss": [
            "The size of tensor a (2) must match the size of tensor b (9) at non-singleton dimension 4"
        ],
        "torch._C._nn.pad_sequence": [
            "Too large tensor shape: shape = [7, 9, 9, 9, 9, 9, 9, 9, 9]",
            "The size of tensor a (7) must match the size of tensor b (9) at non-singleton dimension 8",
            "received an empty list of sequences"
        ],
        "torch._C._nn.reflection_pad1d": [
            "negative dimensions are not allowed",
            "Expected 2D or 3D (batch mode) tensor with possibly 0 batch size and other non-zero dimensions for input, but got: [7, 0, 0]",
            "Argument #4: Padding size should be less than the corresponding input dimension, but got: padding (8, 8) at dimension 1 of input [7, 8, 8, 8, 8, 8, 8]",
            "padding size is expected to be 2, but got: 0",
            "\"reflection_pad1d\" not implemented for 'Half'",
            "Expected out tensor to have dtype float, but got c10::Half instead",
            "Trying to resize storage that is not resizable",
            "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
            "Dimension specified as 0 but tensor has no dimensions"
        ],
        "torch._C._nn.smooth_l1_loss": [
            "The size of tensor a (7) must match the size of tensor b (6) at non-singleton dimension 3"
        ],
        "torch._C._nn.soft_margin_loss": [
            "The size of tensor a (7) must match the size of tensor b (6) at non-singleton dimension 2",
            "result type Float can't be cast to the desired output type Int",
            "Found dtype Int but expected Float"
        ],
        "torch._C._nn.softplus": [
            "\"softplus_cpu\" not implemented for 'Int'",
            "Found dtype Float but expected Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._nn.softshrink": [
            "\"softshrink_cpu\" not implemented for 'Int'",
            "lambda must be greater or equal to 0, but found to be -1.",
            "Trying to resize storage that is not resizable",
            "Found dtype Float but expected Int"
        ],
        "torch._C._nn.upsample_bicubic2d": [
            "Non-empty 4D data tensor expected but got a tensor with sizes [7, 0, 8, 9]",
            "\"compute_indices_weights_cubic\" not implemented for 'ComplexFloat'",
            "\"compute_indices_weights_cubic\" not implemented for 'Long'",
            "\"compute_indices_weights_cubic\" not implemented for 'Int'",
            "Input and output sizes should be greater than 0, but got input (H: 7, W: 8) output (H: -3, W: 3)",
            "It is expected output_size equals to 2, but got size 0",
            "Expected out tensor to have dtype int, but got float instead",
            "\"compute_indices_weights_cubic\" not implemented for 'Char'"
        ],
        "torch._C._nn.upsample_bilinear2d": [
            "Expected static_cast<int64_t>(output_size->size()) == spatial_dimensions to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
            "Non-empty 4D data tensor expected but got a tensor with sizes [9, 0, 3, 8]",
            "\"upsample_bilinear2d_channels_last\" not implemented for 'Int'",
            "Input and output sizes should be greater than 0, but got input (H: 7, W: 1) output (H: -3, W: 3)",
            "It is expected output_size equals to 2, but got size 0"
        ],
        "torch._C._nn.upsample_linear1d": [
            "Non-empty 3D data tensor expected but got a tensor with sizes [8, 0, 1]",
            "\"compute_indices_weights_linear\" not implemented for 'Bool'",
            "\"compute_indices_weights_linear\" not implemented for 'Int'",
            "Input and output sizes should be greater than 0, but got input (W: 8) and output (W: -3)",
            "It is expected output_size equals to 1, but got size 0",
            "Expected out tensor to have dtype float, but got int instead",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._nn.upsample_nearest1d": [
            "Expected static_cast<int64_t>(scale_factors->size()) == spatial_dimensions to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
            "Non-empty 3D data tensor expected but got a tensor with sizes [8, 0, 7]",
            "\"compute_indices_weights_nearest\" not implemented for 'Int'",
            "Input and output sizes should be greater than 0, but got input (W: 8) and output (W: -3)",
            "It is expected output_size equals to 1, but got size 0"
        ],
        "torch._C._nn.upsample_nearest2d": [
            "'complex32'",
            "\"upsample_nearest2d_channels_last\" not implemented for 'Bool'",
            "\"upsample_nearest2d_channels_last\" not implemented for 'Long'",
            "Non-empty 4D data tensor expected but got a tensor with sizes [0, 0, 1, 1]",
            "Input and output sizes should be greater than 0, but got input (H: 1, W: 0) output (H: 1, W: 1)",
            "\"upsample_nearest2d_channels_last\" not implemented for 'Int'",
            "It is expected input_size equals to 4, but got size 3"
        ],
        "torch._C._nn.upsample_nearest3d": [
            "It is expected output_size equals to 3, but got size 0",
            "Input and output sizes should be greater than 0, but got input (D: 8, H: 2, W: 9) output (D: 5, H: 3, W: -3)",
            "'complex32'",
            "\"compute_indices_weights_nearest\" not implemented for 'Bool'",
            "\"compute_indices_weights_nearest\" not implemented for 'Int'"
        ],
        "torch._C._nn.upsample_trilinear3d": [
            "It is expected output_size equals to 3, but got size 0",
            "Input and output sizes should be greater than 0, but got input (D: 9, H: 4, W: 2) output (D: 3, H: -2, W: 3)",
            "\"compute_indices_weights_linear\" not implemented for 'Int'",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_airy_ai": [
            "\"airy_ai_cpu\" not implemented for 'Half'",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_bessel_j0": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_bessel_j1": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_bessel_y0": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_bessel_y1": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_chebyshev_polynomial_t": [
            "The size of tensor a (5) must match the size of tensor b (7) at non-singleton dimension 5",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_chebyshev_polynomial_u": [
            "The size of tensor a (0) must match the size of tensor b (3) at non-singleton dimension 3"
        ],
        "torch._C._special.special_entr": [
            "negative dimensions are not allowed",
            "result type Float can't be cast to the desired output type Long",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_erfcx": [
            "\"erfcx_cpu\" not implemented for 'Half'",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_hermite_polynomial_h": [
            "The size of tensor a (8) must match the size of tensor b (9) at non-singleton dimension 6",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable",
            "negative dimensions are not allowed"
        ],
        "torch._C._special.special_hermite_polynomial_he": [
            "Trying to resize storage that is not resizable",
            "The size of tensor a (5) must match the size of tensor b (9) at non-singleton dimension 6",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch._C._special.special_i0e": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_i1": [
            "\"i1_cpu\" not implemented for 'Half'",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_i1e": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_laguerre_polynomial_l": [
            "The size of tensor a (4) must match the size of tensor b (9) at non-singleton dimension 5",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_log_ndtr": [
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch._C._special.special_modified_bessel_i0": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_modified_bessel_i1": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_modified_bessel_k0": [
            "negative dimensions are not allowed",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_modified_bessel_k1": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_ndtr": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_ndtri": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_scaled_modified_bessel_k0": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_scaled_modified_bessel_k1": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_spherical_bessel_j0": [
            "\"spherical_bessel_j0_cpu\" not implemented for 'Half'",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_xlog1py": [
            "The size of tensor a (4) must match the size of tensor b (7) at non-singleton dimension 2",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch._C._special.special_zeta": [
            "The size of tensor a (7) must match the size of tensor b (6) at non-singleton dimension 3"
        ],
        "torch.abs": [
            "Found dtype Float but expected Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.acos": [
            "result type Float can't be cast to the desired output type Short",
            "Trying to resize storage that is not resizable"
        ],
        "torch.acosh": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.adaptive_avg_pool1d": [
            "adaptive_avg_pool1d() argument 'output_size' should contain one int (got 2)",
            "adaptive_avg_pool2d: elements of output_size must be greater than or equal to 0 but received {1, -1}",
            "Expected 2 to 3 dimensions, but got 1-dimensional tensor for argument #1 'self' (while checking arguments for adaptive_avg_pool1d)"
        ],
        "torch.adaptive_max_pool1d": [
            "adaptive_max_pool1d(): Expected input to have non-zero size for non-batch dimensions, but input has sizes [2, 0, 9] with dimension 1 being empty",
            "\"adaptive_max_pool2d\" not implemented for 'Int'",
            "Trying to create tensor with negative dimension -3: [3, 7, 1, -3]",
            "Expected 2 to 3 dimensions, but got 7-dimensional tensor for argument #1 'self' (while checking arguments for adaptive_max_pool1d)",
            "adaptive_max_pool1d() argument 'output_size' should contain one int (got 0)"
        ],
        "torch.add": [
            "The size of tensor a (7) must match the size of tensor b (5) at non-singleton dimension 6",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.addcdiv": [
            "Integer division with addcdiv is no longer supported, and in a future  release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes.",
            "The size of tensor a (2) must match the size of tensor b (9) at non-singleton dimension 6",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.addcmul": [
            "The size of tensor a (9) must match the size of tensor b (4) at non-singleton dimension 4",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.addmm": [
            "mat1 must be a matrix, got 4-D tensor",
            "mat1 and mat2 shapes cannot be multiplied (3x6 and 2x8)",
            "expand(torch.FloatTensor{[3, 9, 5, 8, 9, 7, 9]}, size=[6, 8]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (7)"
        ],
        "torch.addmv": [
            "expected scalar type Float but found Int",
            "size mismatch, got input (8), mat (9x5), vec (7)",
            "vector + matrix @ vector expected, got 7, 7, 7",
            "Dimension specified as 0 but tensor has no dimensions"
        ],
        "torch.addr": [
            "The expanded size of the tensor (3) must match the existing size (6) at non-singleton dimension 1.  Target sizes: [8, 3].  Tensor sizes: [6, 6]",
            "addr: Expected 1-D argument vec1, but got 7-D"
        ],
        "torch.all": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 9)",
            "all only supports bool tensor for result, got: Float",
            "Trying to resize storage that is not resizable",
            "negative dimensions are not allowed"
        ],
        "torch.amax": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 92)",
            "Expected the dtype for input and out to match, but got Int for input's dtype and Float for out's dtype."
        ],
        "torch.amin": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 55)",
            "amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument."
        ],
        "torch.aminmax": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 55)"
        ],
        "torch.angle": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.any": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
            "any only supports bool tensor for result, got: Float",
            "negative dimensions are not allowed",
            "Trying to resize storage that is not resizable"
        ],
        "torch.argmax": [
            "argmax(): Expected reduction dim 0 to have non-zero size.",
            "Dimension out of range (expected to be in range of [-2, 1], but got 100)"
        ],
        "torch.argmin": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 8)",
            "Expected out tensor to have dtype long int, but got float instead",
            "Trying to resize storage that is not resizable"
        ],
        "torch.argsort": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 5)"
        ],
        "torch.argwhere": [],
        "torch.as_strided": [],
        "torch.as_tensor": [],
        "torch.asin": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.asinh": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.atan": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.atan2": [
            "The size of tensor a (6) must match the size of tensor b (7) at non-singleton dimension 2",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.atanh": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.atleast_1d": [],
        "torch.atleast_2d": [],
        "torch.atleast_3d": [],
        "torch.bernoulli": [
            "bernoulli_ expects p to be in [0, 1], but got p=3"
        ],
        "torch.binary_cross_entropy_with_logits": [
            "output with shape [] doesn't match the broadcast shape [1, 1, 1, 1]",
            "\"log_sigmoid_cpu\" not implemented for 'Int'",
            "result type Float can't be cast to the desired output type Int",
            "The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 3"
        ],
        "torch.bitwise_and": [
            "\"bitwise_and_cpu\" not implemented for 'Float'",
            "The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 2",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Short",
            "\"bitwise_and_cpu\" not implemented for 'Half'"
        ],
        "torch.bitwise_left_shift": [
            "\"lshift_cpu\" not implemented for 'Float'",
            "The size of tensor a (2) must match the size of tensor b (5) at non-singleton dimension 5",
            "Trying to resize storage that is not resizable",
            "\"lshift_cpu\" not implemented for 'Half'"
        ],
        "torch.bitwise_not": [
            "\"bitwise_not_cpu\" not implemented for 'Float'",
            "\"bitwise_not_cpu\" not implemented for 'Half'",
            "Trying to resize storage that is not resizable",
            "Found dtype Float but expected Long"
        ],
        "torch.bitwise_or": [
            "\"bitwise_or_cpu\" not implemented for 'Float'",
            "The size of tensor a (4) must match the size of tensor b (7) at non-singleton dimension 2",
            "Trying to resize storage that is not resizable"
        ],
        "torch.bitwise_right_shift": [
            "\"rshift_cpu\" not implemented for 'Float'",
            "The size of tensor a (8) must match the size of tensor b (9) at non-singleton dimension 4",
            "Trying to resize storage that is not resizable",
            "negative dimensions are not allowed"
        ],
        "torch.bitwise_xor": [
            "The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 4",
            "\"bitwise_xor_cpu\" not implemented for 'Float'",
            "\"bitwise_xor_cpu\" not implemented for 'Half'",
            "result type Float can't be cast to the desired output type Long",
            "Trying to resize storage that is not resizable"
        ],
        "torch.block_diag": [
            "expected Tensor as element 0 in argument 0, but got list",
            "torch.block_diag: Input tensors must have 2 or fewer dimensions. Input 0 has 7 dimensions"
        ],
        "torch.bmm": [
            "batch2 must be a 3D tensor",
            "Expected size for first two dimensions of batch2 tensor to be: [3, 7] but got: [5, 3].",
            "Trying to resize storage that is not resizable",
            "Expected out tensor to have dtype float, but got int instead"
        ],
        "torch.broadcast_tensors": [],
        "torch.broadcast_to": [
            "The expanded size of the tensor (1) must match the existing size (4) at non-singleton dimension 8.  Target sizes: [2, 1, 9, 1, 1, 1, 1, 1, 1].  Tensor sizes: [4]",
            "numel: integer multiplication overflow",
            "expand(torch.FloatTensor{[1, 1]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (2)"
        ],
        "torch.bucketize": [
            "boundaries tensor must be 1 dimension, but got dim(3)",
            "torch.searchsorted(): output tensor's dtype is wrong, it can only be Int(int32) or Long(int64) depending on whether out_int32 flag is True, but we got output tensor's dtype Float and out_int32 flag is False",
            "Trying to resize storage that is not resizable"
        ],
        "torch.cartesian_prod": [
            "Expect a 1D vector, but got shape [0, 0]",
            "meshgrid expects a non-empty TensorList"
        ],
        "torch.cat": [
            "torch.cat(): expected a non-empty list of Tensors",
            "Dimension out of range (expected to be in range of [-2, 1], but got 6)",
            "Name 'AnIx' not found in Tensor[None, None, None, None, None, None, None, None, None].",
            "Trying to resize storage that is not resizable",
            "Too large tensor shape: shape = [9, 9, 9, 9, 9, 9, 9, 9, 9]"
        ],
        "torch.ceil": [
            "Found dtype Float but expected Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.cholesky": [
            "cholesky: A must be batches of square matrices, but they are 1 by 7 matrices"
        ],
        "torch.cholesky_inverse": [
            "cholesky_inverse: A must be batches of square matrices, but they are 8 by 6 matrices",
            "cholesky_inverse: The input tensor A must have at least 2 dimensions.",
            "Trying to resize storage that is not resizable"
        ],
        "torch.chunk": [
            "chunk expects at least a 1-dimensional tensor",
            "chunk expects `chunks` to be greater than 0, got: -3",
            "Dimension out of range (expected to be in range of [-1, 0], but got -4)"
        ],
        "torch.clamp": [
            "The size of tensor a (6) must match the size of tensor b (7) at non-singleton dimension 3",
            "Trying to resize storage that is not resizable"
        ],
        "torch.clamp_": [
            "The size of tensor a (5) must match the size of tensor b (9) at non-singleton dimension 3"
        ],
        "torch.clamp_max": [
            "The size of tensor a (7) must match the size of tensor b (6) at non-singleton dimension 3",
            "Found dtype Float but expected Int",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.clamp_max_": [
            "result type Float can't be cast to the desired output type Int",
            "The size of tensor a (8) must match the size of tensor b (6) at non-singleton dimension 3"
        ],
        "torch.clamp_min": [
            "The size of tensor a (6) must match the size of tensor b (7) at non-singleton dimension 3",
            "Found dtype Float but expected Int",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.clamp_min_": [
            "The size of tensor a (9) must match the size of tensor b (6) at non-singleton dimension 3"
        ],
        "torch.column_stack": [
            "column_stack expects a non-empty TensorList",
            "Tensors must have same number of dimensions: got 6 and 2"
        ],
        "torch.combinations": [
            "Expect a non-negative number, but got -3",
            "Expect a 1D vector, but got shape [6, 6]"
        ],
        "torch.complex": [
            "Expected both inputs to be Half, Float or Double tensors but got Float and Int",
            "The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 3",
            "Expected object of scalar type ComplexFloat but got scalar type Float for argument 'out'"
        ],
        "torch.conj": [],
        "torch.conj_physical": [
            "negative dimensions are not allowed",
            "Found dtype Int but expected Float",
            "Trying to resize storage that is not resizable"
        ],
        "torch.constant_pad_nd": [
            "Length of pad must be even but instead it equals 1"
        ],
        "torch.copysign": [
            "The size of tensor a (7) must match the size of tensor b (8) at non-singleton dimension 2",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.corrcoef": [],
        "torch.cos": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.cosh": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.cosine_embedding_loss": [
            "The size of tensor a (9) must match the size of tensor b (7) at non-singleton dimension 0",
            "0D target tensor expects 1D input tensors, but found inputs with sizes [7, 8, 8, 9] and [7, 9, 9, 9, 9, 9].",
            "0D or 1D target tensor expected, multi-target not supported"
        ],
        "torch.cosine_similarity": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 9)",
            "The size of tensor a (7) must match the size of tensor b (4) at non-singleton dimension 1"
        ],
        "torch.count_nonzero": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 33)"
        ],
        "torch.cov": [
            "cov(): expected input to have two or fewer dimensions but got an input with 7 dimensions",
            "cov(): expected fweights to have one or fewer dimensions but got fweights with 2 dimensions"
        ],
        "torch.cross": [
            "expected scalar type Float but found Int",
            "Dimension out of range (expected to be in range of [-3, 2], but got 56)",
            "The size of tensor a (8) must match the size of tensor b (7) at non-singleton dimension 2",
            "linalg.cross: inputs dimension 1 must have length 3. Got 3 and 2",
            "no dimension of size 3 in input",
            "linalg.cross: inputs must have the same number of dimensions.",
            "Trying to resize storage that is not resizable",
            "Expected out tensor to have dtype float, but got int instead",
            "Dimension specified as 0 but tensor has no dimensions"
        ],
        "torch.cummax": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 7)"
        ],
        "torch.cummin": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 8)"
        ],
        "torch.cumprod": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 8)",
            "Trying to resize storage that is not resizable"
        ],
        "torch.cumsum": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 8)",
            "Trying to resize storage that is not resizable"
        ],
        "torch.cumulative_trapezoid": [
            "The size of tensor a (5) must match the size of tensor b (0) at non-singleton dimension 0",
            "Dimension out of range (expected to be in range of [-7, 6], but got 8)"
        ],
        "torch.deg2rad": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.diag": [
            "diag(): Supports 1D or 2D tensors. Got 7D",
            "Trying to resize storage that is not resizable",
            "Expected out tensor to have dtype float, but got int instead"
        ],
        "torch.diag_embed": [
            "diagonal dimensions cannot be identical -2, 6",
            "Dimension out of range (expected to be in range of [-4, 3], but got 9)"
        ],
        "torch.diagflat": [],
        "torch.diagonal": [
            "diagonal dimensions cannot be identical -3, 0",
            "Dimension out of range (expected to be in range of [-2, 1], but got 8)"
        ],
        "torch.diagonal_copy": [
            "diagonal dimensions cannot be identical -1, 2",
            "Dimension out of range (expected to be in range of [-3, 2], but got 6)",
            "Trying to resize storage that is not resizable"
        ],
        "torch.diff": [
            "Dimension out of range (expected to be in range of [-2, 1], but got -4)",
            "diff expects prepend or append to be the same dimension as input",
            "diff expects the shape of tensor to prepend or append to match that of input except along the differencing dimension; input.size(1) = 7, but got tensor.size(1) = 2",
            "order must be non-negative but got -4",
            "diff expects input to be at least one-dimensional"
        ],
        "torch.digamma": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.div": [
            "The size of tensor a (3) must match the size of tensor b (8) at non-singleton dimension 6",
            "div expected rounding_mode to be one of None, 'trunc', or 'floor' but found 'vmBq'",
            "ZeroDivisionError",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.divide": [
            "The size of tensor a (7) must match the size of tensor b (5) at non-singleton dimension 3",
            "div expected rounding_mode to be one of None, 'trunc', or 'floor' but found 'SzmG'",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.dsplit": [
            "number of sections must be larger than 0, got -4",
            "torch.dsplit attempted to split along dimension 2, but the size of the dimension 6 is not divisible by the split_size 5!",
            "torch.dsplit requires a tensor with at least 3 dimension, but got a tensor with 2 dimensions!"
        ],
        "torch.dstack": [
            "dstack expects a non-empty TensorList"
        ],
        "torch.eq": [
            "The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 4",
            "Trying to resize storage that is not resizable"
        ],
        "torch.erf": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.erfc": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.erfinv": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.exp": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.exp2": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.expm1": [
            "result type Float can't be cast to the desired output type Long",
            "Trying to resize storage that is not resizable"
        ],
        "torch.flatten": [
            "flatten() has invalid args: start_dim cannot come after end_dim",
            "Dimension out of range (expected to be in range of [-2, 1], but got -3)"
        ],
        "torch.flip": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 34)"
        ],
        "torch.fliplr": [
            "Input must be >= 2-d."
        ],
        "torch.flipud": [
            "Input must be >= 1-d."
        ],
        "torch.float_power": [
            "Trying to resize storage that is not resizable",
            "The size of tensor a (3) must match the size of tensor b (9) at non-singleton dimension 1",
            "the output given to float_power has dtype Float but the operation's result requires dtype Double"
        ],
        "torch.floor": [
            "Found dtype Int but expected Float",
            "Trying to resize storage that is not resizable"
        ],
        "torch.floor_divide": [
            "The size of tensor a (6) must match the size of tensor b (9) at non-singleton dimension 4",
            "Trying to resize storage that is not resizable",
            "ZeroDivisionError",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.fmax": [
            "The size of tensor a (2) must match the size of tensor b (9) at non-singleton dimension 6",
            "Trying to resize storage that is not resizable"
        ],
        "torch.fmin": [
            "The size of tensor a (3) must match the size of tensor b (8) at non-singleton dimension 4",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.fmod": [
            "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 4",
            "Trying to resize storage that is not resizable",
            "negative dimensions are not allowed",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.frac": [
            "\"frac_cpu\" not implemented for 'Int'",
            "Found dtype Float but expected Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.frexp": [
            "torch.frexp() only supports floating-point dtypes"
        ],
        "torch.full_like": [
            "Need to provide pin_memory allocator to use pin memory."
        ],
        "torch.gcd": [
            "\"gcd_cpu\" not implemented for 'Half'",
            "\"gcd_cpu\" not implemented for 'Float'",
            "The size of tensor a (4) must match the size of tensor b (8) at non-singleton dimension 5",
            "Trying to resize storage that is not resizable"
        ],
        "torch.ge": [
            "The size of tensor a (7) must match the size of tensor b (6) at non-singleton dimension 3",
            "negative dimensions are not allowed",
            "Trying to resize storage that is not resizable"
        ],
        "torch.geqrf": [
            "'complex32'",
            "\"geqrf_cpu\" not implemented for 'Bool'",
            "\"geqrf_cpu\" not implemented for 'Long'",
            "\"geqrf_cpu\" not implemented for 'Int'",
            "torch.geqrf: input must have at least 2 dimensions."
        ],
        "torch.gradient": [
            "torch.gradient only supports edge_order=1 and edge_order=2.",
            "Dimension out of range (expected to be in range of [-2, 1], but got 8)"
        ],
        "torch.gt": [
            "The size of tensor a (2) must match the size of tensor b (9) at non-singleton dimension 4",
            "Trying to resize storage that is not resizable"
        ],
        "torch.hardshrink": [
            "\"hardshrink_cpu\" not implemented for 'Int'",
            "\"hardshrink_cpu\" not implemented for 'ComplexDouble'",
            "Found dtype Int but expected Float",
            "Trying to resize storage that is not resizable"
        ],
        "torch.heaviside": [
            "heaviside is not yet implemented for tensors with different dtypes.",
            "The size of tensor a (4) must match the size of tensor b (7) at non-singleton dimension 4"
        ],
        "torch.hinge_embedding_loss": [
            "The size of tensor a (7) must match the size of tensor b (9) at non-singleton dimension 3"
        ],
        "torch.histc": [
            "'complex32'",
            "\"linspace_cpu\" not implemented for 'Bool'",
            "\"histogram_cpu\" not implemented for 'Int'",
            "torch.histogram(): bins must be > 0, but got -1 for dimension 0",
            "torch.histc: max must be larger than min",
            "torch.histogram: input tensor and hist tensor should have the same dtype, but got input float and hist int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.histogram": [
            "torch.histogramdd: bins tensor should have one dimension, but got 7 dimensions in the bins tensor for dimension 0",
            "torch.histogramdd: if weight tensor is provided it should have the same shape as the input tensor excluding its innermost dimension, but got input with shape [9, 1] and weight with shape [729]"
        ],
        "torch.histogramdd": [
            "histogramdd: The size of bins must be equal to the innermost dimension of the input.",
            "torch.histogramdd: input tensor should have at least 2 dimensions"
        ],
        "torch.hsplit": [
            "torch.hsplit attempted to split along dimension 0, but the size of the dimension 8 is not divisible by the split_size 0!",
            "torch.hsplit requires a tensor with at least 1 dimension, but got a tensor with 0 dimensions!"
        ],
        "torch.hstack": [
            "hstack expects a non-empty TensorList",
            "Tensors must have same number of dimensions: got 6 and 9",
            "Trying to resize storage that is not resizable"
        ],
        "torch.hypot": [
            "The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 2"
        ],
        "torch.i0": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.igamma": [
            "The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 2"
        ],
        "torch.igammac": [
            "The size of tensor a (8) must match the size of tensor b (9) at non-singleton dimension 6",
            "\"igammac_cpu\" not implemented for 'Int'"
        ],
        "torch.imag": [
            "imag is not implemented for tensors with non-complex dtypes."
        ],
        "torch.index_put": [
            "shape mismatch: value tensor of shape [3, 3, 3, 3] cannot be broadcast to indexing result of shape [1, 1]"
        ],
        "torch.isfinite": [],
        "torch.isin": [
            "Trying to resize storage that is not resizable",
            "Expected out tensor to have dtype bool, but got float instead"
        ],
        "torch.isinf": [],
        "torch.isnan": [],
        "torch.isneginf": [
            "Trying to resize storage that is not resizable",
            "isneginf does not support non-boolean outputs."
        ],
        "torch.isposinf": [
            "isposinf does not support non-boolean outputs.",
            "Trying to resize storage that is not resizable"
        ],
        "torch.isreal": [],
        "torch.kron": [
            "Trying to resize storage that is not resizable"
        ],
        "torch.kthvalue": [
            "Dimension out of range (expected to be in range of [-2, 1], but got 9)",
            "kthvalue(): selected number k out of range for dimension 0"
        ],
        "torch.lcm": [
            "\"lcm_cpu\" not implemented for 'Float'",
            "The size of tensor a (7) must match the size of tensor b (8) at non-singleton dimension 5",
            "Trying to resize storage that is not resizable",
            "\"lcm_cpu\" not implemented for 'Half'"
        ],
        "torch.ldexp": [
            "The size of tensor a (3) must match the size of tensor b (8) at non-singleton dimension 3",
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.le": [
            "The size of tensor a (9) must match the size of tensor b (7) at non-singleton dimension 3",
            "Trying to resize storage that is not resizable"
        ],
        "torch.lerp": [
            "The size of tensor a (9) must match the size of tensor b (8) at non-singleton dimension 2",
            "Trying to resize storage that is not resizable"
        ],
        "torch.lgamma": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.log": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.log10": [
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.log1p": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.log2": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.log_softmax": [
            "'complex32'",
            "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Bool'",
            "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Int'",
            "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
            "Expected out tensor to have dtype float, but got int instead",
            "Trying to resize storage that is not resizable"
        ],
        "torch.logaddexp": [
            "The size of tensor a (9) must match the size of tensor b (3) at non-singleton dimension 6",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int",
            "\"logaddexp_cpu\" not implemented for 'Short'"
        ],
        "torch.logaddexp2": [
            "The size of tensor a (4) must match the size of tensor b (9) at non-singleton dimension 4",
            "\"logaddexp2_cpu\" not implemented for 'Int'",
            "\"logaddexp2_cpu\" not implemented for 'Bool'",
            "'complex32'",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.logcumsumexp": [
            "negative dimensions are not allowed",
            "\"logcumsumexp_out_cpu\" not implemented for 'Int'",
            "Dimension out of range (expected to be in range of [-2, 1], but got 15)",
            "Trying to resize storage that is not resizable",
            "'complex32'",
            "\"logcumsumexp_out_cpu\" not implemented for 'Bool'",
            "expected scalar_type Float but found Int"
        ],
        "torch.logdet": [
            "logdet: A must be batches of square matrices, but they are 6 by 7 matrices",
            "logdet: The input tensor A must have at least 2 dimensions."
        ],
        "torch.logical_and": [
            "The size of tensor a (8) must match the size of tensor b (3) at non-singleton dimension 4",
            "Trying to resize storage that is not resizable"
        ],
        "torch.logical_not": [
            "Trying to resize storage that is not resizable"
        ],
        "torch.logical_or": [
            "The size of tensor a (8) must match the size of tensor b (2) at non-singleton dimension 4",
            "Trying to resize storage that is not resizable"
        ],
        "torch.logical_xor": [
            "The size of tensor a (8) must match the size of tensor b (7) at non-singleton dimension 2",
            "Trying to resize storage that is not resizable"
        ],
        "torch.logit": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch.logsumexp": []
    },
    "unsolved": {
        "torch.Tensor.__and__": [],
        "torch.Tensor.__iand__": [],
        "torch.Tensor.__ior__": [],
        "torch.Tensor.__ixor__": [],
        "torch.Tensor.__lshift__": [
            "\"lshift_cpu\" not implemented for 'Half'"
        ],
        "torch.Tensor.__or__": [
            "\"bitwise_or_cpu\" not implemented for 'Half'"
        ],
        "torch.Tensor.__rshift__": [],
        "torch.Tensor.__xor__": [],
        "torch.Tensor.abs": [],
        "torch.Tensor.abs_": [],
        "torch.Tensor.acos": [],
        "torch.Tensor.acos_": [],
        "torch.Tensor.acosh": [],
        "torch.Tensor.add": [],
        "torch.Tensor.add_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.addbmm": [
            "expand(torch.FloatTensor{[1, 1, 6, 1, 1, 1, 1]}, size=[1, 1]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (7)"
        ],
        "torch.Tensor.addcdiv": [],
        "torch.Tensor.addcdiv_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.addcmul": [],
        "torch.Tensor.addcmul_": [],
        "torch.Tensor.addmm": [
            "\"addmm_impl_cpu_\" not implemented for 'Bool'"
        ],
        "torch.Tensor.addmm_": [],
        "torch.Tensor.addmv": [
            "expected scalar type Float but found Int"
        ],
        "torch.Tensor.addmv_": [
            "expected scalar type Int but found Float"
        ],
        "torch.Tensor.addr": [],
        "torch.Tensor.adjoint": [],
        "torch.Tensor.all": [],
        "torch.Tensor.amax": [
            "amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
            "dim 6 appears multiple times in the list of dims",
            "amax(): Expected reduction dim 0 to have non-zero size."
        ],
        "torch.Tensor.amin": [
            "amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
            "dim 5 appears multiple times in the list of dims",
            "amin(): Expected reduction dim 1 to have non-zero size."
        ],
        "torch.Tensor.aminmax": [
            "aminmax: Expected reduction dim 0 to have non-zero size."
        ],
        "torch.Tensor.angle": [],
        "torch.Tensor.any": [],
        "torch.Tensor.argmax": [],
        "torch.Tensor.argmin": [],
        "torch.Tensor.argsort": [],
        "torch.Tensor.argwhere": [],
        "torch.Tensor.as_strided": [
            "Tensor: invalid storage offset -2",
            "setStorage: sizes [], strides [], storage offset 94, and itemsize 4 requiring a storage size of 380 are out of bounds for storage of size 4"
        ],
        "torch.Tensor.as_strided_": [
            "Tensor: invalid storage offset -4",
            "setStorage: sizes [], strides [], storage offset 45, and itemsize 4 requiring a storage size of 184 are out of bounds for storage of size 168"
        ],
        "torch.Tensor.asin": [],
        "torch.Tensor.asin_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.asinh": [],
        "torch.Tensor.atan": [],
        "torch.Tensor.atan2": [],
        "torch.Tensor.atan2_": [],
        "torch.Tensor.atan_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.atanh": [],
        "torch.Tensor.bitwise_and": [
            "\"bitwise_and_cpu\" not implemented for 'Half'",
            "\"bitwise_and_cpu\" not implemented for 'Double'"
        ],
        "torch.Tensor.bitwise_left_shift": [],
        "torch.Tensor.bitwise_not": [],
        "torch.Tensor.bitwise_not_": [
            "\"bitwise_not_cpu\" not implemented for 'Half'"
        ],
        "torch.Tensor.bitwise_or": [
            "negative dimensions are not allowed"
        ],
        "torch.Tensor.bitwise_xor": [],
        "torch.Tensor.bmm": [],
        "torch.Tensor.broadcast_to": [
            "The expanded size of the tensor (2) must match the existing size (7) at non-singleton dimension 1.  Target sizes: [3, 2, 3, 3, 4, 1, 4].  Tensor sizes: [1, 7, 3, 3, 1, 1, 1]"
        ],
        "torch.Tensor.ceil": [],
        "torch.Tensor.ceil_": [],
        "torch.Tensor.cholesky": [
            "cholesky: A must be batches of square matrices, but they are 2 by 3 matrices",
            "cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite).",
            "cholesky: The input tensor A must have at least 2 dimensions.",
            "\"cholesky_cpu\" not implemented for 'Int'"
        ],
        "torch.Tensor.cholesky_inverse": [
            "cholesky_inverse: A must be batches of square matrices, but they are 6 by 4 matrices",
            "cholesky_inverse: The input tensor A must have at least 2 dimensions.",
            "\"cholesky_inverse_out_cpu\" not implemented for 'Int'"
        ],
        "torch.Tensor.cholesky_solve": [
            "Expected b and A to have the same dtype, but found b of type Float and A of type Int instead.",
            "\"cholesky_solve_cpu\" not implemented for 'Int'"
        ],
        "torch.Tensor.chunk": [],
        "torch.Tensor.clamp": [
            "torch.clamp: At least one of 'min' or 'max' must not be None"
        ],
        "torch.Tensor.clamp_": [
            "torch.clamp: At least one of 'min' or 'max' must not be None"
        ],
        "torch.Tensor.clamp_max": [],
        "torch.Tensor.clamp_max_": [],
        "torch.Tensor.clamp_min": [],
        "torch.Tensor.clamp_min_": [],
        "torch.Tensor.conj": [],
        "torch.Tensor.conj_physical": [],
        "torch.Tensor.copysign": [],
        "torch.Tensor.corrcoef": [
            "corrcoef(): expected input to have two or fewer dimensions but got an input with 7 dimensions"
        ],
        "torch.Tensor.cos": [],
        "torch.Tensor.cos_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.cosh": [],
        "torch.Tensor.cosh_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.count_nonzero": [
            "dim 5 appears multiple times in the list of dims"
        ],
        "torch.Tensor.cov": [
            "cov(): aweights cannot be negative",
            "cov(): expected aweights to have floating point dtype but got aweights with Int dtype",
            "cov(): weights sum to zero, can't be normalized"
        ],
        "torch.Tensor.cummax": [],
        "torch.Tensor.cummin": [],
        "torch.Tensor.cumprod": [],
        "torch.Tensor.cumsum": [],
        "torch.Tensor.cumsum_": [],
        "torch.Tensor.deg2rad": [],
        "torch.Tensor.det": [
            "linalg.det: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch.Tensor.diag": [],
        "torch.Tensor.diag_embed": [],
        "torch.Tensor.diagflat": [],
        "torch.Tensor.diagonal": [
            "negative dimensions are not allowed"
        ],
        "torch.Tensor.diff": [],
        "torch.Tensor.digamma": [],
        "torch.Tensor.digamma_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.dist": [
            "linalg.vector_norm cannot compute the -2 norm on an empty tensor because the operation does not have an identity"
        ],
        "torch.Tensor.div": [],
        "torch.Tensor.div_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.divide": [],
        "torch.Tensor.divide_": [
            "result type Float can't be cast to the desired output type Int",
            "The size of tensor a (7) must match the size of tensor b (3) at non-singleton dimension 2"
        ],
        "torch.Tensor.dot": [],
        "torch.Tensor.dsplit": [
            "torch.dsplit attempted to split along dimension 2, but the size of the dimension 1 is not divisible by the split_size 4!"
        ],
        "torch.Tensor.eq": [],
        "torch.Tensor.erf": [],
        "torch.Tensor.erf_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.erfc": [],
        "torch.Tensor.erfc_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.erfinv": [],
        "torch.Tensor.erfinv_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.exp": [],
        "torch.Tensor.exp2": [],
        "torch.Tensor.exp_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.expand": [
            "The expanded size of the tensor (4) must match the existing size (3) at non-singleton dimension 6.  Target sizes: [3, 7, 7, 6, 3, 1, 4].  Tensor sizes: [4, 3, 4, 2, 7, 3, 3]"
        ],
        "torch.Tensor.expand_as": [],
        "torch.Tensor.expm1": [],
        "torch.Tensor.expm1_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.fill_": [],
        "torch.Tensor.flatten": [
            "negative dimensions are not allowed"
        ],
        "torch.Tensor.flip": [],
        "torch.Tensor.fliplr": [
            "Input must be >= 2-d."
        ],
        "torch.Tensor.flipud": [
            "Input must be >= 1-d."
        ],
        "torch.Tensor.float_power": [
            "negative dimensions are not allowed"
        ],
        "torch.Tensor.float_power_": [],
        "torch.Tensor.floor": [],
        "torch.Tensor.floor_": [],
        "torch.Tensor.floor_divide": [],
        "torch.Tensor.fmax": [],
        "torch.Tensor.fmin": [],
        "torch.Tensor.fmod": [],
        "torch.Tensor.fmod_": [],
        "torch.Tensor.frac": [
            "\"frac_cpu\" not implemented for 'Int'"
        ],
        "torch.Tensor.frac_": [
            "\"frac_cpu\" not implemented for 'Int'"
        ],
        "torch.Tensor.frexp": [
            "torch.frexp() only supports floating-point dtypes"
        ],
        "torch.Tensor.gather": [],
        "torch.Tensor.ge": [],
        "torch.Tensor.ge_": [],
        "torch.Tensor.gt": [],
        "torch.Tensor.hardshrink": [
            "\"hardshrink_cpu\" not implemented for 'Bool'"
        ],
        "torch.Tensor.heaviside": [],
        "torch.Tensor.heaviside_": [
            "The size of tensor a (6) must match the size of tensor b (3) at non-singleton dimension 6"
        ],
        "torch.Tensor.index_select": [
            "INDICES element is out of DATA bounds, id=-549749 axis_dim=6"
        ],
        "torch.Tensor.isinf": [],
        "torch.Tensor.isnan": [],
        "torch.Tensor.ldexp": [],
        "torch.Tensor.ldexp_": [],
        "torch.Tensor.le": [],
        "torch.Tensor.lerp": [],
        "torch.Tensor.lgamma": [],
        "torch.Tensor.lgamma_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.log": [],
        "torch.Tensor.log10": [],
        "torch.Tensor.log10_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.log1p": [],
        "torch.Tensor.log1p_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.log2": [],
        "torch.Tensor.log2_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.log_": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.log_softmax": [
            "negative dimensions are not allowed"
        ],
        "torch.Tensor.logical_and": [],
        "torch.Tensor.logical_not": [],
        "torch.Tensor.logical_not_": [],
        "torch.Tensor.logical_or": [],
        "torch.Tensor.logical_or_": [],
        "torch.Tensor.logical_xor": [],
        "torch.Tensor.logical_xor_": [],
        "torch.Tensor.lt": [],
        "torch.Tensor.masked_fill": [
            "The size of tensor a (3) must match the size of tensor b (7) at non-singleton dimension 6"
        ],
        "torch.Tensor.masked_fill_": [],
        "torch.Tensor.masked_scatter": [
            "negative dimensions are not allowed",
            "Number of elements of source < number of ones in mask"
        ],
        "torch.Tensor.masked_select": [],
        "torch.Tensor.matmul": [
            "Expected size for first two dimensions of batch2 tensor to be: [2016, 6] but got: [2016, 2].",
            "dot : expected both vectors to have same dtype, but found Float and Int"
        ],
        "torch.Tensor.matrix_exp": [
            "linalg.matrix_exp: A must be batches of square matrices, but they are 2 by 3 matrices",
            "linalg.matrix_exp: The input tensor A must have at least 2 dimensions.",
            "linalg.matrix_exp: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch.Tensor.matrix_power": [
            "linalg.matrix_power: A must be batches of square matrices, but they are 7 by 2 matrices",
            "linalg.matrix_power: The input tensor A must have at least 2 dimensions.",
            "linalg.inv: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch.Tensor.max": [],
        "torch.Tensor.maximum": [],
        "torch.Tensor.mean": [
            "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Int"
        ],
        "torch.Tensor.median": [],
        "torch.Tensor.min": [
            "The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 4"
        ],
        "torch.Tensor.minimum": [],
        "torch.Tensor.mm": [],
        "torch.Tensor.mode": [
            "negative dimensions are not allowed"
        ],
        "torch.Tensor.moveaxis": [],
        "torch.Tensor.movedim": [],
        "torch.Tensor.msort": [],
        "torch.Tensor.mul": [],
        "torch.Tensor.mul_": [],
        "torch.Tensor.mv": [],
        "torch.Tensor.mvlgamma": [],
        "torch.Tensor.nan_to_num": [],
        "torch.Tensor.nan_to_num_": [],
        "torch.Tensor.nanmean": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 62)",
            "nansum does not support complex inputs"
        ],
        "torch.Tensor.nanmedian": [],
        "torch.Tensor.narrow": [],
        "torch.Tensor.ne": [],
        "torch.Tensor.neg": [],
        "torch.Tensor.neg_": [],
        "torch.Tensor.new_full": [],
        "torch.Tensor.new_ones": [
            "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 2313851904000 bytes. Error code 12 (Cannot allocate memory)"
        ],
        "torch.Tensor.new_zeros": [
            "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 2197413036000 bytes. Error code 12 (Cannot allocate memory)"
        ],
        "torch.Tensor.nonzero": [],
        "torch.Tensor.norm": [],
        "torch.Tensor.permute": [
            "permute(): duplicate dims are not allowed.",
            "negative dimensions are not allowed"
        ],
        "torch.Tensor.polygamma_": [],
        "torch.Tensor.pow": [],
        "torch.Tensor.pow_": [],
        "torch.Tensor.prod": [],
        "torch.Tensor.ravel": [],
        "torch.Tensor.reciprocal": [],
        "torch.Tensor.reciprocal_": [],
        "torch.Tensor.relu": [],
        "torch.Tensor.relu_": [],
        "torch.Tensor.remainder_": [
            "ZeroDivisionError"
        ],
        "torch.Tensor.repeat": [
            "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1550224166512 bytes. Error code 12 (Cannot allocate memory)"
        ],
        "torch.Tensor.reshape": [
            "shape '[]' is invalid for input of size 720"
        ],
        "torch.Tensor.reshape_as": [],
        "torch.Tensor.resolve_conj": [],
        "torch.Tensor.rot90": [
            "Rotation dim0 out of range, dim0 = 7",
            "expected rotation dims to be different, but got dim0 = 2 and dim1 = 2"
        ],
        "torch.Tensor.round": [],
        "torch.Tensor.round_": [],
        "torch.Tensor.rsqrt": [],
        "torch.Tensor.rsqrt_": [],
        "torch.Tensor.scatter_": [
            "index 690975 is out of bounds for dimension 5 with size 7",
            "scatter(): Expected self.dtype to be equal to src.dtype"
        ],
        "torch.Tensor.scatter_add_": [
            "negative dimensions are not allowed",
            "scatter(): Expected self.dtype to be equal to src.dtype"
        ],
        "torch.Tensor.scatter_reduce_": [
            "index -922995 is out of bounds for dimension 0 with size 7",
            "scatter(): Expected self.dtype to be equal to src.dtype",
            "negative dimensions are not allowed"
        ],
        "torch.Tensor.sigmoid": [],
        "torch.Tensor.sigmoid_": [],
        "torch.Tensor.sign": [],
        "torch.Tensor.sign_": [],
        "torch.Tensor.sin": [],
        "torch.Tensor.sin_": [],
        "torch.Tensor.sinh": [],
        "torch.Tensor.sinh_": [],
        "torch.Tensor.softmax": [
            "\"softmax_kernel_impl\" not implemented for 'Int'"
        ],
        "torch.Tensor.sort": [
            "negative dimensions are not allowed"
        ],
        "torch.Tensor.sqrt": [],
        "torch.Tensor.sqrt_": [],
        "torch.Tensor.squeeze": [],
        "torch.Tensor.squeeze_": [],
        "torch.Tensor.sub": [],
        "torch.Tensor.sub_": [
            "output with shape [] doesn't match the broadcast shape [1, 2, 6, 5, 1, 7, 1]",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.Tensor.sum": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 52)"
        ],
        "torch.Tensor.swapaxes_": [],
        "torch.Tensor.swapdims_": [],
        "torch.Tensor.t": [],
        "torch.Tensor.t_": [],
        "torch.Tensor.tan": [],
        "torch.Tensor.tan_": [],
        "torch.Tensor.tanh": [],
        "torch.Tensor.tanh_": [],
        "torch.Tensor.transpose": [],
        "torch.Tensor.transpose_": [],
        "torch.Tensor.tril": [],
        "torch.Tensor.tril_": [],
        "torch.Tensor.triu_": [],
        "torch.Tensor.true_divide_": [],
        "torch.Tensor.trunc": [],
        "torch.Tensor.trunc_": [],
        "torch.Tensor.type_as": [],
        "torch.Tensor.unfold": [],
        "torch.Tensor.unique_consecutive": [
            "There are 0 sized dimensions, and they aren't selected, so unique cannot be applied"
        ],
        "torch.Tensor.unsqueeze": [],
        "torch.Tensor.unsqueeze_": [],
        "torch.Tensor.view": [
            "shape '[]' is invalid for input of size 16464"
        ],
        "torch.Tensor.view_as": [],
        "torch.Tensor.xlogy_": [],
        "torch.Tensor.zero_": [],
        "torch._C._fft.fft_fft": [
            "The input size 0, plus negative padding 0 and 0 resulted in a negative output size, which is invalid. Check dimension 2 of your input."
        ],
        "torch._C._fft.fft_fft2": [
            "When given, dim and shape arguments must have the same length",
            "FFT dims must be unique",
            "Dimension specified as 1 but tensor has no dimensions",
            "Dimension out of range (expected to be in range of [-1, 0], but got -2)",
            "Invalid normalization mode: \"lhFA\"",
            "negative dimensions are not allowed"
        ],
        "torch._C._fft.fft_fftn": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 49)",
            "Invalid number of data points (0) specified",
            "Got shape with 3 values but input tensor only has 2 dimensions.",
            "When given, dim and shape arguments must have the same length",
            "Dimension specified as 37 but tensor has no dimensions",
            "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 25702536592656 bytes. Error code 12 (Cannot allocate memory)",
            "negative dimensions are not allowed",
            "The input size 0, plus negative padding 0 and 0 resulted in a negative output size, which is invalid. Check dimension 0 of your input."
        ],
        "torch._C._fft.fft_fftshift": [
            "`shifts` required",
            "Dimension out of range (expected to be in range of [-7, 6], but got 43)"
        ],
        "torch._C._fft.fft_hfft": [
            "Invalid normalization mode: \"mOjq\"",
            "Dimension specified as 0 but tensor has no dimensions",
            "MKL FFT error: Intel MKL DFTI ERROR: Inconsistent configuration parameters",
            "The input size 0, plus negative padding 0 and 0 resulted in a negative output size, which is invalid. Check dimension 5 of your input.",
            "negative dimensions are not allowed",
            "Invalid number of data points (-1) specified",
            "hfft expects a floating point output tensor, but got Int"
        ],
        "torch._C._fft.fft_hfft2": [
            "When given, dim and shape arguments must have the same length",
            "Invalid normalization mode: \"uXfZ\"",
            "FFT dims must be unique",
            "Invalid number of data points (0) specified"
        ],
        "torch._C._fft.fft_hfftn": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 61)",
            "Got shape with 2 values but input tensor only has 1 dimensions.",
            "When given, dim and shape arguments must have the same length",
            "Invalid number of data points (0) specified",
            "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1464489600000 bytes. Error code 12 (Cannot allocate memory)",
            "hfftn expects a floating point output tensor, but got Int"
        ],
        "torch._C._fft.fft_ifft": [
            "negative dimensions are not allowed",
            "Invalid number of data points (-3) specified"
        ],
        "torch._C._fft.fft_ifft2": [
            "FFT dims must be unique",
            "When given, dim and shape arguments must have the same length",
            "negative dimensions are not allowed",
            "'complex32'",
            "Dimension out of range (expected to be in range of [-1, 0], but got -2)"
        ],
        "torch._C._fft.fft_ifftn": [
            "Dimension specified as 6 but tensor has no dimensions",
            "Dimension out of range (expected to be in range of [-2, 1], but got 96)",
            "Got shape with 2 values but input tensor only has 0 dimensions.",
            "When given, dim and shape arguments must have the same length",
            "The input size 0, plus negative padding 0 and 0 resulted in a negative output size, which is invalid. Check dimension 4 of your input.",
            "Invalid number of data points (0) specified",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._fft.fft_ifftshift": [
            "`shifts` required",
            "Dimension out of range (expected to be in range of [-7, 6], but got 53)"
        ],
        "torch._C._fft.fft_ihfft": [
            "Dimension specified as 0 but tensor has no dimensions",
            "Invalid number of data points (-3) specified",
            "negative dimensions are not allowed",
            "MKL FFT error: Intel MKL DFTI ERROR: Inconsistent configuration parameters",
            "The input size 0, plus negative padding 0 and 0 resulted in a negative output size, which is invalid. Check dimension 0 of your input.",
            "Found dtype ComplexDouble but expected ComplexFloat"
        ],
        "torch._C._fft.fft_ihfft2": [
            "FFT dims must be unique"
        ],
        "torch._C._fft.fft_ihfftn": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 81)",
            "Got shape with 3 values but input tensor only has 1 dimensions.",
            "Invalid number of data points (0) specified",
            "When given, dim and shape arguments must have the same length",
            "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 1854998179664 bytes. Error code 12 (Cannot allocate memory)",
            "The input size 0, plus negative padding 0 and 0 resulted in a negative output size, which is invalid. Check dimension 1 of your input.",
            "Trying to resize storage that is not resizable",
            "Found dtype ComplexFloat but expected ComplexDouble"
        ],
        "torch._C._fft.fft_irfft": [
            "MKL FFT error: Intel MKL DFTI ERROR: Inconsistent configuration parameters",
            "Unsupported dtype Half"
        ],
        "torch._C._fft.fft_irfft2": [
            "FFT dims must be unique",
            "When given, dim and shape arguments must have the same length",
            "Invalid normalization mode: \"zXSf\"",
            "Invalid number of data points (0) specified",
            "Trying to resize storage that is not resizable",
            "MKL FFT error: Intel MKL DFTI ERROR: Inconsistent configuration parameters"
        ],
        "torch._C._fft.fft_irfftn": [
            "irfftn must transform at least one axis",
            "Invalid number of data points (0) specified",
            "Invalid normalization mode: \"oIFK\"",
            "Dimension out of range (expected to be in range of [-6, 5], but got 65)",
            "When given, dim and shape arguments must have the same length",
            "Dimension specified as 52 but tensor has no dimensions",
            "irfftn expects a floating point output tensor, but got Int"
        ],
        "torch._C._fft.fft_rfft": [
            "Dimension specified as 0 but tensor has no dimensions",
            "Invalid number of data points (-4) specified",
            "MKL FFT error: Intel MKL DFTI ERROR: Inconsistent configuration parameters",
            "negative dimensions are not allowed"
        ],
        "torch._C._fft.fft_rfft2": [
            "FFT dims must be unique",
            "rfftn expects a real-valued input tensor, but got ComplexDouble",
            "negative dimensions are not allowed"
        ],
        "torch._C._fft.fft_rfftn": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 54)",
            "Got shape with 6 values but input tensor only has 1 dimensions.",
            "When given, dim and shape arguments must have the same length",
            "Invalid number of data points (0) specified",
            "rfftn must transform at least one axis"
        ],
        "torch._C._linalg.linalg_cholesky": [
            "linalg.cholesky: A must be batches of square matrices, but they are 3 by 1 matrices",
            "linalg.cholesky: (Batch element 0): The factorization could not be completed because the input is not positive-definite (the leading minor of order 2 is not positive-definite).",
            "linalg.cholesky: The input tensor A must have at least 2 dimensions.",
            "linalg.cholesky: Expected a floating point or complex tensor as input. Got Int",
            "negative dimensions are not allowed",
            "Expected out tensor to have dtype float, but got int instead"
        ],
        "torch._C._linalg.linalg_cholesky_ex": [
            "linalg.cholesky: A must be batches of square matrices, but they are 2 by 6 matrices",
            "linalg.cholesky_ex: (Batch element 1): The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite).",
            "linalg.cholesky: The input tensor A must have at least 2 dimensions.",
            "linalg.cholesky: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch._C._linalg.linalg_cond": [
            "linalg.cond(ord=1): A must be batches of square matrices, but they are 4 by 3 matrices",
            "linalg.cond(ord=fro): The input tensor A must have at least 2 dimensions.",
            "linalg.cond: Expected result to be safely castable from Float dtype, but got result with dtype Int",
            "Trying to resize storage that is not resizable",
            "negative dimensions are not allowed"
        ],
        "torch._C._linalg.linalg_det": [],
        "torch._C._linalg.linalg_eig": [
            "linalg.eig: A must be batches of square matrices, but they are 3 by 5 matrices",
            "linalg.eig: The input tensor A must have at least 2 dimensions.",
            "Unknown Complex ScalarType for Int"
        ],
        "torch._C._linalg.linalg_eigh": [
            "linalg.eigh: A must be batches of square matrices, but they are 5 by 6 matrices",
            "linalg.eigh: The input tensor A must have at least 2 dimensions.",
            "\"linalg_eigh_cpu\" not implemented for 'Int'"
        ],
        "torch._C._linalg.linalg_eigvals": [
            "Unknown Complex ScalarType for Int",
            "negative dimensions are not allowed",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._linalg.linalg_eigvalsh": [
            "linalg.eigh: The input tensor A must have at least 2 dimensions.",
            "linalg.eigh: A must be batches of square matrices, but they are 3 by 7 matrices",
            "\"linalg_eigh_cpu\" not implemented for 'Int'"
        ],
        "torch._C._linalg.linalg_inv": [
            "linalg.inv: A must be batches of square matrices, but they are 7 by 1 matrices",
            "Trying to resize storage that is not resizable",
            "linalg.inv: The input tensor A must have at least 2 dimensions.",
            "Expected out tensor to have dtype float, but got int instead",
            "linalg.inv: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch._C._linalg.linalg_inv_ex": [
            "linalg.inv: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch._C._linalg.linalg_ldl_factor": [
            "torch.linalg.ldl_factor_ex: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch._C._linalg.linalg_ldl_factor_ex": [
            "torch.linalg.ldl_factor_ex: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch._C._linalg.linalg_lstsq": [
            "This function doesn't handle types other than float and double",
            "\"linalg_lstsq_cpu\" not implemented for 'Int'",
            "linalg_lstsq() got an unexpected keyword argument 'solution'"
        ],
        "torch._C._linalg.linalg_lu": [],
        "torch._C._linalg.linalg_lu_factor": [
            "\"lu_cpu\" not implemented for 'Long'"
        ],
        "torch._C._linalg.linalg_lu_factor_ex": [],
        "torch._C._linalg.linalg_matrix_norm": [
            "linalg.matrix_norm: dims must be different. Got (1, 1)",
            "linalg.matrix_norm: Expected a floating point or complex tensor as input. Got Int",
            "amax(): Expected reduction dim 5 to have non-zero size.",
            "Trying to resize storage that is not resizable",
            "linalg.matrix_norm expected out tensor dtype Float but got: Int"
        ],
        "torch._C._linalg.linalg_matrix_power": [
            "linalg.matrix_power: A must be batches of square matrices, but they are 7 by 5 matrices",
            "linalg.inv: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch._C._linalg.linalg_matrix_rank": [
            "linalg.eigh: A must be batches of square matrices, but they are 4 by 6 matrices",
            "This function doesn't handle types other than float and double",
            "linalg.svd: Expected a floating point or complex tensor as input. Got Int",
            "\"linalg_eigh_cpu\" not implemented for 'Int'"
        ],
        "torch._C._linalg.linalg_norm": [
            "linalg.norm: If dim is specified, it must be of length 1 or 2. Got []",
            "linalg.vector_norm: Expected a floating point or complex tensor as input. Got Int",
            "Dimension out of range (expected to be in range of [-2, 1], but got 27)",
            "linalg.matrix_norm: dim must be a 2-tuple. Got 62",
            "linalg.matrix_norm: Order 84 not supported.",
            "linalg.matrix_norm: The input tensor A must have at least 2 dimensions."
        ],
        "torch._C._linalg.linalg_pinv": [
            "linalg.eigh: A must be batches of square matrices, but they are 1 by 4 matrices",
            "negative dimensions are not allowed",
            "'complex32'"
        ],
        "torch._C._linalg.linalg_qr": [],
        "torch._C._linalg.linalg_slogdet": [],
        "torch._C._linalg.linalg_solve": [
            "Expected out tensor to have dtype float, but got int instead"
        ],
        "torch._C._linalg.linalg_solve_ex": [
            "linalg.solve: Expected a floating point or complex tensor as input. Got Int",
            "linalg.solve: Expected A and B to have the same dtype, but found A of type Float and B of type Int instead"
        ],
        "torch._C._linalg.linalg_solve_triangular": [
            "linalg.solve_triangular: A must be batches of square matrices, but they are 4 by 2 matrices",
            "Trying to resize storage that is not resizable",
            "The size of tensor a (6) must match the size of tensor b (5) at non-singleton dimension 3",
            "\"triangular_solve_cpu\" not implemented for 'Int'"
        ],
        "torch._C._linalg.linalg_svd": [
            "linalg.svd: The input tensor A must have at least 2 dimensions.",
            "linalg.svd: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch._C._linalg.linalg_svdvals": [
            "torch.linalg.svd: keyword argument `driver=` is only supported on CUDA inputs with cuSOLVER backend.",
            "linalg.svd: The input tensor A must have at least 2 dimensions.",
            "linalg.svd: Expected a floating point or complex tensor as input. Got Int",
            "Trying to resize storage that is not resizable",
            "Expected out tensor to have dtype float, but got int instead"
        ],
        "torch._C._linalg.linalg_tensorinv": [
            "Expected self to satisfy the requirement prod(self.shape[ind:]) == prod(self.shape[:ind]), but got 5 != 4",
            "linalg.inv: Expected a floating point or complex tensor as input. Got Int",
            "tensorinv: Expected result to be safely castable from Float dtype, but got result with dtype Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._linalg.linalg_tensorsolve": [
            "Expected self to satisfy the requirement prod(self.shape[other.ndim:]) == prod(self.shape[:other.ndim]), but got 1 != 62500",
            "Dimension out of range (expected to be in range of [-7, 6], but got 62)",
            "linalg.solve: Expected A and B to have the same dtype, but found A of type Float and B of type Int instead",
            "linalg.solve: Expected a floating point or complex tensor as input. Got Int",
            "tensorsolve: Expected result to be safely castable from Float dtype, but got result with dtype Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._linalg.linalg_vander": [],
        "torch._C._linalg.linalg_vecdot": [
            "negative dimensions are not allowed"
        ],
        "torch._C._linalg.linalg_vector_norm": [
            "Dimension out of range (expected to be in range of [-1, 0], but got 68)",
            "negative dimensions are not allowed"
        ],
        "torch._C._nn.adaptive_max_pool2d": [],
        "torch._C._nn.adaptive_max_pool3d": [
            "Trying to create tensor with negative dimension -1: [7, 5, -1, 3, 5]"
        ],
        "torch._C._nn.avg_pool2d": [
            "pad should be at most half of effective kernel size, but got pad=4, kernel_size=-3 and dilation=1",
            "pad must be non-negative, but got pad: -3",
            "Dimension specified as -3 but tensor has no dimensions",
            "divisor must be not zero",
            "negative dimensions are not allowed",
            "stride should not be zero",
            "Expected 3D or 4D (batch mode) tensor with optional 0 dim batch size for input, but got:[1, 1, 1, 2, 1, 3, 1]",
            "kernel size should be greater than zero, but got kH: 0 kW: 7",
            "Expected out tensor to have dtype int, but got float instead",
            "Given input size: (7x1x4). Calculated output size: (7x-2x-2). Output size is too small",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._nn.avg_pool3d": [
            "kernel size should be greater than zero, but got kT: 0 kH: 4 kW: 3",
            "input image (T: 1 H: 4 W: 4) smaller than kernel size (kT: 4 kH: 5 kW: 2)",
            "avg_pool3d(): Expected input's non-batch dimensions to have positive length, but input has a shape of [7, 0, 7, 0, 1] and non-batch dimension 0 has length zero!",
            "divisor must be not zero",
            "non-empty 4D or 5D (batch mode) tensor expected for input",
            "negative dimensions are not allowed",
            "pad should be at most half of effective kernel size, but got pad=7, kernel_size=5 and dilation=1",
            "pad must be non-negative, but got pad: -2"
        ],
        "torch._C._nn.flatten_dense_tensors": [],
        "torch._C._nn.gelu": [
            "\"GeluKernelImpl\" not implemented for 'Long'"
        ],
        "torch._C._nn.gelu_": [],
        "torch._C._nn.huber_loss": [
            "\"huber_cpu\" not implemented for 'ComplexFloat'"
        ],
        "torch._C._nn.l1_loss": [],
        "torch._C._nn.log_sigmoid": [
            "expected scalar type Float but found Int",
            "negative dimensions are not allowed"
        ],
        "torch._C._nn.max_pool2d_with_indices": [
            "pad must be non-negative, but got pad: -3",
            "pad should be at most half of effective kernel size, but got pad=0, kernel_size=-1 and dilation=6",
            "pad should be smaller than or equal to half of kernel size, but got padW = 7, padH = 5, kW = 7, kH = 5",
            "stride should not be zero"
        ],
        "torch._C._nn.mse_loss": [
            "reduction == Reduction::Mean || reduction == Reduction::Sum INTERNAL ASSERT FAILED at \"../aten/src/ATen/native/Loss.cpp\":97, please report a bug to PyTorch. ",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch._C._nn.pad_sequence": [],
        "torch._C._nn.reflection_pad1d": [
            "Dimension specified as 0 but tensor has no dimensions"
        ],
        "torch._C._nn.smooth_l1_loss": [
            "reduction == Reduction::Mean || reduction == Reduction::Sum INTERNAL ASSERT FAILED at \"../aten/src/ATen/native/Loss.cpp\":86, please report a bug to PyTorch. ",
            "smooth_l1_loss does not support negative values for beta.",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch._C._nn.soft_margin_loss": [
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch._C._nn.softplus": [
            "\"softplus_cpu\" not implemented for 'Long'",
            "\"softplus_cpu\" not implemented for 'Bool'"
        ],
        "torch._C._nn.softshrink": [
            "\"softshrink_cpu\" not implemented for 'Long'",
            "\"softshrink_cpu\" not implemented for 'Int'"
        ],
        "torch._C._nn.upsample_bicubic2d": [
            "Must specify exactly one of output_size and scale_factors",
            "Expected static_cast<int64_t>(scale_factors->size()) == spatial_dimensions to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
            "\"compute_indices_weights_cubic\" not implemented for 'Bool'",
            "Trying to resize storage that is not resizable",
            "negative dimensions are not allowed",
            "\"compute_indices_weights_cubic\" not implemented for 'Int'",
            "Non-empty 4D data tensor expected but got a tensor with sizes [6, 0, 1, 4]",
            "\"compute_indices_weights_cubic\" not implemented for 'Long'",
            "\"compute_indices_weights_cubic\" not implemented for 'Short'"
        ],
        "torch._C._nn.upsample_bilinear2d": [
            "Must specify exactly one of output_size and scale_factors",
            "It is expected output_size equals to 2, but got size 0",
            "ArrayRef: invalid index Index = 0; Length = 0",
            "\"upsample_bilinear2d_channels_last\" not implemented for 'Long'",
            "Trying to resize storage that is not resizable",
            "Expected out tensor to have dtype int, but got float instead",
            "Non-empty 4D data tensor expected but got a tensor with sizes [4, 0, 2, 7]"
        ],
        "torch._C._nn.upsample_linear1d": [
            "Expected static_cast<int64_t>(output_size->size()) == spatial_dimensions to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
            "Must specify exactly one of output_size and scale_factors",
            "It is expected output_size equals to 1, but got size 4",
            "\"compute_indices_weights_linear\" not implemented for 'Long'",
            "\"compute_indices_weights_linear\" not implemented for 'Int'"
        ],
        "torch._C._nn.upsample_nearest1d": [
            "Must specify exactly one of output_size and scale_factors",
            "Input and output sizes should be greater than 0, but got input (W: 1) and output (W: 0)",
            "It is expected output_size equals to 1, but got size 0",
            "\"compute_indices_weights_nearest\" not implemented for 'Int'",
            "Trying to resize storage that is not resizable",
            "Expected out tensor to have dtype float, but got int instead",
            "Non-empty 3D data tensor expected but got a tensor with sizes [4, 0, 4]"
        ],
        "torch._C._nn.upsample_nearest2d": [
            "Expected static_cast<int64_t>(scale_factors->size()) == spatial_dimensions to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
            "Must specify exactly one of output_size and scale_factors",
            "It is expected output_size equals to 2, but got size 5",
            "ArrayRef: invalid index Index = 0; Length = 0",
            "Input and output sizes should be greater than 0, but got input (H: 5, W: 6) output (H: -2, W: 4)",
            "Trying to resize storage that is not resizable",
            "Expected out tensor to have dtype float, but got int instead",
            "negative dimensions are not allowed"
        ],
        "torch._C._nn.upsample_nearest3d": [
            "Must specify exactly one of output_size and scale_factors",
            "Expected static_cast<int64_t>(scale_factors->size()) == spatial_dimensions to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
            "It is expected output_size equals to 3, but got size 0",
            "\"compute_indices_weights_nearest\" not implemented for 'Long'",
            "Input and output sizes should be greater than 0, but got input (D: 7, H: 3, W: 3) output (D: -3, H: 7, W: 3)",
            "Trying to resize storage that is not resizable",
            "negative dimensions are not allowed",
            "Expected out tensor to have dtype int, but got float instead",
            "Non-empty 5D data tensor expected but got a tensor with sizes [7, 0, 6, 7, 6]"
        ],
        "torch._C._nn.upsample_trilinear3d": [
            "Must specify exactly one of output_size and scale_factors",
            "Expected static_cast<int64_t>(output_size->size()) == spatial_dimensions to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
            "It is expected output_size equals to 3, but got size 0",
            "ArrayRef: invalid index Index = 0; Length = 0",
            "Non-empty 5D data tensor expected but got a tensor with sizes [7, 0, 7, 1, 6]",
            "Expected out tensor to have dtype float, but got int instead",
            "Input and output sizes should be greater than 0, but got input (D: 0, H: 4, W: 6) output (D: 7, H: 4, W: 2)"
        ],
        "torch._C._special.special_airy_ai": [],
        "torch._C._special.special_bessel_j0": [],
        "torch._C._special.special_bessel_j1": [],
        "torch._C._special.special_bessel_y0": [],
        "torch._C._special.special_bessel_y1": [],
        "torch._C._special.special_chebyshev_polynomial_t": [
            "result type Float can't be cast to the desired output type Int",
            "Trying to resize storage that is not resizable"
        ],
        "torch._C._special.special_chebyshev_polynomial_u": [
            "negative dimensions are not allowed",
            "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 4",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch._C._special.special_entr": [],
        "torch._C._special.special_erfcx": [
            "negative dimensions are not allowed"
        ],
        "torch._C._special.special_hermite_polynomial_h": [],
        "torch._C._special.special_hermite_polynomial_he": [
            "The size of tensor a (2) must match the size of tensor b (7) at non-singleton dimension 5",
            "result type Float can't be cast to the desired output type Int",
            "negative dimensions are not allowed"
        ],
        "torch._C._special.special_i0e": [],
        "torch._C._special.special_i1": [],
        "torch._C._special.special_i1e": [],
        "torch._C._special.special_laguerre_polynomial_l": [
            "The size of tensor a (7) must match the size of tensor b (5) at non-singleton dimension 6",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch._C._special.special_log_ndtr": [],
        "torch._C._special.special_modified_bessel_i0": [],
        "torch._C._special.special_modified_bessel_i1": [],
        "torch._C._special.special_modified_bessel_k0": [],
        "torch._C._special.special_modified_bessel_k1": [],
        "torch._C._special.special_ndtr": [
            "negative dimensions are not allowed"
        ],
        "torch._C._special.special_ndtri": [],
        "torch._C._special.special_scaled_modified_bessel_k0": [
            "\"scaled_modified_bessel_k0_cpu\" not implemented for 'Half'"
        ],
        "torch._C._special.special_scaled_modified_bessel_k1": [],
        "torch._C._special.special_spherical_bessel_j0": [
            "negative dimensions are not allowed"
        ],
        "torch._C._special.special_xlog1py": [
            "result type Float can't be cast to the desired output type Int",
            "negative dimensions are not allowed"
        ],
        "torch._C._special.special_zeta": [
            "The size of tensor a (5) must match the size of tensor b (6) at non-singleton dimension 5",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.abs": [],
        "torch.acos": [],
        "torch.acosh": [],
        "torch.adaptive_avg_pool1d": [
            "\"adaptive_avg_pool2d\" not implemented for 'Int'",
            "adaptive_avg_pool2d(): Expected input to have non-zero size for non-batch dimensions, but input has sizes [7, 6, 1, 0] with dimension 3 being empty",
            "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Int"
        ],
        "torch.adaptive_max_pool1d": [],
        "torch.add": [],
        "torch.addcdiv": [
            "Integer division with addcdiv is no longer supported, and in a future  release addcdiv will perform a true division of tensor1 and tensor2. The historic addcdiv behavior can be implemented as (input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype) for integer inputs and as (input + value * tensor1 / tensor2) for float inputs. The future addcdiv behavior is just the latter implementation: (input + value * tensor1 / tensor2), for all dtypes."
        ],
        "torch.addcmul": [
            "negative dimensions are not allowed"
        ],
        "torch.addmm": [
            "expand(torch.FloatTensor{[6, 3, 6, 6]}, size=[6, 3]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (4)",
            "self and mat2 must have the same dtype, but got Int and Float",
            "Trying to resize storage that is not resizable",
            "Expected out tensor to have dtype float, but got int instead"
        ],
        "torch.addmv": [
            "Trying to resize storage that is not resizable",
            "Expected out tensor to have dtype float, but got int instead",
            "expected scalar type Int but found Float"
        ],
        "torch.addr": [
            "expand(torch.FloatTensor{[7, 5, 6, 5, 1, 3, 3]}, size=[7, 7]): the number of sizes provided (2) must be greater or equal to the number of dimensions in the tensor (7)",
            "The expanded size of the tensor (5) must match the existing size (6) at non-singleton dimension 1.  Target sizes: [5, 5].  Tensor sizes: [1, 6]",
            "Trying to resize storage that is not resizable"
        ],
        "torch.all": [
            "Dimension out of range (expected to be in range of [-7, 6], but got 58)",
            "negative dimensions are not allowed",
            "all: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."
        ],
        "torch.amax": [
            "amax(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
            "dim 0 appears multiple times in the list of dims",
            "amax(): Expected reduction dim 2 to have non-zero size.",
            "Trying to resize storage that is not resizable"
        ],
        "torch.amin": [
            "amin(): Expected reduction dim -3 to have non-zero size.",
            "dim 1 appears multiple times in the list of dims",
            "amin(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.",
            "Trying to resize storage that is not resizable"
        ],
        "torch.aminmax": [
            "aminmax: Expected reduction dim 4 to have non-zero size."
        ],
        "torch.angle": [],
        "torch.any": [
            "negative dimensions are not allowed",
            "Dimension out of range (expected to be in range of [-7, 6], but got 79)",
            "Trying to resize storage that is not resizable",
            "any: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."
        ],
        "torch.argmax": [
            "Expected out tensor to have dtype long int, but got float instead",
            "Dimension out of range (expected to be in range of [-7, 6], but got 79)",
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.",
            "argmax(): Expected reduction dim 2 to have non-zero size."
        ],
        "torch.argmin": [
            "argmin(): Expected reduction dim 3 to have non-zero size."
        ],
        "torch.argsort": [
            "argsort: You passed a dimname (string) to this op in place of a dimension index but it does not yet support this behavior. Please pass a dimension index to work around this."
        ],
        "torch.argwhere": [],
        "torch.as_strided": [
            "Tensor: invalid storage offset -4",
            "mismatch in length of strides and shape",
            "setStorage: sizes [], strides [], storage offset 0, and itemsize 4 requiring a storage size of 4 are out of bounds for storage of size 0"
        ],
        "torch.as_tensor": [],
        "torch.asin": [],
        "torch.asinh": [
            "negative dimensions are not allowed"
        ],
        "torch.atan": [],
        "torch.atan2": [],
        "torch.atanh": [],
        "torch.atleast_1d": [
            "Could not run 'aten::random_.from' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::random_.from' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31470 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44611 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:22277 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16033 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: registered at ../aten/src/ATen/functorch/BatchRulesRandomness.cpp:367 [kernel]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: registered at ../aten/src/ATen/VmapModeRegistrations.cpp:37 [kernel]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
            "qint16",
            "'complex32'"
        ],
        "torch.atleast_2d": [
            "Could not run 'aten::random_.from' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::random_.from' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31470 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44611 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:22277 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16033 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: registered at ../aten/src/ATen/functorch/BatchRulesRandomness.cpp:367 [kernel]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: registered at ../aten/src/ATen/VmapModeRegistrations.cpp:37 [kernel]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
            "qint16",
            "'complex32'"
        ],
        "torch.atleast_3d": [
            "Could not run 'aten::random_.from' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::random_.from' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31470 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44611 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:22277 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16033 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: registered at ../aten/src/ATen/functorch/BatchRulesRandomness.cpp:367 [kernel]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: registered at ../aten/src/ATen/VmapModeRegistrations.cpp:37 [kernel]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
            "'complex32'",
            "qint16"
        ],
        "torch.bernoulli": [
            "Expected p_in >= 0 && p_in <= 1 to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)",
            "\"bernoulli_tensor_cpu_p_\" not implemented for 'Int'",
            "Trying to resize storage that is not resizable"
        ],
        "torch.binary_cross_entropy_with_logits": [
            "negative dimensions are not allowed"
        ],
        "torch.bitwise_and": [
            "\"bitwise_and_cpu\" not implemented for 'Half'"
        ],
        "torch.bitwise_left_shift": [
            "result type Float can't be cast to the desired output type Int",
            "negative dimensions are not allowed"
        ],
        "torch.bitwise_not": [
            "\"bitwise_not_cpu\" not implemented for 'Half'"
        ],
        "torch.bitwise_or": [
            "\"bitwise_or_cpu\" not implemented for 'Half'"
        ],
        "torch.bitwise_right_shift": [
            "Trying to resize storage that is not resizable",
            "\"rshift_cpu\" not implemented for 'Float'",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.bitwise_xor": [
            "Trying to resize storage that is not resizable",
            "'complex32'"
        ],
        "torch.block_diag": [
            "torch.block_diag: Input tensors must have 2 or fewer dimensions. Input 0 has 7 dimensions"
        ],
        "torch.bmm": [
            "Expected size for first two dimensions of batch2 tensor to be: [2, 6] but got: [5, 7].",
            "expected scalar type Float but found Int"
        ],
        "torch.broadcast_tensors": [
            "Could not run 'aten::random_.from' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::random_.from' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31470 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44611 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:22277 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16033 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: registered at ../aten/src/ATen/functorch/BatchRulesRandomness.cpp:367 [kernel]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: registered at ../aten/src/ATen/VmapModeRegistrations.cpp:37 [kernel]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
            "The size of tensor a (4) must match the size of tensor b (0) at non-singleton dimension 2",
            "'complex32'",
            "qint16"
        ],
        "torch.broadcast_to": [],
        "torch.bucketize": [],
        "torch.cartesian_prod": [],
        "torch.cat": [
            "Sizes of tensors must match except in dimension 5. Expected size 5 but got size 7 for tensor number 2 in the list.",
            "Tensors must have same number of dimensions: got 5 and 7",
            "Name 'dkEx' not found in Tensor[None, None, None, None, None, None, None].",
            "Name 'dim_index' not found in Tensor[].",
            "zero-dimensional tensor (at position 0) cannot be concatenated",
            "negative dimensions are not allowed",
            "torch.cat(): input types can't be cast to the desired output type Int"
        ],
        "torch.ceil": [],
        "torch.cholesky": [
            "cholesky: A must be batches of square matrices, but they are 2 by 4 matrices",
            "cholesky: The factorization could not be completed because the input is not positive-definite (the leading minor of order 1 is not positive-definite).",
            "cholesky: The input tensor A must have at least 2 dimensions.",
            "\"cholesky_cpu\" not implemented for 'Int'",
            "cholesky: Expected result to be safely castable from Float dtype, but got result with dtype Int"
        ],
        "torch.cholesky_inverse": [
            "\"cholesky_inverse_out_cpu\" not implemented for 'Int'",
            "cholesky_inverse: Expected result to be safely castable from Float dtype, but got result with dtype Int"
        ],
        "torch.chunk": [],
        "torch.clamp": [
            "torch.clamp: At least one of 'min' or 'max' must not be None",
            "The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 6",
            "Trying to resize storage that is not resizable",
            "Found dtype Int but expected Float",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.clamp_": [
            "torch.clamp: At least one of 'min' or 'max' must not be None",
            "output with shape [] doesn't match the broadcast shape [4, 6, 6, 2, 1, 5, 1]",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.clamp_max": [],
        "torch.clamp_max_": [],
        "torch.clamp_min": [],
        "torch.clamp_min_": [
            "output with shape [] doesn't match the broadcast shape [1, 1, 2, 1, 5, 1, 1]",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.column_stack": [
            "Sizes of tensors must match except in dimension 1. Expected size 6 but got size 7 for tensor number 1 in the list.",
            "Trying to resize storage that is not resizable",
            "torch.cat(): input types can't be cast to the desired output type Int",
            "Tensors must have same number of dimensions: got 5 and 6"
        ],
        "torch.combinations": [],
        "torch.complex": [
            "Expected both inputs to be Half, Float or Double tensors but got ComplexFloat and Float"
        ],
        "torch.conj": [],
        "torch.conj_physical": [],
        "torch.constant_pad_nd": [],
        "torch.copysign": [],
        "torch.corrcoef": [
            "corrcoef(): expected input to have two or fewer dimensions but got an input with 7 dimensions"
        ],
        "torch.cos": [
            "negative dimensions are not allowed"
        ],
        "torch.cosh": [],
        "torch.cosine_embedding_loss": [],
        "torch.cosine_similarity": [],
        "torch.count_nonzero": [
            "negative dimensions are not allowed"
        ],
        "torch.cov": [
            "cov(): expected fweights to have integral dtype but got fweights with Float dtype",
            "cov(): expected aweights to have one or fewer dimensions but got aweights with 7 dimensions",
            "cov(): fweights cannot be negative"
        ],
        "torch.cross": [],
        "torch.cummax": [
            "negative dimensions are not allowed",
            "Name 'RXcB' not found in Tensor[None, None, None, None, None, None, None].",
            "Name 'Jyql' not found in Tensor[]."
        ],
        "torch.cummin": [
            "Name 'jwai' not found in Tensor[None, None, None, None, None, None, None].",
            "Name 'HyJF' not found in Tensor[]."
        ],
        "torch.cumprod": [
            "Name 'HfWK' not found in Tensor[None, None, None, None, None, None, None].",
            "Name 'iyxn' not found in Tensor[].",
            "negative dimensions are not allowed"
        ],
        "torch.cumsum": [
            "Name 'hWQR' not found in Tensor[None, None, None, None, None, None, None].",
            "Name 'vTES' not found in Tensor[].",
            "negative dimensions are not allowed"
        ],
        "torch.cumulative_trapezoid": [
            "slice() cannot be applied to a 0-dim tensor.",
            "negative dimensions are not allowed"
        ],
        "torch.deg2rad": [],
        "torch.diag": [],
        "torch.diag_embed": [],
        "torch.diagflat": [],
        "torch.diagonal": [
            "Name 'oMMb' not found in Tensor[None, None, None, None, None, None, None].",
            "Name 'rnNZ' not found in Tensor[]."
        ],
        "torch.diagonal_copy": [
            "Expected out tensor to have dtype int, but got float instead"
        ],
        "torch.diff": [
            "diff expects the shape of tensor to prepend or append to match that of input except along the differencing dimension; input.size(0) = 6, but got tensor.size(0) = 3",
            "Dimension out of range (expected to be in range of [-7, 6], but got 20)",
            "order must be non-negative but got -4",
            "diff expects input to be at least one-dimensional",
            "diff expects prepend or append to be the same dimension as input",
            "Trying to resize storage that is not resizable"
        ],
        "torch.digamma": [],
        "torch.div": [
            "The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 6",
            "negative dimensions are not allowed"
        ],
        "torch.divide": [
            "div expected rounding_mode to be one of None, 'trunc', or 'floor' but found 'JGqp'",
            "The size of tensor a (6) must match the size of tensor b (7) at non-singleton dimension 5",
            "Trying to resize storage that is not resizable"
        ],
        "torch.dsplit": [],
        "torch.dstack": [
            "Sizes of tensors must match except in dimension 2. Expected size 1 but got size 2 for tensor number 1 in the list.",
            "Tensors must have same number of dimensions: got 5 and 6",
            "Trying to resize storage that is not resizable",
            "torch.cat(): input types can't be cast to the desired output type Int"
        ],
        "torch.eq": [],
        "torch.erf": [],
        "torch.erfc": [],
        "torch.erfinv": [],
        "torch.exp": [],
        "torch.exp2": [],
        "torch.expm1": [],
        "torch.flatten": [
            "flatten() has invalid args: start_dim cannot come after end_dim",
            "Name 'dRfP' not found in Tensor[None, None, None, None, None, None, None].",
            "Name 'RkcU' not found in Tensor[]."
        ],
        "torch.flip": [
            "dim 4 appears multiple times in the list of dims"
        ],
        "torch.fliplr": [],
        "torch.flipud": [],
        "torch.float_power": [
            "The size of tensor a (5) must match the size of tensor b (2) at non-singleton dimension 5",
            "negative dimensions are not allowed"
        ],
        "torch.floor": [],
        "torch.floor_divide": [
            "ZeroDivisionError",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.fmax": [
            "negative dimensions are not allowed",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.fmin": [],
        "torch.fmod": [
            "negative dimensions are not allowed",
            "ZeroDivisionError"
        ],
        "torch.frac": [
            "\"frac_cpu\" not implemented for 'Int'",
            "\"frac_cpu\" not implemented for 'Long'",
            "\"frac_cpu\" not implemented for 'Bool'"
        ],
        "torch.frexp": [],
        "torch.full_like": [],
        "torch.gcd": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.ge": [],
        "torch.geqrf": [
            "\"geqrf_cpu\" not implemented for 'Half'"
        ],
        "torch.gradient": [
            "torch.gradient expected each dimension size to be at least edge_order+1",
            "Dimension specified as 0 but tensor has no dimensions",
            "dim 4 appears multiple times in the list of dims",
            "torch.gradient only supports edge_order=1 and edge_order=2.",
            "Dimension out of range (expected to be in range of [-7, 6], but got 73)",
            "torch.gradient expected spacing to be unspecified, a scalar or it's spacing and dim arguments to have the same length, but got a spacing argument of length 0 and a dim argument of length 1.",
            "torch.gradient expected spacing to be unspecified, a scalar, or a list of length equal to 'self.dim() = 7', since dim argument was not given, but got a list of length 0",
            "Could not run 'aten::random_.from' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::random_.from' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31470 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44611 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:22277 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16033 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: registered at ../aten/src/ATen/functorch/BatchRulesRandomness.cpp:367 [kernel]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: registered at ../aten/src/ATen/VmapModeRegistrations.cpp:37 [kernel]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
            "qint16",
            "'complex32'"
        ],
        "torch.gt": [
            "negative dimensions are not allowed",
            "Trying to resize storage that is not resizable"
        ],
        "torch.hardshrink": [
            "'complex32'",
            "\"hardshrink_cpu\" not implemented for 'Long'"
        ],
        "torch.heaviside": [
            "Trying to resize storage that is not resizable",
            "heaviside is not yet implemented for tensors with different dtypes."
        ],
        "torch.hinge_embedding_loss": [],
        "torch.histc": [
            "\"histogram_cpu\" not implemented for 'Long'",
            "torch.histc: range of [-inf, -inf] is not finite",
            "negative dimensions are not allowed"
        ],
        "torch.histogram": [
            "torch.histogram(): bins must be > 0, but got 0 for dimension 0",
            "torch.histogramdd: input tensor and bins tensors should have the same dtype, but got input with dtype float and bins for dimension 0 with dtype int",
            "torch.histogramdd: if weight tensor is provided, input tensor and weight tensor should have the same dtype, but got input(float), and weight(int)",
            "torch.histogramdd: for a 1-dimensional histogram range should have 2 elements, but got 0",
            "torch.histogramdd: if weight tensor is provided it should have the same shape as the input tensor excluding its innermost dimension, but got input with shape [1, 1] and weight with shape [1512]",
            "\"histogramdd\" not implemented for 'Int'"
        ],
        "torch.histogramdd": [
            "torch.histogramdd: for a 6-dimensional histogram range should have 12 elements, but got 3",
            "torch.histogram(): bins must be > 0, but got 0 for dimension 0",
            "torch.histogramdd: if weight tensor is provided it should have the same shape as the input tensor excluding its innermost dimension, but got input with shape [4, 4] and weight with shape [6, 6, 6]",
            "negative dimensions are not allowed",
            "\"histogramdd\" not implemented for 'Int'",
            "number of steps must be non-negative",
            "torch.histogramdd: if weight tensor is provided, input tensor and weight tensor should have the same dtype, but got input(float), and weight(int)",
            "torch.histogramdd: bins tensor should have at least 1 element, but got 0 elements in the bins tensor for dimension 3",
            "Dimension specified as -1 but tensor has no dimensions",
            "[enforce fail at alloc_cpu.cpp:117] err == 0. DefaultCPUAllocator: can't allocate memory: you tried to allocate 2940367562500 bytes. Error code 12 (Cannot allocate memory)",
            "histogramdd: The size of bins must be equal to the innermost dimension of the input.",
            "torch.histogramdd: input tensor should have at least 2 dimensions",
            "Could not run 'aten::random_.from' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::random_.from' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31470 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44611 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:22277 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16033 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: registered at ../aten/src/ATen/functorch/BatchRulesRandomness.cpp:367 [kernel]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: registered at ../aten/src/ATen/VmapModeRegistrations.cpp:37 [kernel]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
            "qint16",
            "'complex32'",
            "torch.histogramdd: input tensor and bins tensors should have the same dtype, but got input with dtype float and bins for dimension 0 with dtype int",
            "torch.histogramdd: expected 1 sequences of bin edges for a 1-dimensional histogram but got 2",
            "histogramdd(): argument 'bins' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\""
        ],
        "torch.hsplit": [],
        "torch.hstack": [
            "Sizes of tensors must match except in dimension 1. Expected size 7 but got size 5 for tensor number 1 in the list.",
            "Tensors must have same number of dimensions: got 2 and 7",
            "negative dimensions are not allowed",
            "torch.cat(): input types can't be cast to the desired output type Int"
        ],
        "torch.hypot": [
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.i0": [],
        "torch.igamma": [
            "\"igamma_cpu\" not implemented for 'Int'",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.igammac": [
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.imag": [],
        "torch.index_put": [
            "ntensor >= 3 INTERNAL ASSERT FAILED at \"../aten/src/ATen/native/cpu/IndexKernelUtils.h\":10, please report a bug to PyTorch. ",
            "strides() called on an undefined Tensor",
            "negative dimensions are not allowed",
            "masked_fill_ only supports boolean masks, but got mask with dtype nullptr (uninitialized)",
            "Could not run 'aten::random_.from' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::random_.from' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31470 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44611 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:22277 [kernel]\nNamed: fallthrough registered at ../aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4942 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:19741 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_1.cpp:16033 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: registered at ../aten/src/ATen/functorch/BatchRulesRandomness.cpp:367 [kernel]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: registered at ../aten/src/ATen/VmapModeRegistrations.cpp:37 [kernel]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\n",
            "Index put requires the source and destination dtypes match, got Float for the destination and Int for the source.",
            "'complex32'",
            "qint16",
            "too many indices for tensor of dimension 0 (got 2)"
        ],
        "torch.isfinite": [],
        "torch.isin": [
            "negative dimensions are not allowed"
        ],
        "torch.isinf": [],
        "torch.isnan": [],
        "torch.isneginf": [],
        "torch.isposinf": [],
        "torch.isreal": [],
        "torch.kron": [
            "result type Float can't be cast to the desired output type Int"
        ],
        "torch.kthvalue": [
            "kthvalue(): Expected reduction dim 3 to have non-zero size.",
            "Name 'ZSPg' not found in Tensor[None, None, None, None, None, None, None].",
            "Name 'dHmL' not found in Tensor[]."
        ],
        "torch.lcm": [
            "\"lcm_cpu\" not implemented for 'Half'"
        ],
        "torch.ldexp": [],
        "torch.le": [
            "negative dimensions are not allowed"
        ],
        "torch.lerp": [
            "The size of tensor a (7) must match the size of tensor b (6) at non-singleton dimension 6",
            "expected dtype float for `end` but got dtype int",
            "Trying to resize storage that is not resizable",
            "result type Float can't be cast to the desired output type Int",
            "negative dimensions are not allowed",
            "Found dtype Int but expected Float"
        ],
        "torch.lgamma": [],
        "torch.log": [],
        "torch.log10": [],
        "torch.log1p": [],
        "torch.log2": [],
        "torch.log_softmax": [
            "Name 'dTBA' not found in Tensor[None, None, None].",
            "Name 'bfpL' not found in Tensor[].",
            "negative dimensions are not allowed"
        ],
        "torch.logaddexp": [
            "\"logaddexp_cpu\" not implemented for 'Int'",
            "\"logaddexp_cpu\" not implemented for 'Char'",
            "\"logaddexp_cpu\" not implemented for 'Long'",
            "negative dimensions are not allowed"
        ],
        "torch.logaddexp2": [
            "\"logaddexp2_cpu\" not implemented for 'ComplexDouble'",
            "\"logaddexp2_cpu\" not implemented for 'Long'",
            "'complex32'",
            "negative dimensions are not allowed"
        ],
        "torch.logcumsumexp": [
            "Name 'njxg' not found in Tensor[None, None].",
            "Name 'Ujgn' not found in Tensor[].",
            "negative dimensions are not allowed"
        ],
        "torch.logdet": [
            "logdet: Expected a floating point or complex tensor as input. Got Int"
        ],
        "torch.logical_and": [],
        "torch.logical_not": [
            "negative dimensions are not allowed"
        ],
        "torch.logical_or": [],
        "torch.logical_xor": [],
        "torch.logit": [
            "negative dimensions are not allowed"
        ],
        "torch.logsumexp": [
            "output with shape [] doesn't match the broadcast shape [1, 1, 1, 1, 1, 1, 1]",
            "Dimension out of range (expected to be in range of [-2, 1], but got 95)",
            ""
        ]
    }
}