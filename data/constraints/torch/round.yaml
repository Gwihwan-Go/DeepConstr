alias: torch.round
constraints:
  decimals:
    default: 0
    dtype: int
    init: false
    required: false
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 22
package: torch
pass_rate: 0.73
rules:
- cot: 'The error arises because the "round_cpu" operation is not implemented for
    ''ComplexDouble'' type. Looking at the given values, the input and output tensors
    are of type ''complex128'', which is equivalent to ''ComplexDouble''. Therefore,
    Left : input.dtype and out.dtype. It says that ''round_cpu'' operation is not
    available for ''ComplexDouble'', so Op : !=, and Right : ''complex128''.'
  target: '"round_cpu" not implemented for ''ComplexDouble'''
  txt: (out.dtype != 'complex128') or (input.dtype != 'complex128')
- cot: 'The error is due to the ''round_cpu'' function not being implemented for ''ComplexFloat''.
    It seems that the ''input'' and ''out'' tensors are of dtype ''complex64''. Therefore,
    Left : input.dtype and out.dtype should be corrected. "Op : !=", and Right : ''complex64''.
    It implies that the dtype should not be ''complex64'' for the ''round_cpu'' function
    to work properly.'
  target: '"round_cpu" not implemented for ''ComplexFloat'''
  txt: out.dtype != 'complex64'
- cot: 'This error is triggered because the dtype of the ''out'' tensor does not match
    the expected dtype. While the expected dtype is ComplexFloat, the dtype of the
    ''out'' tensor is ComplexDouble. Therefore, the dtype of the ''out'' tensor needs
    to be corrected. Thus, Left: out.dtype, Op: ==, Right: ComplexFloat. On the other
    hand, it should not be ComplexDouble, so Op: !=, Right: ComplexDouble.'
  target: Found dtype ComplexDouble but expected ComplexFloat
  txt: (out.dtype != ComplexDouble) and ((input.dtype != ComplexDouble) or ((type(out)
    == "ComplexFloat") and (type(input) == "ComplexFloat"))) and (out.dtype != ComplexDouble)
- cot: The error is due to the attempt to resize 'out' tensor which is not resizable.
    It seems that the shape of 'out' tensor does not match with the shape of 'input'
    tensor after the operation. In this case, the shape of 'input' tensor is [10],
    but the shape of 'out' tensor is [1]. Therefore, the shape of 'out' tensor should
    be equal to the shape of 'input' tensor.
  target: Trying to resize storage that is not resizable
  txt: out.shape == input.shape
time_cost: 2420.1251792907715
title: torch.round
tokens_used: 14795
trained: true
