alias: torch.logical_and
constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  other:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 20
package: torch
pass_rate: 1.0
rules:
- cot: 'Error is triggered because of size mismatch between tensor a and b at non-singleton
    dimension 0, Let''s see what the args were. It seems the size of tensor ''input''
    (a) and ''other'' (b) at non-singleton dimension 0 are different. Therefore, Left
    : input.size(0). op : ==, right : other.size(0).'
  target: The size of tensor a (8) must match the size of tensor b (6) at non-singleton
    dimension 0
  txt: input.size(0) == other.size(0)
- cot: "Given the error 'Trying to resize storage that is not resizable' and the values\
    \ provided for 'input', 'other' and 'out', the constraint that needs to be met\
    \ is that the shape and rank of the 'out' tensor should match the shape and rank\
    \ of the 'input' and 'other' tensors. \n\nHere, the 'input' and 'other' tensors\
    \ have the same shape [7, 5, 10] and their rank is 3, while the 'out' tensor has\
    \ a different shape [7, 7, 3] and the same rank. Hence, it seems the 'out' tensor's\
    \ shape is trying to be resized to match the 'input' and 'other' tensors, causing\
    \ the error. \n\nTo prevent the error:\n\n1. Ensure that the shape of the 'out'\
    \ tensor matches the shape of 'input' and 'other' tensors.\n2. Ensure that the\
    \ rank of the 'out' tensor matches the rank of 'input' and 'other' tensors.\n\n\
    So the logical relationship constraint should be:"
  target: Trying to resize storage that is not resizable
  txt: out.shape == input.shape
time_cost: 1529.13450050354
title: torch.logical_and
tokens_used: 12287
trained: true
