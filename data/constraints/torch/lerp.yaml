alias: torch.lerp
constraints:
  end:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
  weight:
    default: null
    dtype: float32,float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
infered_history: []
infered_times: 25
package: torch
pass_rate: 0.98
rules:
- cot: 'Error is triggered because we are trying to resize storage. In this case,
    the storage refers to the ''out'' tensor. The ''out'' tensor shape should match
    the operation results. In this operation, the result shape is same as ''input''
    tensor. Therefore, left : out.shape, out.rank and op : == right : input.shape,
    input.rank'
  target: Trying to resize storage that is not resizable
  txt: (weight.shape == input.shape) and (out.rank==input.rank and all(out.shape[i]==input.shape[i]
    for i in range(out.rank)))
- cot: 'The error indicates that the ''out'' tensor is of type Bool, but it was expected
    to be Float. Therefore, the data type of ''out'' should be corrected to match
    Float. It can be done by checking the type of ''out''. Thus, Left : out.dtype.
    It says expected type is Float, so Op : ==, and Right : float32. It also implies
    that the type cannot be Bool, so Op : !=, and Right : bool.'
  target: Found dtype Bool but expected Float
  txt: out.dtype == "float32"
- cot: 'The error occurs due to a mismatch in expected and actual ''weight'' tensor
    datatype. Expected dtype for ''weight'' is float, but got dtype short int. From
    the value information, dtype for ''weight'' is `int16` but it should be `float32`.
    Therefore, the dtype of ''weight'' tensor Op : ==, Right : float32.'
  target: expected dtype float for `weight` but got dtype short int
  txt: weight.dtype == 'float32'
- cot: 'The error is saying that the ''end'' tensor is of type complex128, but it
    was expected to be of type float. To prevent this error, the ''end'' tensor should
    be converted to float type before being used. Therefore, the ''end'' tensor data
    type, Op : ==, and Right : float32.'
  target: expected dtype float for `end` but got dtype c10::complex<double>
  txt: (end.dtype == input.dtype) and (end.dtype == "float32")
- cot: 'Error is triggered because size of tensors at non-singleton dimension 2 doesn''t
    match. What the args were? tensor a (8) is ''input'' with 8 on 2nd dim. So 2nd
    dim of ''input'' should be corrected. Therefore, Left : input.shape[2]. tensor
    b (2) is ''end'' which have 2 on 2nd dim. Therefore, Right : end.shape[2]'
  target: The size of tensor a (8) must match the size of tensor b (2) at non-singleton
    dimension 2
  txt: (all(end.shape[i] == weight.shape[i] or end.shape[i] == 1 or weight.shape[i]
    == 1 for i in range(-1, -min(len(end.shape), len(weight.shape))-1, -1)) and all(input.shape[i]
    == weight.shape[i] or input.shape[i] == 1 or weight.shape[i] == 1 for i in range(-1,
    -min(len(input.shape), len(weight.shape))-1, -1))) and ((len(end.shape)==len(weight.shape))
    and ((len(input.shape)==len(end.shape)) and (all(input.shape[i] == end.shape[i]
    for i in range(len(input.shape))))))
time_cost: 9936.803626537323
title: torch.lerp
tokens_used: 17264
trained: true
