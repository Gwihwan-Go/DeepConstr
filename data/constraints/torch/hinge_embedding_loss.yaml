constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  margin:
    default: 1.0
    dtype: float
    init: false
    required: false
  reduce:
    default: null
    dtype: bool
    init: false
    required: false
  reduction:
    default: mean
    dtype: Literal["none", "mean", "sum"]
    init: false
    required: false
  size_average:
    default: null
    dtype: bool
    init: false
    required: false
  target:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
package: torch
pass_rate: 0.95
rules:
- cot: 'Error is triggered because size of tensors at non-singleton dimension 9 doesn''t
    match. Let''s see the tensors ''input'' and ''target''. The ''input'' tensor has
    6 dimensions and the ''target'' tensor has 10 dimensions. The error occurs at
    dimension 9. However, ''input'' tensor doesn''t have dimension 9, so we can''t
    match its size with any dimension of ''target''. The problem is that the two tensors
    are not compatible for the operation since their dimensions are not equal and
    can''t be broadcasted. Therefore, the constraints should be:'
  target: The size of tensor a (2) must match the size of tensor b (6) at non-singleton
    dimension 9
  txt: (all(input[i] == target[i] for i in range(len(input)))) and ((input[8]
    == target[8]) and (len(input.shape) == len(target.shape) and all(input.shape[i]
    == target.shape[i] or input.shape[i] == 1 or target.shape[i] == 1 for i in range(-1,
    -min(len(input.shape), len(target.shape))-1, -1))))
title: torch.nn.functional.hinge_embedding_loss
