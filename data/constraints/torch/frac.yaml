alias: torch.frac
constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 18
package: torch
pass_rate: 0.78
rules:
- cot: 'The error is due to the "frac_cpu" operation not being implemented for ''Char''
    type. In this case, the input and output tensors are of ''int8'' type, which is
    equivalent to ''Char''. The operation "frac_cpu" should be applied on tensors
    of a different type. Therefore, the Left : type(input) or type(out) should be
    corrected. It says that it cannot be ''int8'' for operation "frac_cpu", so Op
    : !=, and Right : ''int8''.'
  target: '"frac_cpu" not implemented for ''Char'''
  txt: type(out) != 'int8'
- cot: The error is due to trying to resize a storage that is not resizable. Looking
    at the provided values, 'input' and 'out' tensors have different dimensions. In
    this case, the shape of 'out' must match the shape of 'input'. Hence, the constraint
    is that the shape of 'input' should be equal to the shape of 'out'.
  target: Trying to resize storage that is not resizable
  txt: input.shape == out.shape
- cot: 'The error is triggered because the dtype of ''out'' is float32, but it is
    expected to be float64 (Double). So, the dtype of ''out'' should be corrected
    to match the dtype of ''input''. Therefore, Left : type(out), Op : ==, and Right
    : type(input).'
  target: Found dtype Float but expected Double
  txt: type(out) == type(input)
time_cost: 642.9052736759186
title: torch.frac
tokens_used: 10827
trained: true
