alias: torch.float_power
constraints:
  exponent:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool,int
    init: false
    required: true
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool,int
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 38
package: torch
pass_rate: 0.35
rules:
- cot: 'The error is triggered because the function float_power() requires specific
    combinations of arguments, but the provided arguments do not match any of the
    expected combinations. Looking at the argument values, ''input'' is an integer,
    ''exponent'' is an integer, and ''out'' is a tensor. The allowed combinations
    are: (Tensor, Tensor, *, Tensor), (Number, Tensor, *, Tensor), or (Tensor, Number,
    *, Tensor). Therefore, the problem could be the type of ''input'' and ''exponent''.
    They should be either both Tensors or one of them should be number and the other
    one a Tensor. We can express this as:'
  target: "float_power() received an invalid combination of arguments - got (out=Tensor,\
    \ exponent=int, input=int, ), but expected one of:\n * (Tensor input, Tensor exponent,\
    \ *, Tensor out)\n * (Number self, Tensor exponent, *, Tensor out)\n * (Tensor\
    \ input, Number exponent, *, Tensor out)"
  txt: (type(exponent) == type(out)) and (type(exponent) == Number)
- cot: 'The error arises because the output given to float_power has dtype Float but
    the operation''s result requires dtype Double. From the values, it seems ''input''
    is of dtype int16, ''exponent'' is int32 and ''out'' is float32. Therefore, the
    dtype of ''out'' should be consistent with the operation''s result. Left : type(out),
    op : ==, comparator : Double'
  target: the output given to float_power has dtype Float but the operation's result
    requires dtype Double
  txt: (out.dtype == input.dtype) and (type(out) == Double)
- cot: 'Error is triggered because of resizing a non-resizable storage. Let''s consider
    the values given. The ''input'' tensor has a shape of [9, 4, 6, 2, 6, 6, 1, 1,
    4, 1] and the ''out'' tensor has a shape of [6, 2, 9, 2, 6, 6]. The ''out'' tensor
    should be the same shape as the ''input'' tensor to avoid resizing. Therefore,
    Left : out.shape, it implies that the shape of the output tensor is the problem,
    so Op : ==, and Right : input.shape.'
  target: Trying to resize storage that is not resizable
  txt: out.shape == input.shape
time_cost: 3054.6453120708466
title: torch.float_power
tokens_used: 28276
trained: true
