constraints:
  input:
    default: 'null'
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  other:
    default: 'null'
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: 'null'
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 45
package: torch
pass_rate: 0.0
rules:
- cot: 'The error is caused due to the ScalarType BFloat16 being unsupported. What
    the args were? The tensor ''input'' is of type BFloat16, which is causing the
    error. Therefore, Left : input.dtype. It says that cannot be BFloat16, so Op :
    !=, and Right : BFloat16.'
  target: Got unsupported ScalarType BFloat16
  txt: input.dtype != BFloat16
- cot: 'The error is triggered because the result type Half (float16) cannot be cast
    to the desired output type Int (int32). Let''s examine the types of values ''input'',
    ''other'', and ''out''. The dtype of ''input'' is float16, ''other'' is int16,
    and ''out'' is int32. Therefore, to prevent this error, the dtype of ''input''
    and ''out'' should be the same. Left : input.dtype, op : ==, comparator : out.dtype'
  target: result type Half can't be cast to the desired output type Int
  txt: (input.dtype in [Float32, Float64]) and ((other.dtype==out.dtype) and (input.dtype==out.dtype))
- cot: 'The error is triggered because we are trying to resize a storage that is not
    resizable. Let''s see what the args were. From the given values, ''input'', ''other'',
    and ''out'' are tensors. In this operation, the output tensor ''out'' must match
    the shape of the operation result which should be derived from operations on ''input''
    and ''other'' tensors. Thus, the shape of ''out'' tensor should be equal to the
    shape of the result after the operation on ''input'' and ''other'' tensors. Therefore,
    left : out.shape, op : ==, right : result.shape. However, to arrive at result.shape,
    both input and other tensors'' shapes should be equivalent. Therefore, left: input.shape,
    op: ==, right: other.shape.'
  target: Trying to resize storage that is not resizable
  txt: out.rank==input.rank and all(out.shape[i]==input.shape[i] for i in range(out.rank))
    and input.rank==other.rank and all(input.shape[i]==other.shape[i] for i in range(input.rank))
- cot: The error arises because the 'lshift_cpu' operation is not implemented for
    'Float' type tensors. The tensors 'input', 'other', and 'out' are all provided
    as float32, which is not compatible. Therefore, the type of these tensors should
    be corrected to a type that is compatible with the 'lshift_cpu' operation.
  target: '"lshift_cpu" not implemented for ''Float'''
  txt: out.dtype != 'float32'
time_cost: 6384.06613612175
title: torch.bitwise_left_shift
tokens_used: 28444
trained: true
