constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  other:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 18
package: torch
pass_rate: 1.0
rules:
- cot: "The error is triggered because the result type Long cannot be cast to Bool.\
    \ It seems that the types of 'input', 'other', and 'out' are inconsistent, being\
    \ int64, int16, and Bool respectively. Therefore, the types of these arguments\
    \ should be consistent to avoid the error. \nLeft : type(input), op : ==, comparator\
    \ : type(other) or type(out)"
  target: result type Float can't be cast to the desired output type Int
  txt: type(input)==type(other)
- cot: "The error is triggered because we are trying to resize 'out' tensor's storage\
    \ which is not resizable. Let's look at the provided values. The 'out' tensor\
    \ has the same rank as 'input' and 'other', but its last dimension is one less\
    \ than 'input' and 'other'. Therefore, 'out' tensor shape should be exactly matched\
    \ with the 'input' and 'other' tensor shape. \n\nTherefore, left : out.shape,\
    \ out.dtype op : ==, == right : input.shape, input.dtype and same constraints\
    \ should be applied to 'other' tensor."
  target: Trying to resize storage that is not resizable
  txt: out.shape == input.shape and out.dtype == input.dtype and out.shape == other.shape
    and out.dtype == other.dtype
- cot: 'The error is caused by the ''fmax'' operation not being implemented for complex
    tensors. The ''input'' tensor is of dtype complex64, which is causing the issue.
    Therefore, the dtype of ''input'' tensor needs to be corrected. Left : ''input.dtype'',
    It says that dtype cannot be complex, so Op : !=, and Right : complex64.'
  target: fmax not implemented for complex tensors.
  txt: (type(other) not in ["complex64", "complex128"]) or (input.dtype == int64)
time_cost: 3766.2505621910095
title: torch.fmax
tokens_used: 11329
trained: true
