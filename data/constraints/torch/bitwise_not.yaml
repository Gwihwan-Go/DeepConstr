alias: torch.bitwise_not
constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 15
package: torch
pass_rate: 0.95
rules:
- cot: 'This error arises because an attempt was made to resize a tensor that isn''t
    resizable. Let''s analyze the input and output tensors. The input tensor has the
    shape [4, 4, 7, 10, 8, 9] and the output tensor has the shape [4, 1, 3, 8, 6].
    The output tensor shape does not match the input tensor shape. We need to ensure
    that the tensor shapes match before resizing. Therefore, the left: input.shape,
    op: ==, and right: out.shape.'
  target: Trying to resize storage that is not resizable
  txt: input.shape == out.shape
- cot: 'The error is due to the operation ''bitwise_not_cpu'' not being implemented
    for ''Float'' datatype. Here, both ''input'' and ''out'' are of type ''float32''.
    The ''bitwise_not_cpu'' operation is not compatible with the ''float32'' datatype.
    Therefore, the datatype of ''input'' and ''out'' should be corrected to be compatible
    with the ''bitwise_not_cpu'' operation. As a constraint, the datatype of ''input''
    and ''out'' should not be ''float32'', so Op: !=, and Right: ''float32''.'
  target: '"bitwise_not_cpu" not implemented for ''Float'''
  txt: (out.dtype != 'float32')
- cot: 'The error is caused because the output tensor has a dtype of ComplexDouble
    while the expected dtype is Float. So, there''s a mismatch between the expected
    and actual output type.


    To generate constraints that do not trigger this error, we need to ensure that
    the type of the output tensor is always Float. This means the dtype of ''out''
    must be the same as the dtype of ''input'' which is Float32.


    The constraint for the ''out'' tensor dtype can be expressed as:'
  target: Found dtype ComplexDouble but expected Float
  txt: type(out) == type(input)
time_cost: 624.2675673961639
title: torch.bitwise_not
tokens_used: 8146
trained: true
