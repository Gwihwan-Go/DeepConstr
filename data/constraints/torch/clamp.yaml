alias: torch.clamp
constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  max:
    default: null
    dtype: int32,int64,int8,int16,float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
  min:
    default: null
    dtype: int32,int64,int8,int16,float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
dupe_with: torch.clamp
infered_history: []
infered_times: 6
package: torch
pass_rate: 0.05
rules:
- cot: 'The error is triggered because size of tensors at non-singleton dimension
    4 doesn''t match. What the args were? the two tensors matching problem at non-singleton
    dimension seems like a broadcasting request. Broadcasting in tensor operations
    requires that the trailing dimensions of the tensors are either 1 or the same.
    We start from the last dimension because broadcasting aligns dimensions from the
    end. From the values we see that the tensor at position 4 is of size 6 in ''input''
    tensor and size 2 in ''min'' tensor. Therefore, we can revise the constraints
    as follows:'
  target: The size of tensor a (6) must match the size of tensor b (2) at non-singleton
    dimension 4
  txt: (input.shape[4] == max.shape[0]) and (all(input.shape[i] == min.shape[i] or
    input.shape[i] == 1 or min.shape[i] == 1 for i in range(-1, -min(len(input.shape),
    len(min.shape))-1, -1)))
skipped: true
skipped_reason: dupe
time_cost: 185.72692918777466
title: torch.clamp
tokens_used: 3952
