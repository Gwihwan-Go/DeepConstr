constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  other:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 27
package: torch
pass_rate: 0.0
rules:
- cot: 'Error is triggered because the size of tensors at non-singleton dimension
    7 doesn''t match. It looks like a tensor size matching problem at non-singleton
    dimension. Given the values, it seems like a broadcasting request where the last
    dimensions of the tensors should be either 1 or the same. Let''s see what the
    args were. Therefore, we can revise the constraints as follows:'
  target: The size of tensor a (6) must match the size of tensor b (4) at non-singleton
    dimension 7
  txt: all(input.shape[i] == other.shape[i] or input.shape[i] == 1 or other.shape[i]
    == 1 for i in range(-1, -min(len(input.shape), len(other.shape))-1, -1))
- cot: 'Error occurs when trying to resize a storage that is not resizable. The probable
    cause is that the ''input'', ''other'' or ''out'' have been declared as non-resizable,
    but later in the code there is an attempt to resize them. Let''s see what the
    values were. ''input'', ''other'' and ''out'' are tensors of float32 type with
    respective dimensions. The tensors may not be resizable, so Left : ''input'',
    ''other'' or ''out'' tensor dimension. It implies that resizing them is a problem,
    so Op : ==, and Right : their initial dimensions.'
  target: Trying to resize storage that is not resizable
  txt: (other.rank <= out.rank) and ((out.rank==input.rank and all(out.shape[i]==input.shape[i]
    for i in range(out.rank))) and ('out'.shape == [8, 7, 6]))
- cot: The error is indicating that the "lcm_cpu" operation is not implemented for
    the 'Float' datatype. The tensors 'input', 'other', and 'out' are all of type
    float32 which is not compatible with the 'lcm_cpu' operation. This operation might
    only be compatible with certain integer types (i.e., int8, int16, int32, int64,
    uint8, uint16, uint32, uint64). Thus, the data type of 'input', 'other', and 'out'
    need to be validated before they are passed into the 'lcm_cpu' operation. The
    constraint is that the type of these tensors must be within the allowed types.
  target: '"lcm_cpu" not implemented for ''Float'''
  txt: type(out) in ["int8", "int16", "int32", "int64", "uint8", "uint16", "uint32",
    "uint64"]
time_cost: 6973.982981204987
title: torch.lcm
tokens_used: 15981
trained: true
