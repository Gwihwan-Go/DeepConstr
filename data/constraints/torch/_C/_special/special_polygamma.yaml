constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  n:
    default: null
    dtype: int
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 25
package: torch
pass_rate: 0.97
rules:
- cot: 'The output data type is ''Short'' but the result data type is ''Float''. The
    output data type and the result data type should be the same to prevent this error.
    The output data type should be changed from ''Short'' to ''Float'' or the result
    data type should be changed from ''Float'' to ''Short''. Let''s see what the ''out''
    and ''input'' were. The type of ''out'' is ''Short'' and the type of ''input''
    is ''Float'', but we don''t know which one should be changed. Therefore, Left
    : out.dtype or input.dtype. Op : == Right : Float'
  target: result type Float can't be cast to the desired output type Short
  txt: (input.dtype == out.dtype) and ((out.dtype != Short) and (input.dtype==Float))
- cot: 'The error is triggered because we''re trying to resize a storage that isn''t
    resizable. This means that the ''out'' tensor shape has to match the operation
    results. In this case, the result shape is the shape of ''input''. From the given
    values, we can see that ''out'' and ''input'' have different shapes. Therefore,
    the Left : out.shape, out.rank; Op : ==; and Right : input.shape, input.rank.'
  target: Trying to resize storage that is not resizable
  txt: (len(input) == n) and (out.rank==input.rank and all(out.shape[i]==input.shape[i]
    for i in range(out.rank)))
time_cost: 25016.579087257385
title: torch._C._special.special_polygamma
tokens_used: 11982
trained: true
