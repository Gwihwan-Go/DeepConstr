constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  other:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 38
package: torch
pass_rate: 0.83
rules:
- cot: 'Error is triggered because size of tensors at non-singleton dimension 9 doesn''t
    match. What the args were? the two tensors matching problem at non-singleton dimension
    looks like broadcasting request. Broadcasting in tensor operations requires that
    the trailing dimensions of the tensors are either 1 or the same. We start from
    the last dimension because broadcasting aligns dimensions from the end. Therefore,
    we can revise the constraints as follows:'
  target: The size of tensor a (2) must match the size of tensor b (6) at non-singleton
    dimension 9
  txt: all(input.shape[i] == other.shape[i] or input.shape[i] == 1 or other.shape[i]
    == 1 for i in range(-1, -min(len(input.shape), len(other.shape))-1, -1))
- cot: "From the provided values, it seems the error is triggered because the 'out'\
    \ tensor is trying to be resized to match the 'input' tensor. However, the 'out'\
    \ tensor is not resizable. From the size of the tensors, it's clear that the 'out'\
    \ tensor does not match the 'input' tensor in dimensions. \n\nTherefore, to prevent\
    \ this error, we should ensure that the 'out' tensor has the same dimensions as\
    \ the 'input' tensor before the operation. Also, the \"out\" tensor should have\
    \ the capacity to be resized.\n\nThe constraints can be formulated as below:"
  target: Trying to resize storage that is not resizable
  txt: (other.rank==input.rank and all(other.shape[i]==input.shape[i] for i in range(other.rank)))
- cot: 'The error is caused because the result type Float cannot be cast to the desired
    output type Int. Let''s see what the args were. The input and the other tensors
    are of type float32 and the output tensor is of type int32. Therefore, the output
    tensor type should be changed to match the input tensor type. So, Left : ''out''
    tensor type, Op : ==, Right : ''input'' tensor type.'
  target: result type Float can't be cast to the desired output type Int
  txt: ((input.dtype==Float) and ((out.dtype==ComplexDouble) or (out.dtype == float32)))
    or (out.dtype==ComplexDouble) or ((input.dtype==Float) and ((out.dtype==ComplexDouble)
    or (out.dtype == float32)))
time_cost: 25964.84917998314
title: torch._C._special.special_zeta
tokens_used: 22483
trained: true
