constraints:
  n:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
  x:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
infered_history: []
infered_times: 29
package: torch
pass_rate: 0.93
rules:
- cot: 'The error is caused because the result of a complex operation cannot be cast
    to the float type. Here, ''n'' is of complex type and ''x'' and ''out'' are of
    float type. So the operation involving ''n'' leads to a ComplexDouble result that
    can''t be cast to Float. Thus, the datatype of ''n'' should be changed to float
    to prevent this error. Therefore, Left : n.dtype, Op : ==, Right : tf.float32.
    Also, to ensure the output is also of the same type, Left : out.dtype, Op : ==,
    Right : tf.float32.'
  target: result type Float can't be cast to the desired output type Short
  txt: out.dtype == tf.float32
- cot: 'Error is triggered because size of tensors at non-singleton dimension 8 doesn''t
    match. What the args were? the two tensors matching problem at non-singleton dimension
    looks like broadcasting request. Broadcasting in tensor operations requires that
    the trailing dimensions of the tensors are either 1 or the same. Based on the
    given tensor sizes, we can revise the constraints as follows:'
  target: The size of tensor a (8) must match the size of tensor b (7) at non-singleton
    dimension 8
  txt: ('n'[8] == 'n'[7]) and (all(x.shape[i] == n.shape[i] or x.shape[i] == 1 or
    n.shape[i] == 1 for i in range(-1, -min(len(x.shape), len(n.shape))-1, -1)))
- cot: 'Error is triggered because of trying to resize a tensor that is not resizable.
    Let''s see what the args were. The ''out'' tensor size doesn''t match the ''x''
    tensor size. So, the constraint would be that the ''out'' tensor must have the
    same size as the ''x'' tensor. Therefore, Left : out.size, Op : ==, Right : x.size'
  target: Trying to resize storage that is not resizable
  txt: (max(n.shape) <= 1) and (out.size == x.size)
time_cost: 30783.06796169281
title: torch._C._special.special_laguerre_polynomial_l
tokens_used: 19151
trained: true
