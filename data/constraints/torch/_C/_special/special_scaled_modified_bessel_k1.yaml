constraints:
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
  x:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
infered_history: []
infered_times: 23
package: torch
pass_rate: 0.62
rules:
- cot: 'The error occurs because the result type Float cannot be cast to the desired
    output type Short. Let''s see what the args were. The dtype of ''out'' is int16
    (short) and ''x'' is float32. Therefore, the dtypes of two arguments should be
    consistent. Left : out.dtype, op : ==, comparator : x.dtype or type(result)'
  target: result type Float can't be cast to the desired output type Short
  txt: out.dtype==x.dtype
- cot: 'The error is due to the function "scaled_modified_bessel_k1_cpu" not supporting
    the data type ''ComplexFloat''. In the given values, both ''out'' and ''x'' tensors
    have the dtype ''complex64'' which corresponds to ''ComplexFloat''. Therefore,
    the data type of both ''out'' and ''x'' should be corrected so they do not trigger
    the error. Hence, Left : dtype of ''out'' and ''x'', Op : !=, Right : ''complex64''.'
  target: '"scaled_modified_bessel_k1_cpu" not implemented for ''ComplexFloat'''
  txt: x.dtype != complex64
- cot: 'The error is triggered because we are trying to resize a tensor that is not
    resizable. From the values given, it seems the ''out'' tensor shape doesn''t match
    the ''x'' tensor shape. Therefore, we need to ensure that ''out'' tensor shape
    matches with the ''x'' tensor shape. Hence, left : out.shape, out.rank, op : ==,
    right : x.shape, x.rank.'
  target: Trying to resize storage that is not resizable
  txt: (out.rank == x.rank and all(out.shape[i] ==x.shape[i] for i in range(out.rank)))
time_cost: 1563.983546257019
title: torch._C._special.special_scaled_modified_bessel_k1
tokens_used: 12301
trained: true
