constraints:
  input:
    default: 'null'
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  norm:
    default: 'null'
    dtype: Literal['forward', 'backward', 'ortho']
    init: false
    required: false
  out:
    default: 'null'
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 61
package: torch
pass_rate: 0.15
rules:
- cot: 'The error is due to invalid configuration parameters in the MKL FFT operation.
    The problem most likely lies in the input tensor''s shape or the ''backward''
    norm used in the FFT operation. Let''s see what the args were. The dimensions
    of the input and output tensors seem to be different which could be causing the
    issue, so the dimensions of input and out should be same. Therefore, Left : input.shape,
    Op : ==, and Right : out.shape. As for the ''backward'' norm, there isn''t enough
    information to generate a specific constraint, but it''s worth verifying if this
    is a valid option for the norm parameter in the MKL FFT operation.'
  target: 'MKL FFT error: Intel MKL DFTI ERROR: Invalid configuration parameters'
  txt: (input.dim == out.dim) or ((input.shape[-1] == 2) and (input.shape == out.shape))
- cot: 'The error is triggered because the ''out'' tensor shape is not the same as
    the ''input'' tensor shape and yet we are trying to resize it. The ''out'' tensor
    is not resizable because its dimensions are fixed. So, the dimensions of the ''input''
    tensor and the ''out'' tensor must be the same to prevent the error. Therefore,
    left : out.shape, right : input.shape, and op : ==.'
  target: Trying to resize storage that is not resizable
  txt: (norm == 'forward') and ((all(input.shape[i] == out.shape[i] for i in range(len(input.shape))))
    and (all(out.shape[i] == input.shape[i] for i in range(len(out.shape)))))
- cot: 'The error message indicates that an invalid number of data points, specifically
    0, have been provided. In order to prevent this error, we need to ensure that
    the input data contains at least one data point.


    The input tensor in this case has a shape of [4, 1, 3, 2, 7, 2, 6, 6, 8, 1], which
    means it has multiple dimensions and each dimension has a certain number of data
    points. To prevent the error, we need to ensure that none of these dimensions
    has 0 data points.


    Therefore, the constraints can be expressed as:'
  target: Invalid number of data points (0) specified
  txt: all(i > 0 for i in input.shape)
- cot: 'The error indicates that the hfftn function requires a floating point output
    tensor, but a complex data type tensor is provided. Therefore, the data type of
    the ''out'' tensor should be a floating point data type, not complex. So, the
    constraint would be: Left : out.dtype, Op : ==, Right : float'
  target: hfftn expects a floating point output tensor, but got Int
  txt: out.dtype == float
- cot: 'The error message states that the hfftn function must transform at least one
    axis. The inputs are a scalar integer tensor ''input'', a string ''norm'' with
    value ''forward'', and a 3D float32 tensor ''out''. However, there is no given
    axis for the hfftn to transform. So, there is no dimension to perform the operation
    on. The hfftn function transforms along all axes by default if no axis is specified,
    but in this case, the ''input'' tensor is a scalar and doesn''t have any axes.
    To avoid this, we need to make sure that the ''input'' tensor has at least one
    dimension. Therefore, the constraint could be:'
  target: hfftn must transform at least one axis
  txt: input.dim > 0
- cot: The error suggests that the dtype 'Half' is unsupported. Given that the 'input'
    tensor has dtype of float16 (Half), this is likely causing the error. Similarly,
    the 'out' tensor has dtype of float32 which might not be compatible with some
    operations. Therefore, the dtype of 'input' and 'out' tensors should be changed
    to a supported type.
  target: Unsupported dtype Half
  txt: input.dtype in ["int16", "int32", "int64", "uint8", "uint16", "uint32", "uint64",
    "float32", "float64"]
time_cost: 9154.930923223495
title: torch._C._fft.fft_hfftn
tokens_used: 40786
trained: true
