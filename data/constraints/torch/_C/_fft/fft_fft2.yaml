constraints:
  input:
    default: 'null'
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  norm:
    default: 'null'
    dtype: Literal['forward', 'backward', 'ortho']
    init: false
    required: false
  out:
    default: 'null'
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 25
package: torch
pass_rate: 0.72
rules:
- cot: 'Error is triggered because we are trying to resize ''out'' tensor and it''s
    not resizable. Let''s see what the args were. In this case, the ''input'' tensor
    has a shape of [6, 8, 6, 6, 1, 3, 4, 1, 3] and the ''out'' tensor has a shape
    of [2, 8, 2, 6, 1, 8, 1, 1, 2]. So, it means that ''out'' tensor shape should
    be matched with the ''input'' tensor shape. Therefore, left : out.shape, out.rank
    op : == right : input.shape, input.rank'
  target: Trying to resize storage that is not resizable
  txt: out.rank==input.rank and all(out.shape[i]==input.shape[i] for i in range(out.rank))
- cot: 'The error appears due to the dimension being out of range. In this case, the
    function is expecting a dimension in the range of [-1, 0] but got -2. In this
    context, the range is determined by the rank of the tensor minus 1 (-n to n-1).
    The rank of the given tensor is 1, hence the range becomes [-1, 0]. The ''dim''
    argument in this operation is -2, which is out of the valid range.


    We need to ensure that the ''dim'' value is always within the valid range of the
    tensor''s rank. This range is [-n, n-1] (inclusive), where n is the rank of the
    tensor.


    To prevent this error, the constraint for the ''dim'' parameter can be expressed
    as:'
  target: Dimension out of range (expected to be in range of [-1, 0], but got -2)
  txt: input.dtype == out.dtype and input.shape == out.shape
- cot: 'The error arises because the function fftn expects a complex output tensor,
    however the ''out'' tensor is of type Float32. This can be prevented by ensuring
    that the output tensor ''out'' is of a complex type. Therefore, the relationship
    constraint can be defined as: Left : type(out), Op : ==, Right : complex.'
  target: fftn expects a complex output tensor, but got Float
  txt: type(out)==complex
time_cost: 994.1661493778229
title: torch._C._fft.fft_fft2
tokens_used: 17066
trained: true
