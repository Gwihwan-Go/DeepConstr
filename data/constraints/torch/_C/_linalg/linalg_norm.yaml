constraints:
  dim:
    default: null
    dtype: int
    init: false
    required: false
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  keepdim:
    default: false
    dtype: bool
    init: false
    required: false
  ord:
    default: null
    dtype: int,float
    init: false
    required: false
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 12
package: torch
pass_rate: 0.98
rules:
- cot: 'The error arises because there is an attempt to resize a storage that is not
    resizable. From the given values, it is not clear which value is responsible for
    the resizing. Therefore, we must ensure that the ''input'' tensor, which has the
    shape [9, 8, 6, 8, 6, 6, 1], matches the output tensor, which is expected to have
    the shape [3, 6, 8]. Since the ''input'' tensor''s shape does not match the ''output''
    tensor''s shape, the operation is attempting to resize the ''input''. Therefore,
    Left : input.shape, Op : ==, and Right : out.shape.'
  target: Trying to resize storage that is not resizable
  txt: input.shape == out.shape
- cot: 'The error is due to the ''dim'' value being out of the valid range for the
    ''input'' tensor. The function is trying to access the dimension 7 of the input
    tensor, but its shape is []. Therefore, the valid dimension range for the given
    tensor is [-1, 0]. To prevent this error in the future, the ''dim'' value should
    always be in the range [-n, n-1], where n is the number of dimensions in the ''input''
    tensor.


    The constraint for the ''dim'' parameter can be expressed as:'
  target: Dimension out of range (expected to be in range of [-1, 0], but got 7)
  txt: dim < len(input.shape)
- cot: 'The error has occurred because the data type of ''out'' tensor is not matching
    with the expected data type. The function linalg.norm expects the ''out'' tensor
    to be of dtype float, but it is of dtype int16. So, the constraint to prevent
    this error will be:'
  target: 'linalg.norm expected out tensor dtype Float but got: Short'
  txt: (out.dtype == torch.float32 or out.dtype == torch.float64) and (type(out) ==
    type(input))
time_cost: 623.2399981021881
title: torch._C._linalg.linalg_norm
tokens_used: 7993
trained: true
