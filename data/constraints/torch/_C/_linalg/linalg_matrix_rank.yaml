constraints:
  atol:
    default: null
    dtype: float32,float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
  hermitian:
    default: false
    dtype: bool
    init: false
    required: false
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
  rtol:
    default: null
    dtype: float32,float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 51
package: torch
pass_rate: 0.2
rules:
- cot: 'Error is triggered because the size of tensor a (atol) doesn''t match the
    size of tensor b (input and rtol) at non-singleton dimension 0. Let''s see what
    the args were. It seems the first dimension of ''atol'' tensor is not same as
    ''input'' and ''rtol''. Therefore, Left : atol.shape[0]. Op : ==, and Right :
    input.shape[0] or rtol.shape[0].'
  target: The size of tensor a (2) must match the size of tensor b (8) at non-singleton
    dimension 0
  txt: (atol.shape[0] == input.shape[0]) or ((all(atol.shape[i] == rtol.shape[i] or
    atol.shape[i] == 1 or rtol.shape[i] == 1 for i in range(-1, -min(len(atol.shape),
    len(rtol.shape))-1, -1))) or ((all(input.shape[i] == rtol.shape[i] or input.shape[i]
    == 1 or rtol.shape[i] == 1 for i in range(-1, -min(len(input.shape), len(rtol.shape))-1,
    -1))) and (atol.shape[0] == input.shape[0])))
- cot: 'The error is due to an attempt to resize a storage that is not resizable.
    This could be due to the dimensions of the input tensors. For instance, the ''input''
    tensor has a dimension of [3, 10, 10, 1, 1, 8, 3], while the ''out'' tensor has
    a dimension of [4, 1, 3, 8]. Therefore, the dimensions of ''input'' and ''out''
    tensors should be the same. Additionally, ''atol'' and ''rtol'' tensors have different
    dimensions. Therefore, the dimensions of ''atol'' and ''rtol'' tensors should
    be same. This can be checked using the following constraints:'
  target: Trying to resize storage that is not resizable
  txt: (hermitian == False) and (len(atol) == len(rtol))
- cot: 'Error is triggered because the input tensor does not have at least 2 dimensions,
    Let''s see what the args were. The input tensor dimension is less than 2. Therefore,
    Left : input.dim. Op : >=, and Right : 2.'
  target: 'torch.linalg.matrix_rank: The input tensor input must have at least 2 dimensions.'
  txt: input.dim >= 2
- cot: 'This error is triggered because a complex data type is not supported in the
    ''atol'' tensor for the torch.linalg.matrix_rank operation. Looking at the arguments,
    ''atol'' tensor is of complex128 type which is not supported. To prevent this
    error, constrain the ''atol'' tensor to only support non-complex data types. Therefore,
    the constraint can be:'
  target: 'torch.linalg.matrix_rank: atol tensor of complex type is not supported.
    Got ComplexDouble'
  txt: atol.dtype != torch.complex128
- cot: "The error arises due to the output tensor 'out' being of Bool data type while\
    \ the function torch.linalg.matrix_rank expects the output to be safely castable\
    \ from Long data type. Let's see what the args were. The dtype of 'out' seems\
    \ to be the cause of the issue. \n\nTherefore, the constraint that needs to be\
    \ corrected would be the data type of 'out'. The data type of 'out' should be\
    \ 'Long'. Hence, Left : out.dtype Op : == Right : Long."
  target: 'torch.linalg.matrix_rank: Expected result to be safely castable from Long
    dtype, but got result with dtype Bool'
  txt: out.dtype == Long
time_cost: 23972.046797037125
title: torch._C._linalg.linalg_matrix_rank
tokens_used: 35740
trained: true
