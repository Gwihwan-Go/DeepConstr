constraints:
  dim:
    default: 'null'
    dtype: int
    init: false
    required: false
  keepdim:
    default: 'false'
    dtype: bool
    init: false
    required: false
  ord:
    default: '2'
    dtype: int,float
    init: false
    required: false
  out:
    default: 'null'
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
  x:
    default: 'null'
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
infered_history: []
infered_times: 51
package: torch
pass_rate: 0.67
rules:
- cot: 'The error is due to an invalid dimension index. What are the args? The function
    is trying to perform an operation on dimension 1 of the input tensor ''x'', but
    the error message indicates that valid dimensions are in the range of [-1, 0].
    Therefore, the ''dim'' value should be a valid dimension of the input tensor ''x''.
    So, Left : dim, Op : <, Right : len(x.shape) and Left : dim, Op : >=, Right :
    -len(x.shape). Also, the ''out'' tensor has a shape of [3] which does not match
    with the shape of the input tensor ''x''. The shapes should be compatible for
    the operation. Hence, Left : x.shape, Op : ==, Right : out.shape.'
  target: Dimension out of range (expected to be in range of [-1, 0], but got 1)
  txt: dim < len(x.shape)
- cot: 'The error is triggered because of an attempt to resize a storage that is not
    resizable. Looking at the values, ''x'' is a tensor of complex128 type with dimensions
    [9,2] and ''out'' is a tensor of float64 type with no dimensions. The problem
    might be due to the incompatible ''x'' and ''out'' tensors. There are no constraints
    that specify the dimensionality or type of ''out'' tensor. It should be the same
    as ''x'' in order to avoid resizing issues. Therefore, Left : out.dtype, Op :
    ==, and Right : x.dtype.'
  target: Trying to resize storage that is not resizable
  txt: out.shape == x.shape
- cot: 'Error is triggered because the input tensor x is of type Char. It expects
    a floating point or complex tensor. Let''s see what the args were. The argument
    x : Tensor: int8:[10, 9] is the problem. Therefore, the datatype of
    x should be corrected, which is x.dtype. Left : x.dtype. It says that expected
    floating point or complex, so Op : ==, and Right : float or complex. It implies
    that datatype cannot be Char, so Op : !=, and Right : Char.'
  target: 'linalg.vector_norm: Expected a floating point or complex tensor as input.
    Got Char'
  txt: x.dtype == float or x.dtype == complex
- cot: 'The error is triggered because the ''out'' tensor''s datatype is complex128,
    but it is expected to be double. Let''s see what the args were. The ''out'' tensor
    has dtype complex128. Therefore, the dtype of ''out'' tensor should be corrected.
    Left : type(''out''), it says that should be equal to double, so Op : ==, and
    Right : double. It also implies that ''out'' tensor''s dtype cannot be complex128,
    so Op : !=, and Right : complex128.'
  target: Expected out tensor to have dtype double, but got c10::complex<double> instead
  txt: out.dtype != complex128
- cot: The error occurs because the data type of the output tensor is integer, but
    the function expects it to be float. It seems the 'out' tensor has a type of int32,
    which is not matching with the expected type - float. So, the constraint we need
    to add is to make sure the 'out' tensor data type is the same as the 'x' tensor
    data type.
  target: Expected out tensor to have dtype float, but got double instead
  txt: out.dtype == float
time_cost: 4243.395208835602
title: torch._C._linalg.linalg_vector_norm
tokens_used: 33781
trained: true
