constraints:
  init:
    default: 0.25
    dtype: float
    init: true
    required: false
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  num_parameters:
    default: 1
    dtype: int
    init: true
    required: false
infered_history: []
infered_times: 27
package: torch
pass_rate: 0.7
rules:
- cot: 'Error is triggered because the number of parameters does not match with the
    input channel size. What the args were? num_parameters is 2, and channel size
    which is the second value of input shape, is 6. Therefore, the constraints to
    avoid this error would be that the num_parameters must be equal to the second
    dimension of the input tensor. Therefore, we can revise the constraints as follows:'
  target: Mismatch of parameter numbers and input channel size. Found parameter numbers
    = 2 and channel size = 6.
  txt: (num_parameters == input.shape[-1]) or ((num_parameters == input.shape[0])
    and (num_parameters == input.shape[1]))
- cot: 'The error is caused because prelu operation does not support type promoting
    between Char and Float. Let''s see what the args were. It seems the ''init'' value
    is a Float while ''input'' is a Tensor of type Int8. This inconsistency in data
    types is causing the error. Therefore, the data types of ''init'' and ''input''
    should be consistent for prelu operation. Let''s consider Left : type(init), op
    : ==, and Right : input.dtype.'
  target: Not allow zero-dim input tensor.
  txt: len(input.shape) > 0
- cot: 'Error is triggered because of trying to create a tensor with a negative dimension.
    It expects to have non-negative dimension. What the args were? It seems ''num_parameters''
    has a negative value which is causing the issue. Therefore, Left : num_parameters.
    It implies that negative values are problem, so Op : >=, and Right : 0.'
  target: 'Trying to create tensor with negative dimension -1: [-1]'
  txt: num_parameters >= 0
time_cost: 846.7133839130402
title: torch.nn.PReLU
tokens_used: 18797
trained: true
