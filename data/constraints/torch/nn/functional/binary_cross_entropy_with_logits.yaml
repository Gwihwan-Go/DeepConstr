alias: torch.nn.functional.binary_cross_entropy_with_logits
constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  pos_weight:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
  reduction:
    default: null
    dtype: Literal["none", "mean", "sum"]
    init: false
    required: true
  target:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  weight:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
package: torch
pass_rate: 0.03
rules:
- cot: 'Error is triggered because the size of input tensor and target tensor do not
    match. In order to prevent this error, we need to ensure that the size of the
    input tensor and target tensor are the same. Therefore, the constraint should
    be:'
  target: Target size (torch.Size([2, 1, 3, 8, 1, 8, 9])) must be the same as input
    size (torch.Size([2]))
  txt: input.shape == target.shape
- cot: 'Based on this error, it seems that the dimensions of the tensors do not match.
    The error message indicates that the size of tensor a (7) must match the size
    of tensor b (2) at non-singleton dimension 1. Looking at the given tensors, it
    seems that the ''input'', ''target'', ''weight'', and ''pos_weight'' tensors might
    be related to this error. We can generate the following constraints:

    - The dimension 1 of ''input'' and ''target'' tensors should match the dimension
    0 of ''pos_weight'' tensor.

    - The dimension 0 of ''weight'' tensor should match the dimension 1 of ''input''
    and ''target'' tensors.

    - The dimension 2 of ''weight'' tensor should match the dimension 0 of ''pos_weight''
    tensor.


    Therefore, the constraints can be written as:'
  target: The size of tensor a (7) must match the size of tensor b (2) at non-singleton
    dimension 1
  txt: (all(input.shape[i] == pos_weight.shape[i] or input.shape[i] == 1 or pos_weight.shape[i]
    == 1 for i in range(-1, -min(len(input.shape), len(pos_weight.shape))-1, -1)))
    and ((pos_weight.shape[1] == len(weight)) or (input.shape[1] == pos_weight.shape[0]))
skipped: true
skipped_reason: Segmentation fault (core dumped)
title: torch.nn.functional.binary_cross_entropy_with_logits
