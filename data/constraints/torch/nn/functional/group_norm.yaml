constraints:
  bias:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
  eps:
    default: 1e-05
    dtype: float
    init: false
    required: false
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  num_groups:
    default: null
    dtype: int
    init: false
    required: true
  weight:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 54
package: torch
pass_rate: 0.0
rules:
- cot: 'The error is triggered because the dtype of the input parameters is mixed,
    which is not allowed. Let''s see what the args were. The ''input'' had a dtype
    of int32, ''weight'' had a dtype of float32, and ''bias'' had a dtype of int8.
    However, the system expects all parameters to have a scalar type of Float. Therefore,
    the dtypes of all the parameters should be consistent and of type Float. Left
    : input.dtype, op : ==, comparator : weight.dtype, bias.dtype, type(eps)'
  target: 'mixed dtype (CPU): expect parameter to have scalar type of Float'
  txt: input.dtype==weight.dtype
- cot: 'The error is triggered by an unsupported implementation of GroupNormKernelImpl
    for ''Char'' type. Looking at the args, the problematic tensor seems to be of
     int8 type. Therefore, the Tensors should not be of ''Char'' type or
    specifically, int8 type. Therefore, Left : input.dtype, weight.dtype, bias.dtype.
    It says that GroupNormKernelImpl not implemented for ''Char'', so Op : !=, and
    Right : ''Char'' or ''int8''.'
  target: '"GroupNormKernelImpl" not implemented for ''Char'''
  txt: weight.dtype != 'int8'
- cot: 'The error is triggered because a Double type is expected but a Float type
    is found. Let''s see the what''s args were. The dtype of ''input'' and ''weight''
    is float64, which is Double type, and the dtype of ''bias'' is float32, which
    is Float type. Therefore, the dtype of ''bias'' should be consistent with ''input''
    and ''weight''. Left : bias.dtype, op : ==, comparator : input.dtype or weight.dtype'
  target: expected scalar type Double but found Float
  txt: bias.dtype==input.dtype
- cot: 'The error message indicates that the ''weight'' tensor is expected to be a
    vector with size equal to the number of channels in the ''input'' tensor. However,
    the ''weight'' tensor has an empty shape [], and ''input'' tensor has a shape
    of [8, 5, 4]. Therefore, the length of the ''weight'' tensor should be equal to
    the number of channels in ''input'' tensor. Number of channels can be inferred
    from the dimension of the ''input'' tensor. If we assume channels are represented
    by the last dimension (in this case, 4), then the expected shape of ''weight''
    tensor should be [4]. Hence, the constraint can be written as follows:'
  target: Expected weight to be a vector of size equal to the number of channels in
    input, but got weight of shape [] and input of shape [8, 5, 4]
  txt: (len(weight.shape) == 1) or ((len(weight.shape) == 1) and (len(weight) == input.shape[-1]))
- cot: 'The error is due to the number of channels in the input not being divisible
    by num_groups. The input has a shape of [8, 4, 6, 6, 1, 3, 4, 1, 3, 2] and num_groups
    is 6. From the error message, it is clear that num_groups should be a factor of
    the number of channels in the input. Therefore, the number of channels in the
    input tensor (input.shape[1]) should be divisible by num_groups. So, Left : input.shape[1]
    % num_groups, Op : ==, and Right : 0.'
  target: Expected number of channels in input to be divisible by num_groups, but
    got input of shape [8, 4] and num_groups=5
  txt: input.shape[1] % num_groups == 0
- cot: 'This error is raised because the input size for training is not as expected.
    It expects more than 1 value per channel, but got only 1 value. Let''s see what
    the args were. The input tensor size is [1, 1] which means it contains only one
    value per channel. Therefore, the size of input tensor''s channel (input.size[1])
    should be greater than 1. Left : input.size[1]. Op : >, and Right : 1.'
  target: Expected more than 1 value per channel when training, got input size [1,
    1]
  txt: (input.shape[2] > 1) and (input.size[1] > 1)
- cot: 'The error is triggered due to an attempt to divide or modulo by zero. In this
    case, ''num_groups'' is 0 which could potentially be used in a division or modulo
    operation. Therefore, we need to ensure that ''num_groups'' is not zero. Left
    : num_groups, op : !=, right : 0.'
  target: integer division or modulo by zero
  txt: num_groups != 0
- cot: 'Error is triggered because the input tensor has only 1 dimension while the
    function expects at least 2 dimensions. Let''s see what the args were. It seems
    ''input'' tensor has 1 dimension, and is the problem. To correct this, we need
    to ensure that ''input'' tensor has at least 2 dimensions. Therefore, Left : input.ndims().
    It says that expected at least 2, so Op : >=, and Right : 2.'
  target: Expected at least 2 dimensions for input tensor but received 1
  txt: input.ndims() >= 2
time_cost: 4996.186146259308
title: torch.nn.functional.group_norm
tokens_used: 34781
trained: true
