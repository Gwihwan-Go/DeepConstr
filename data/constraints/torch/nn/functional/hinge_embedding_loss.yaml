constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  margin:
    default: 1.0
    dtype: float
    init: false
    required: false
  reduce:
    default: null
    dtype: bool
    init: false
    required: false
  reduction:
    default: mean
    dtype: Literal["none", "mean", "sum"]
    init: false
    required: false
  size_average:
    default: null
    dtype: bool
    init: false
    required: false
  target:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
package: torch
pass_rate: 0.98
rules:
- cot: 'Error is triggered because the size of tensor ''a'' (8) must match the size
    of tensor ''b'' (9) at non-singleton dimension 8. Let''s see what the args were.
    The length of dimensions of input and target tensors are not equal. Therefore,
    Left : len(input), Op : ==, and Right : len(target).'
  target: The size of tensor a (8) must match the size of tensor b (9) at non-singleton
    dimension 8
  txt: (input.dim==target.dim) and ((all(input.shape[i] == target.shape[i] or input.shape[i]
    == 1 or target.shape[i] == 1 for i in range(-1, -min(len(input.shape), len(target.shape))-1,
    -1))) and ((len(input) == len(target))))
- cot: 'Based on the given values, the error arises because we are trying to apply
    the ''clamp'' operation on a complex type tensor. In this case, ''input'' tensor
    is of complex128 type, which is not supported. Therefore, ''input'' tensor type
    should be checked and corrected to avoid this error. The ''input'' tensor should
    not be a complex type. So, the Left : input.dtype, Op : not in, Right : [''complex64'',
    ''complex128''].'
  target: clamp is not supported for complex types
  txt: input.dtype not in ["complex64", "complex128"]
title: torch.nn.functional.hinge_embedding_loss
