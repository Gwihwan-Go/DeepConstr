constraints:
  beta:
    default: '1.0'
    dtype: float
    init: false
    required: false
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  reduce:
    default: null
    dtype: bool
    init: false
    required: false
  reduction:
    default: mean
    dtype: Literal['none', 'mean', 'sum']
    init: false
    required: false
  size_average:
    default: null
    dtype: bool
    init: false
    required: false
  target:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
infered_history: []
infered_times: 16
package: torch
pass_rate: 0.93
rules:
- cot: 'Error is triggered because the size of tensor a and tensor b do not match
    at non-singleton dimension 6. Let''s see what the args were. It seems dimensions
    of input and target are not same, so input.dim and target.dim should be corrected.
    Therefore, Left : input.dim. Op : ==, and Right : target.dim'
  target: The size of tensor a (10) must match the size of tensor b (6) at non-singleton
    dimension 6
  txt: (input.shape[9] == target.shape[9]) and ((input.shape[6] == target.shape[6])
    and ((all(input.shape[i] == target.shape[i] or input.shape[i] == 1 or target.shape[i]
    == 1 for i in range(len(input.shape)))) and ((input.shape[2] == target.shape[2])
    and (input.dim == target.dim))))
- cot: 'The error suggests that the ''beta'' value for smooth_l1_loss must not be
    negative. The given ''beta'' value is -1.2463660479891137, which is negative and
    thus causing the error. Therefore, Left: ''beta'', Op: ''>='', and Right: 0.'
  target: smooth_l1_loss does not support negative values for beta.
  txt: beta >= 0
time_cost: 981.9476292133331
title: torch.nn.functional.smooth_l1_loss
tokens_used: 10691
trained: true
