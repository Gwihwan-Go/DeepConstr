constraints:
  inplace:
    default: false
    dtype: bool
    init: false
    required: false
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
infered_history: []
infered_times: 2
package: torch
pass_rate: 0.85
rules:
- cot: 'The error is due to the fact that the "silu_cpu" function is not implemented
    for ''Char'' or int8 type tensor. Checking the value dictionary, the ''input''
    tensor is of type int8. Therefore, the dtype of ''input'' tensor should be corrected
    to avoid this error. Therefore, Left : input.dtype. Since it''s not implemented
    for ''Char'' or int8 type, so Op : !=, and Right : ''int8''.'
  target: '"silu_cpu" not implemented for ''Char'''
  txt: input.dtype != 'int8'
time_cost: 60.335253953933716
title: torch.nn.functional.silu
tokens_used: 1278
trained: true
