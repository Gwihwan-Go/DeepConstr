constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  max_norm:
    default: null
    dtype: float
    init: false
    required: false
  norm_type:
    default: 2.0
    dtype: float
    init: false
    required: false
  padding_idx:
    default: null
    dtype: int
    init: false
    required: false
  scale_grad_by_freq:
    default: false
    dtype: bool
    init: false
    required: false
  sparse:
    default: false
    dtype: bool
    init: false
    required: false
  weight:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
package: torch
pass_rate: 0.0
rules:
- cot: 'The error "Padding_idx must be within num_embeddings" indicates that the padding_idx
    value is outside the range of num_embeddings. In this case, num_embeddings is
    the total size of the vocabulary, i.e., the first dimension of the weight tensor.
    Here, ''weight'' tensor has a shape of [2, 7], which indicates that num_embeddings
    is 2. However, padding_idx is 2 which is outside of the range of num_embeddings.
    Therefore, Left : padding_idx should be corrected. It must be less than num_embeddings,
    so Op : ''<'', and Right : weight.shape[0].'
  target: Padding_idx must be within num_embeddings
  txt: padding_idx < weight.shape[0]
- cot: The error message indicates that a 4-dimensional tensor is being used where
    a 2-dimensional tensor is expected. Looking at the given values, we see that 'weight'
    is a 4-dimensional tensor. You are trying to use the 'embedding_renorm_' function,
    which requires a 2-dimensional tensor as input. Therefore, the dimensions of 'weight'
    should be corrected. It says that expected 2, so we compare 'weight.ndims()' with
    2. Also, it implies that dimension cannot be 4, so we compare 'weight.ndims()'
    with 4.
  target: 'Expected 2-dimensional tensor, but got 4-dimensional tensor for argument
    #1 ''self'' (while checking arguments for embedding_renorm_)'
  txt: weight.ndims() == 2
- cot: 'Error is triggered because the ''indices'' argument is not of the correct
    scalar types. It expects to have either Long or Int type, but got torch.FloatTensor
    instead. Let''s see what the args were. It seems ''input'' is the problem because
    it is a float32 tensor, and it should be either Long or Int type. Therefore, Left
    : input.dtype. Op : ==, and Right : Long or Int. Also, It implies that dtype cannot
    be Float, so Op : !=, and Right : Float.'
  target: 'Expected tensor for argument #2 ''indices'' to have one of the following
    scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments
    for embedding_renorm_)'
  txt: input.dtype != Float
skipped: True 
skipped_reason: unsupported constriants
title: torch.nn.functional.embedding
