constraints:
  approximate:
    default: none
    dtype: Literal['none', 'tanh']
    init: true
    required: false
  input:
    default: 'null'
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
infered_history: []
infered_times: 3
package: torch
pass_rate: 0.9
rules:
- cot: 'The error message indicates that the "GeluKernelImpl" is not implemented for
    ''Long'' datatype, which is int64 in this case. The input tensor is of type int64
    as per the values provided. In order to avoid this error, the datatype of ''input''
    tensor needs to be changed to a type that is compatible with the "GeluKernelImpl".
    Therefore, Left : input.dtype should be corrected. It is currently ''int64'' which
    is causing the error, so Op : !=, and Right : ''int64''.'
  target: '"GeluKernelImpl" not implemented for ''Long'''
  txt: input.dtype != 'int64'
time_cost: 140.28014063835144
title: torch.nn.GELU
tokens_used: 1934
trained: true
