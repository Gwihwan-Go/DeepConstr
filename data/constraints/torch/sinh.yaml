alias: torch.sinh
constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
infered_history: []
infered_times: 14
package: torch
pass_rate: 0.98
rules:
- cot: 'The error is due to the attempt to resize a storage that is not resizable.
    The tensors provided in the ''input'' and ''out'' values are of different sizes.
    The ''input'' tensor has a size of [6, 8, 6, 6, 1, 3, 4, 1, 3, 2] and the ''out''
    tensor has a size of [8, 2, 6, 6, 8, 1, 2, 2, 6, 1]. This indicates that the sizes
    of the ''input'' and ''out'' tensors should be equal to prevent resizing operation
    on non-resizable storage. Therefore, Left: len(input), Op: ==, Right: len(out).'
  target: Trying to resize storage that is not resizable
  txt: (all(out.shape[i]==input.shape[i] for i in range(out.rank))) and (len(input)
    == len(out))
- cot: 'The error is triggered because the result type ComplexDouble cannot be cast
    to the desired output type Float. Let''s see the args were. The dtype of ''input''
    is complex128 and the dtype of ''out'' is float32. Therefore, the dtypes of ''input''
    and ''out'' should be same or the ''out'' should be able to accept complex numbers.
    Left : type(input), op : ==, Right : type(out)'
  target: result type ComplexDouble can't be cast to the desired output type Float
  txt: (input.dtype == 'Float32') and (type(input)==type(out))
time_cost: 1157.6333677768707
title: torch.sinh
tokens_used: 8015
trained: true
