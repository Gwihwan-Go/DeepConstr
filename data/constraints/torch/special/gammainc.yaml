alias: torch.special.gammainc
constraints:
  input:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: true
  other:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
  out:
    default: null
    dtype: float16,float32,float64,uint8,complex64,complex128,int8,int16,int32,int64,bool
    init: false
    required: false
infered_history: []
infered_times: 34
package: torch
pass_rate: 0.75
rules:
- cot: 'The error is triggered because result type Float cannot be cast to the Char.
    Let''s see the what''s args were. The dtype of ''input'' and ''other'' is float32,
    and ''out'' is int8. Therefore, the dtypes of all arguments should be consistent.
    Left : type(input), op : ==, comparator : type(out) or type(other)'
  target: result type Float can't be cast to the desired output type Char
  txt: (type(out)==type(other)) and (type(input)==type(out))
- cot: 'Based on the given values, the error is triggered due to mismatch in tensor
    sizes. It''s clear that the size of tensor ''input'' which is 9, does not match
    the size of tensor ''other'' which is 2. This inconsistency needs to be corrected.
    Therefore, Left: input.size, Op: ''=='', and Right: other.size.'
  target: The size of tensor a (9) must match the size of tensor b (2) at non-singleton
    dimension 0
  txt: (all(out.shape[i] == other.shape[i] or out.shape[i] == 1 or other.shape[i]
    == 1 for i in range(-1, -min(len(out.shape), len(other.shape))-1, -1))) and (input.size
    == other.size)
- cot: 'The error is triggered because the ''out'' tensor is being resized, which
    is not allowed. Let''s see what the arguments (''args'') were. It seems that ''out''
    tensor has an empty shape ([]), but it should match the shape of ''input'' tensor
    (which is [2, 8] in this case). Thus, the ''out'' tensor''s shape should be corrected
    to be the same as ''input'' tensor''s shape. Therefore, Left : out.shape, Op :
    ==, Right : input.shape.'
  target: Trying to resize storage that is not resizable
  txt: out.shape == input.shape
- cot: 'The error arises because the "igamma_cpu" function is not implemented for
    complex float types. Looking at the input values, ''input'', ''out'', and ''other''
    are tensors. ''input'' and ''out'' are of complex64 type while ''other'' is of
    float32 type. Therefore, the constraint should be that the data type of ''input''
    and ''out'' should not be complex64 as it is not supported by the "igamma_cpu"
    function. So, the Left : type(input) and type(out) should be corrected. It implies
    that type cannot be ''ComplexFloat'', so Op : !=, and Right : ''ComplexFloat''.'
  target: '"igamma_cpu" not implemented for ''ComplexFloat'''
  txt: type(out) != 'ComplexFloat'
time_cost: 2696.1291007995605
title: torch.special.gammainc
tokens_used: 20912
trained: true
