constraints:
  alpha:
    default: 0.2
    dtype: float
    init: false
    required: false
  features:
    default: null
    dtype: float16,bfloat16,float32,float64
    init: false
    required: true
  gradients:
    default: null
    dtype: float16,bfloat16,float32,float64
    init: false
    required: true
  name:
    default: null
    dtype: str
    init: false
    required: false
infered_history:
- 5
- 4
- 1
- 4
infered_times: 15
package: tf
pass_rate: 0.54
rules:
- cot: 'The error is triggered because the function is trying to perform a computation
    on a bfloat16 tensor and a float tensor. However, both tensors in generated args
    should have the same data type. It seems that the ''features'' tensor is float32,
    but the ''gradients'' tensor is of type bfloat16. Therefore, Left : type(features),
    which is the type of tensor features, should be corrected. It says that should
    be equal to the datatype of tensor gradients, so Op : ==, and Right : type(gradients).'
  target: 'cannot compute LeakyReluGrad as input #1(zero-based) was expected to be
    a bfloat16 tensor but is a float tensor [Op:LeakyReluGrad] name:'
  txt: type(features)==type(gradients)
- cot: "Error is due to the mismatch in the sizes and shapes of the inputs to the\
    \ operation LeakyReluGrad. Here, input 0 has shape [6,1] and input 1 has shape\
    \ [8,2,6,6,8]. Possible inputs can be 'gradients' for input 0 and 'features' for\
    \ input 1. \n\nTherefore, Left : gradients.shape, Right : features.shape"
  target: 'Inputs to operation LeakyReluGrad of type LeakyReluGrad must have the same
    size and shape. Input 0: [6,1] != input 1: [8,2,6,6,8] [Op:LeakyReluGrad] name:'
  txt: gradients.shape == features.shape
- cot: 'The error is due to the ''T'' attribute having an ''int32'' type, but it should
    be either ''half'', ''bfloat16'', ''float'', or ''double''. The tensors ''gradients''
    and ''features'' are provided as ''int32'' which is incorrect. Therefore, the
    type of ''gradients'' and ''features'' should be corrected to one of the allowed
    types. The constraint is: type(gradients) and type(features) should be in [''half'',
    ''bfloat16'', ''float'', ''double'']. The operator is ''in'', and the right value
    is [''half'', ''bfloat16'', ''float'', ''double''].'
  target: "Value for attr 'T' of int32 is not in the list of allowed values: half,\
    \ bfloat16, float, double\n ; NodeDef: ; Op backprops:T; attr=alpha:float,default=0.2;\
    \ attr=T:type,default=DT_FLOAT,allowed=[DT_HALF, DT_BFLOAT16, DT_FLOAT, DT_DOUBLE]>\
    \ [Op:LeakyReluGrad] name:"
  txt: type(gradients) in ['half', 'bfloat16', 'float', 'double'] and type(features)
    in ['half', 'bfloat16', 'float', 'double']
- cot: The error occurs because the input tensors 'gradients' and 'features' have
    more dimensions than the operation 'LeakyReluGrad' can handle. The operation can
    only handle up to 8 dimensions, but the input tensors have 10 dimensions. Hence,
    to avoid this error, the number of dimensions of the input tensors should be less
    than or equal to 8.
  target: 'We only handle up to Tensor::dims() up to 8, not 10 [Op:LeakyReluGrad]
    name:'
  txt: len(gradients.dim)<=8, len(features.dim)<=8
time_cost: 513.0382318496704
title: tf.raw_ops.LeakyReluGrad
tokens_used: 9269
trained: true
