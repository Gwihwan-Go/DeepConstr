args:
  dtype:
  - Tensor
  - Tensor
  - Tensor
  is_pos:
  - true
  - false
  - false
  name:
  - self
  - exponent
  - out
  required:
  - true
  - true
  - true
name: torch.float_power
package: torch
pass_rate: 100
rules:
- - cot: ''
    length: 1
    target:
      choosen_dtype:
        exponent: tensor
        out: tensor
        self: tensor
      msg: the output given to float_power has dtype Float but the operation's result
        requires dtype Double
      package: torch
    txt: dtype(out) == Double
  - f1_score: 100.0
    overall_score: 100
    precision: 100.0
    recall: 100.0
- - cot: "The error is triggered because the size of tensor a (8) does not match the\
      \ size of tensor b (9) at non-singleton dimension 2. To prevent this error,\
      \ we need to ensure that the dimensions and shapes of the tensors are consistent.\
      \ \n\nThe revised constraint is:"
    length: 1
    target:
      choosen_dtype:
        exponent: tensor
        out: tensor
        self: tensor
      msg: The size of tensor a (3) must match the size of tensor b (9) at non-singleton
        dimension 1
      package: torch
    txt: exponent.dim == self.dim and exponent.shape == self.shape
  - f1_score: 97.40259740259741
    overall_score: 100
    precision: 100.0
    recall: 94.9367088607595
- - cot: The error is due to mismatch in shape between the 'out' tensor and the result
      of the operation. Here, 'out' tensor shape should match with the shape of 'self'
      tensor, as the output of the operation is supposed to have same shape as the
      'self' tensor. Therefore, the constraint will be that 'out' tensor's shape and
      rank must be equal to 'self' tensor's shape and rank.
    length: 1
    target:
      choosen_dtype:
        exponent: tensor
        out: tensor
        self: tensor
      msg: Trying to resize storage that is not resizable
      package: torch
    txt: out.rank==self.rank and all(out.shape[i]==self.shape[i] for i in range(out.rank))
  - f1_score: 75.94936708860759
    overall_score: 100
    precision: 100.0
    recall: 61.224489795918366
