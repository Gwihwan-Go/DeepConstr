args:
  dtype:
  - Tensor
  - Tensor
  - number
  - Tensor
  is_pos:
  - true
  - false
  - false
  - false
  name:
  - self
  - other
  - alpha
  - out
  required:
  - true
  - true
  - false
  - true
name: torch.subtract
package: torch
pass_rate: 100
rules:
- - cot: 'Error is triggered because the size of tensor a (2) doesn''t match the size
      of tensor b (9) at non-singleton dimension 1. The arguments involved in this
      error are self and other. In order to prevent this error, we need to make sure
      that the dimensions of self and other tensors are the same at non-singleton
      dimension 1. Therefore, the constraint can be formulated as follows:'
    length: 1
    target:
      choosen_dtype:
        alpha: int
        other: tensor
        out: tensor
        self: tensor
      msg: The size of tensor a (4) must match the size of tensor b (9) at non-singleton
        dimension 2
      package: torch
    txt: self.shape[1] == other.shape[1]
  - f1_score: 76.53061224489797
    overall_score: 100
    precision: 100.0
    recall: 61.98347107438017
- - cot: synthesized
    length: 3
    target:
      choosen_dtype:
        alpha: int
        other: tensor
        out: tensor
        self: tensor
      msg: Trying to resize storage that is not resizable
      package: torch
    txt: ((out.rank == self.rank and all(out.shape[i] == self.shape[i] for i in range(out.rank)))
      or (out.shape[0]==5)) and (out.rank == other.rank and all(out.shape[i] == other.shape[i]
      for i in range(out.rank)))
  - f1_score: 78.63695937090432
    overall_score: 100
    precision: 100.0
    recall: 64.79481641468682
- - cot: synthesized
    length: 3
    target:
      choosen_dtype:
        alpha: int
        other: tensor
        out: tensor
        self: tensor
      msg: result type Float can't be cast to the desired output type Int
      package: torch
    txt: ((dtype(other) == dtype(out)) and (dtype(out) == int32)) and (dtype(self)
      == dtype(out))
  - f1_score: 67.57062146892656
    overall_score: 100
    precision: 100.0
    recall: 51.02389078498294
