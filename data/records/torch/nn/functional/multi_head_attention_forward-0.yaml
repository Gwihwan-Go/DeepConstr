args:
  dtype:
  - Tensor
  - Tensor
  - Tensor
  - int
  - int
  - Optional[Tensor]
  - Optional[Tensor]
  - Optional[Tensor]
  - Optional[Tensor]
  - bool
  - float
  - Tensor
  - Optional[Tensor]
  - bool
  - Optional[Tensor]
  - bool
  - Optional[Tensor]
  - bool
  - Optional[Tensor]
  - Optional[Tensor]
  - Optional[Tensor]
  - Optional[Tensor]
  - Optional[Tensor]
  - bool
  - bool
  is_pos:
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  name:
  - query
  - key
  - value
  - embed_dim_to_check
  - num_heads
  - in_proj_weight
  - in_proj_bias
  - bias_k
  - bias_v
  - add_zero_attn
  - dropout_p
  - out_proj_weight
  - out_proj_bias
  - training
  - key_padding_mask
  - need_weights
  - attn_mask
  - use_separate_proj_weight
  - q_proj_weight
  - k_proj_weight
  - v_proj_weight
  - static_k
  - static_v
  - average_attn_weights
  - is_causal
  required:
  - true
  - true
  - true
  - true
  - true
  - false
  - false
  - false
  - false
  - true
  - true
  - true
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
  - false
name: torch.nn.functional.multi_head_attention_forward
package: torch
pass_rate: 0
